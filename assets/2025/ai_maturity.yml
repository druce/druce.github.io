---
# AI Maturity Assessment Dimensions
# Source: dimensions_old.js
# 12 dimensions with 4 maturity levels each (Crawl, Walk, Run, Fly)

dimensions:
- id: 1
  short: Leadership
  full: Strategic Vision & Leadership
  description: Strategic vision and leadership commitment to AI initiatives across the organization.
  helpText: This dimension assesses how AI is incorporated into the organization's strategic planning, the level of executive
    sponsorship, and the clarity of AI-related goals and objectives.
  levels:
  - level: 1
    name: Crawl
    questions:
    - Are rank-and-file employees exploring the use of AI tools on their own initiative?
    - Is management discussing potential applications of AI?
    - Has leadership started assessing into how AI can benefit the business?
  - level: 2
    name: Walk
    questions:
    - Has the firm set a bold vision for AI anchored in business outcomes, with engageed, visible sponsorship form senior
      leadership?
    - Is there an AI steering committee with a transparent process for selecting AI initiatives, incorporating business functions,
      legal, HR, compliance, IT, CISO, and other relevant stakeholders?
    - 'Are use cases and value creation opportunities ranked by suitability: impact, effort, risk, and learning/transferability?'
    - Has leadership selected use cases for initial POCs?
    - Do initial pilot use cases have specific, measurable, achievable, relevant, and time-bound (SMART) AI business objectives?
    - Has the organization allocated a budget for AI initiatives?
    - Do initial pilots have a compelling business case to ensure key stakeholders understand the 'why', and AI projects align
      with their needs and expectations?
  - level: 3
    name: Run
    questions:
    - Is leadership championing AI as a strategic priority?
    - Does leadership have a comprehensive medium-term AI roadmap with clear timelines and KPI targets, including ROI and
      adoption?
    - Has leadership budgeted Change-the-Business spending vs. Run-the-Business spending, commensurate with the step-change
      in technology?
    - Are technical teams, business leaders, and frontline employees well aligned to maximize AI effectiveness?
    - Are AI objectives integrated into departmental and individual performance goals?
    - Is there regular leadership review of AI initiative progress, KPIs, and outcomes?
  - level: 4
    name: Fly - broad adoption, a nimble AI-first institution
    questions:
    - Are AI capabilities central to the organization's competitive advantage?
    - Does leadership have a long-term strategy, deeply embedded in the organization's culture, processes, and technology
      stack?
    - Are there success metrics linked to the strategy, with a structured evaluation frameworks?
    - Is there a Chief AI Officer or other C-level individual responsible for AI leadership?
    - Does the leadership and board regularly review AI strategy and competitive positioning?
    - Does leadership guide decisions with a strong understanding of AI strategic opportunities and challenges, legal and
      ethical risk management, technical and data capabilities and requirements?
    - Is the company culture AI-first with AI well-integrated, strategic, transformative, and pervasive?
- id: 2
  short: Team
  full: Teams, Talent, Organizational Design
  description: AI talent, team structure, and organizational readiness for AI adoption. A good team wants to work with other good people, and with the best tools, so a strong team is the critical element to kickstart an AI success flywheel.
  helpText: 'AI adoption is a people-first transformation. Invest in recruitment, training, deploying a central AI team, cross-functional implementation teams, and AI skills at all levels. Allocate resources strategically: often as much as 70% on people and processes, against 30% on technology and data'
  levels:
  - level: 1
    name: Crawl
    questions:
    - Are individuals using AI tools without structure or coordination?
    - Is there self-taught AI talent without designated roles?
  - level: 2
    name: Walk
    questions:
    - Has the firm evaluated the organization's AI skill gaps and developed a plan to address them?
    - Is there a plan to build a central team with AI and cross-functional implementation expertise, or to organize roles
      and team structures to support AI initiatives?
    - Has an AI component been added as necessary to all job descriptions and staffing requirements?
    - Has the firm develped a recruitment/retention plan and identified strong recruiting partners for AI talent?
    - Has the firm identified and empowered AI champions for training as front-line AI and subject-matter experts?
    - Is the firm celebrating and sharing learnings, wins, and new best practices implemented?
  - level: 3
    name: Run
    questions:
    - Is there a dedicated AI team or center of excellence?
    - Are there lean, multifunctional AI teams with roles like Technical Lead, Data Engineer, and Domain Knowledge Expert/Product
      Manager, responsible for productionizing AI?
    - Is there a clear career path for AI-specific roles?
    - Are AI champions embedded at all levels across business units?
    - Are rank-and-file team members empowered to build grass-roots workflows and no-code applications using AI?
    - Are experiments, implementation milestonese, and innovations celebrated and learnings disseminated?
  - level: 4
    name: Fly
    questions:
    - Is there a regular AI talent pipeline through academic recruiting and internships?
    - Is there an AI-first culture of innovation
    - Does the company have a strong AI engineering team and culture?
    - Is AI talent and expertise pervasive throughout the organization?
    - Are teams able to restructure around AI automation?
- id: 3
  short: Data
  full: Data Engineering & Quality
  description: Data infrastructure, quality, governance, and availability for AI applications. High-quality, well-governed
    data is the fuel for AI. You can build a tall tower but if the foundation is crumbling or superstructure is misaligned,
    you're gonna have a bad time
  helpText: AI success requires unified access to data and APIs. Organizations should transition from siloed data
    pipelines to reusable data products. Data governance should extend to unstructured data.
  levels:
  - level: 1
    name: Crawl
    questions:
    - Are employees primarily using individually downloaded and maintained data?
    - Are teams managing data islands in siloed spreadsheets and files, and exposing it to AI?
    - Is data quality managed informally or ad-hoc?
  - level: 2
    name: Walk
    questions:
    - Has a data inventory been launched to identify data sources in use, catalog data types and formats, assess data quality and completeness,
      and map data flows and dependencies?
    - Are datasets being shared with AI tools being mapped, e.g, structured and unstructured data, file
      servers, databases, data lakes, etc.?
    - Is all data to be shared with AI tools being tagged for resource based access controls, a semantic layer AI can understand, and lineage/provenance back
      to original source?
    - Is data infrastructure ready to provide complete, accurate, timely, secure, scalable, well-catalogued data to AI?
    - Is the firm leveraging AI to building a central data engineering toolchain? (e.g. an enterprise data warehouse, Airflow,
      dbt, with ingestion, data quality and semantic layers?
  - level: 3
    name: Run
    questions:
    - Are data silos consolidated and organized in a data warehouse?
    - Has the firm started automating ingestion and analytical pipelines on recurrent schedules or triggers?
    - Have data sets been prioritized for exposure to natural language question answering, considering cost of time currently
      spent accessing them and the value of exposing data more efficiently to business users?
    - Has a data governance framework been launched, to achieve high quality, well catalogued, permissioned, single source of truth
      (e.g. data warehouse) for any data needed by AI?
    - Is a data quality layer being applied to data, starting with simple checks on the most valuable datasets and use
      cases?
    - If traditional ML is in use, have end-to-end continuous MLops training pipelines been deployed to regularly update models
      with new data?
  - level: 4
    name: Fly
    questions:
    - Are there robust pipelines with automations, monitoring, and alerting for timely data ingestion and updating, to serve complete, accurate, timely, secure, scalable, well-catalogued
      data for AI?
    - Are comprehensive role-based and identity-based access controls in place across all data assets?
    - Do automated systems continuously validate and improve data quality?
    - Is data redacted and anonymized to enable analysis and insights without overpermissioning?
    - Do data pipelines have automatic anomaly detection, alerting, and self-healing?
    - Is the firm gathering long term interaction streams and full 'world models' to train future assistants?
    - Is the firm preparing for improved AI, AGI and superintelligence, and what data, guardrails future AI
      could leverage?
- id: 4
  short: Tech Platforms
  full: Technology Platforms & Infrastructure
  description: AI platforms, development infrastructure, and deployment capabilities.
  helpText: Assesses the technological foundation for AI, including platforms, tools, infrastructure scalability, and MLOps
    capabilities.
  levels:
  - level: 1
    name: Crawl
    questions:
    - Are employees using consumer-grade AI tools (ChatGPT, Claude, etc.)?
    - Is AI experimentation happening primarily on personal devices or accounts?
    - Is IT exploring formal AI infrastructure and monitoring?
  - level: 2
    name: Walk
    questions:
    - Has the firm deployed 'horizontal' enterprise AI platforms with SSO and usage monitoring (e.g. Microsoft Copilot, ChatGPT
      Enterprise etc.)?
    - Are prompt libraries, and best practices documented and shared with users?
    - Has the firm chosen an integrated ecosystem / enterprise cloud infrastructure (Azure, Vertex, Bedrock) to implement
      AI capabilities, integrating into existing stacks, and scale across business units?
    - Do developers have access to enterprise coding IDEs and agentic assistants?
    - Has the firm begun to implement, test, and soft launch 'vertical' AI pilots and POCs, with systematic evaluation, user
      feedback loop, and deployment criteria?
    - Are no-code/low-code tools such as Microsoft Copilot Studio / Agent Studio available for business users to build simple
      automations?
    - Has the firm begun to pilot agentic solutions to specific tasks, including vertical SaaS solutions and in-house development
      using agent frameworks?
    - Has the firm selected an agent framework like OpenAI Agents SDK, Microsoft Agent Framework, LangChain for advanced agent
      development?
    - Has the firm implemented a standard AI output evaluation solution?
    - Has the firm made plans to link all resources to AI and replace legacy systems that hinder AI adoption with adaptable, AI-enabled
      architectures?
    - Has the firm begun to develop an integrated toolchain for building AI solutions, including coding assistants, agent frameworks, evaluation
      and testing frameworks, CI/CD, project management, bug reporting?
    - Has day-to-day automation begun for day-to-day IT help desk triage, support, troubleshooting, system admin, to allow
      AI skill-building and free teams to implement advanced AI workflows?
    - Is there strong separation between development/test/staging/production?
    - Are sandboxes available to run automated coding sessions in complete isolation to experiment without impacting either
      prod or dev?
  - level: 3
    name: Run
    questions:
    - Is there a robust, scalable AI tech infrastructure and platform for developing and deploying AI solutions at scale with
      defined SLAs?
    - Is there a standardized, integrated end-to-end AI development toolchain covering coding, testing, evaluation, deployment,
      observability, and incident management?
    - Is there a dev pipeline to move AI pilots into production efficiently, with robust automated testing and evaluation?
    - Are prompt libraries, reusable components, and best practices being developed and documented?
    - Are agent solutions easily discoverable by end-users via a directory/app store?
    - Is there a capability to build complex agent workflows, with high autonomy, context engineering, long- and short-term
      memory, parallel/concurrent multi-agent architectures?
    - Is the form exploring advanced agentic architectures, swarms, inter-agent communication, MCP integrations, skills, advanced
      RAG. etc.?
    - Do production AI systems implement retry logic, fallbacks, and validation checks, composing reliable systems from unreliable
      components?
    - Are production AI systems monitored for anomalies and regressions?
    - Are there AI-powered CI/CD pipelines, with AI PR reviews and automated tests?
    - Do rank and file employees have the ability to to prototype AI workflows, that can then be scaled robustly and securely
      across the firm by IT?
  - level: 4
    name: Fly
    questions:
    - Has the firm moved beyond bespoke 0 to 1 development, to AI-assisted industrialized delivery at scale?
    - Are prompts, model versions, tools, and configurations versioned and reproducible, is the full AI system state treated
      as a deployable artifact?
    - Is the firm using AI-driven DevOps to manage the full lifecycle of a large fleet of AI applications?
    - Is the firm using AI to deploy and maintain large numbers of robust AI-enabled apps with security, observability, scalability?
    - Does the firm have a repeatable, measured path from idea to production deployment, with predictable timelines (weeks
      not months) and resource requirements?
    - Is the firm implementing advanced capabilities like complex agent swarms, knowledge graphs, advanced RAG, stream processing?
    - Is the firm implementing autonomous agents, with evals and clear criteria for reliability, cost, and promotion to production?
    - Do business users have governed self-service platforms to build, test, and deploy AI workflows?
    - Has the firm implemented automated prompt optimization?
    - Is AI automation deployed on auto-scaling infrastructure?
    - Does the firm support multiple major AI platforms and clouds?
    - Has the firm developed the ability to do fine-tuning and train models, including local models for AI/data sovereignty
    - Is model performance degradation automatically detected, and are models automatically retrained based on performance degradation
      or drift detection?
- id: 5
  short: Observability
  full: Observability, Controls, Logging, Monitoring
  description: Monitoring, logging, performance tracking, and observability of AI systems.
  helpText: Measures the firm's ability to track AI usage, monitor performance, maintain logs and audit trails, and gain visibility into AI
    system operations.
  levels:
  - level: 1
    name: Crawl
    questions:
    - Is AI usage monitored informally and incompletely?
    - Is success measured anecdotally (if at all)?
  - level: 2
    name: Walk
    questions:
    - Is there a unified firmwide logging infrastructure, including immutable logging of AI system inputs and outputs where
      required?
    - Are there documented log retention policies?
    - Are logs reviewed regularly to understand adoption, track successes and identify common problems areas which can be improved?
    - Is the firm collecting broad usage metrics like number of users/queries/tokens, interactions per user by business function/intent?
    - Is the firm measuring leadership defined, meaningful KPIs aligned with enterprise goals?
    - Is the firm collecting surveys/vibe checks/user labeling of good/bad interactions?
    - Is there visibility into AI compute costs by team, use case, or application?
    - Do all AI interactions support auditability, and regulatory inquiries?
  - level: 3
    name: Run
    questions:
    - Are key metrics (number of queries, users, costs, etc.) available in daily reports and real-time dashboards?
    - Is there a centralized AI gateway between users/applications and LLM providers?
    - Is there a prompt management / versioning / evaluation / observability / telemetry solution
    - Is the firm able to do deep traces and perform failure analysis of complex workflows?
    - Are inference latency, throughput, and cost SLAs defined per workload and monitored with alerting?
    - Is feedback gathered systematically, fostering ongoing learning, refining use cases, and improving KPIs?
    - Is there real-time monitoring of model drift and data quality?

  - level: 4
    name: Fly
    questions:
    - Do monitoring systems automatically trigger AI agentic workflows to research and remediate in addition to alerting?
    - Is there predictive alerting based on higher order derivatives and anomalies?
- id: 6
  short: Governance
  full: Governance
  description: AI governance policies, risk management, and regulatory compliance. AI governance isn't just a regulatory compliance
    requirement—it's a strategic enabler that allows businesses to scale AI confidently, minimize risks of uncontrolled adoption,
    and build trust with leadership and the user community.
  helpText: Evaluates governance frameworks, policies, ethical review processes, and compliance mechanisms to ensure AI
    initiatives meet requirements, manage risks, and are aligned on objectives and priorities.
  levels:
  - level: 1
    name: Crawl
    questions:
    - Have employees been made aware of potential AI-related risks?
    - Is management aware of all AI-specific compliance regulations and frameworks?
    - Is risk management for AI informal or non-existent?
  - level: 2
    name: Walk
    questions:
    - Are there documented and communicated AI acceptable usage policies and recommended best-practice guidelines?
    - Has the firm inventoried all AI systems and classified systems by risk level based on data sensitivity and decision
      impact?
    - Has the firm inventoried applicable AI regulations in all jurisdictions it operates in, including SEC regs like Reg
      S-P, state legislation, EU AI Act?
    - Has the firm established clear ownership and accountability for AI governance (e.g., AI governance committee, responsible
      AI officer)?
    - Has the firm implemented basic input/output guardrails for highest-risk systems?
    - Has the firm begun to systematically assess governance standards such as the NIST Risk Management Framework and ISO standards 42001
      and 23894?
  - level: 3
    name: Run
    questions:
    - Has the firm adopted published governance standards for data protection, privacy, responsible AI use, in whole or in
      part?
    - Are all AI projects subject to framework compliance review and risk assessment?
    - Is there a formal AI governance committee with regular meeting and review cadence?
    - Are AI vendor contracts reviewed for conformance, including liability, audit rights, data ownership, zero-day retention, training on firm data,
      and model rights?
    - Is IP protection systematically managed for AI-generated content?
    - Are business continuity and disaster recovery plans around AI regularly tested?
    - Have tabletop exercises been conducted to plan AI-related incident response?
    - Does the AI governance model appropriately balance centralized top-down oversight with decentralized bottom-up experimentation
      and delivery?
    - Is there documentation of AI decision-making processes for audit trails and explainability?
    - Automated, daily or real-time reporting of potential AI-detected compliance issues
  - level: 4
    name: Fly
    questions:
    - Are governance and risk management standards understood and implemented by 1st line creators, executors, operations and 2nd line managers
      and quality assurance?
    - Are governance standards reviewed by external auditors and supported by consultants as appropriate?
    - Are critical governance policies automatically enforced through technical controls?
    - Is there continuous compliance monitoring with automated reporting?
    - Is AI used to help governance and decision-making processes, reduce biases, and increase transparency?
- id: 7
  short: Security
  full: Security and Threat Management
  description: AI-specific security controls, threat awareness, and cyberdefense.
  helpText: Assesses security practices for AI systems, including data protection, threat detection, and AI-specific vulnerabilities.
  levels:
  - level: 1
    name: Crawl
    questions:
    - Are employees regularly made aware of the risks of hallucinations, sharing sensitive data with public AI tools, and required to self-certify with appropriate documentation like attestations?
    - Is there awareness of AI-specific security threats like deepfake impersonation, LLM phishing?
    - Are AI tools being adopted without security review by 'shadow IT' power users?
  - level: 2
    name: Walk
    questions:
    - Has the firm conducted a threat modeling exercise specific to AI systems, covering most common threats like OWASP Top
      10?
    - Are access controls enforced using RBAC/IAM with MFA for AI infrastructure?
    - Is sensitive data blocked by DLP and other measures from being shared with external AI tools; or are external AI tools fully blocked in favor of trusted enterprise AI (more secure)?
    - Are there AI-SPM, DSPM, and other security solutions in place to prevent AI-related security issues?
    - Are IT staff identified to manage risks like prompt injection and data leakage, and 'red teaming' to identify issues?
    - Are enterprise AI tools deployed with thoroughly reviewed and enhanced security configurations?
    - Has the firm updated employee training and phishing tests to protect against emerging AI-enhanced threats like smishing,
      vishing, spear-phishing, whaling, QRishing etc.
    - Is there a formal AI incident response playbook with defined escalation procedures?
  - level: 3
    name: Run
    questions:
    - Are AI assistants and tools deployed with comprehensive security controls like auto-redaction of MNPI or blocking of
      sensitive or problematic prompts, and rate-limiting and cost alerts?
    - Are all agents documented, restricted to least privilege, sandboxed, with appropriate guardrails and human in the loop
      escalation?
    - Are agents operating within an RBAC/IAM framweork with least privilege and a standard set of guardrails?
    - Are threat intelligence feeds in place covering AI/ML attack patterns, and integration into detection?
    - Is the firm using AI for cyber defense, log monitoring and triage (AI SecOps)?
    - Are security controls integrated into the ML and CI/CD pipelines with automated scanning for vulnerabilities?
    - Is there continuous adversarial testing (automated red-teaming) of production AI systems?
    - 'Has the firm considered upgrading AI apps to more private deployment models: enterprise SaaS -> private cloud -> on-prem?'
  - level: 4
    name: Fly
    questions:
    - Do AI agents operate with non-human identity management in zero-trust security architectures?
    - Is security audited by a third party to conform to industry standards like NIST AI Risk Management Framework, OWASP
      LLM Security Checklist
    - Do AI-powered security tools have visibility into all systems to detect and respond autonomously to threats?
    - Are real-time threat intelligence feeds automatically integrated into detection?
- id: 8
  short: Training
  full: Training and Education
  description: AI training programs, education initiatives, and knowledge management.
  helpText: The highest-performing companies train every employee to leverage AI-driven insights in daily decision-making,
    and cultivate an AI-first data-driven culture through training.
  levels:
  - level: 1
    name: Crawl
    questions:
    - Is AI training limited to individual self-learning?
    - Are employees discovering and learning AI tools on their own?
  - level: 2
    name: Walk
    questions:
    - Is there broad firm-wide AI training for all employees on AI basics, prompting, and best practices, including specific
      training for senior leaders?
    - Are all employees trained on all aspects of acceptable use policies?
    - Is human validation required of all AI-generated work, with training on how to validate all AI output, and never to
      accept potential hallucinations uncritically?
    - Does training attempt to overcome resistance and adoption barriers across different demographic groups (like among older workers,
      women who are less likely to adopt)?
    - Do IT employees have access to advanced training in data science, machine learning, and AI deployment to build in-house
      AI expertise?
    - Is there documentation of AI best practices and guidelines?
    - Are there online forums for sharing AI knowledge and use cases?
    - Are there regular in-person forums (lunch-and-learns, show-and-tells) for sharing AI use cases and best practices?
  - level: 3
    name: Run
    questions:
    - Are there defined learning paths for priority use cases and for different roles (analyst, PM, engineer, executive)?
    - Are case studies from actual firm AI workflows used in training?
    - Is there a formal certification program for internal AI competencies?
    - Is there continuous learning with regular AI training exercises, customized training based on user roles, and an emphasis on AI-first thinking for all workflows?
    - Is there an AI-enabled knowledge management system for AI practices and system documentation?
    - Can users share prompts and best practices through peer-to-peer knowledge sharing formalized through internal wikis, knowledgebases, and communities of practice?
  - level: 4
    name: Fly
    questions:
    - Does the firm leverage multiple learning modalities (online platforms, video, MOOCs, hands-on labs), with custom firm
      curricula?
    - Does the company support external certifications and advanced degrees?
    - Is there a formal AI academy or university partnership?
    - Do employees contribute to AI research and publications?
- id: 9
  short: Analytics & MLOps
  full: Analytics, AI Development & MLOps
  description: Firms of sufficient size should develop centralized AI/data science/MLops capability to deploy
    traditional machine learning, deep learning, and generative AI, with best-practice MLops and model lifecycle management.
  helpText: Evaluates the organization's ability to develop, deploy, and maintain AI models with modern MLOps practices.
  levels:
  - level: 1
    name: Crawl
    questions:
    - Are analytics limited to data provided by vendor AI tools?
    - Is expertise in data science and ML limited to self-motivated members of IT and data teams?
  - level: 2
    name: Walk
    questions:
    - Can the firm implement reporting and dashboards, business intelligence, data visualization, using a standard platform like Power BI or Tabbleau?
    - Is the firm reporting AI usage volume metrics, user feedback scores, and business KPIs like cost and churn?
    - Has the firm considered developing a centralized AL/ML team like an AI Center of Excellence (CoE) to build AI capabilities
      as company-wide platformss, execute cross-functional projects, and provide best practices, governance, and training
      resources?
  - level: 3
    name: Run
    questions:
    - Has the firm implemented real-time dashboards with KPIs and business metrics?
    - Has the firm implemented automated regulatory and compliance reporting with audit trails?
    - Has the firm implemented LLM evaluation frameworks (Promptfoo, RAGAS, DeepEval) for accuracy, faithfulness, relevance testing?
    - Are A/B testing and champion/challenger models implemented?
    - Can the firm do End-to-end workflow trace evaluations?
    - Are custom models developed and deployed for specific use cases?
    - 'Can the firm implement traditional machine learning models: Supervised learning (classification/regression), unsupervised?
      learning (clustering/dimensionality reduction), time series analysis (forecasting/anomaly detection), ensemble methods?
      (XGBoost/LightGBM) using scikit-learn/statsmodels?'
    - Can the firm apply natural language processing to text using web scraping, research/news feeds using traditional NLP and generative AI?
    - Do models use automated feature engineering and model selection?
    - Can the firm use knowledge graphs for relationship mapping, ownership structures, supply chain analysis, graph RAG?
    - Can the firm apply attribution and explainability with SHAP, LIME?
    - Can the form do multi-modal analysis (GPT-4 Vision) for charts, satellite imagery, presentation decks?
    - Can the firm do portfolio optimization (CVXPY, Riskfolio-Lib)?
    - Can the firm implment Factor models, backtesting frameworks, scenario analysis, and stress testing?
    - Is there a complete MLOps pipeline from development to production?
    - Are there risk management dashboards with real-time position monitoring, exposure analysis, alerts for threshold breach detection?
  - level: 4
    name: Fly
    questions:
    - Has the firm deployed custom or fine-tuned models (domain-specific embeddings, classification) for proprietary use cases?
    - Has the firm implemented alt data pipelines?
    - Has the firm implemented knowledge management at scale on real-time news and market intelligence?
    - Can the firm do advanced time series analysis (GluonTS, deep reinfrcement learning) for forecasting, regime detection, pattern recognition?
    - Do systems automatically optimize for specified objectives as new data becomes available?
- id: 10
  short: Use Cases
  full: Use Cases
  description: 'AI application deployment, use case portfolio, and business impact. Walk: POCs with relatively low risk, high
    impact, transferable learning; Run: human in the loop assistants; Fly; agentic automation. Initially focus on
    easy adoption, minimal disruption, and quick wins with simple, short-term, and narrow AI use cases, often without a coherent
    long-term plan.'
  helpText: Assesses the breadth and depth of targeted AI use cases, from individual productivity to autonomous agents handling end-to-end
    processes.
  levels:
  - level: 1
    name: Crawl
    questions:
    - Is AI use opportunistic without defined use cases and business objectives?
  - level: 2
    name: Walk
    questions:
    - Is there a systematic process to inventory and rank AI use cases by business function?
    - |
      Has the firm selected a few specific manual, contained workflows for pilots: retrieval, triage, summarization, data enrichment and draft generation—measured by impact, improvement in quality and productivity.<br />

      Example use cases:<br />
      • Meeting transcription and summarization (investor calls, internal meetings)<br />
      • Research report summarization<br />
      • Simple Q&A chatbots for firm knowledge bases such as policies, HR knowledgebase<br />
      • Email generation for standard communications, and response suggestions<br />
      • Simple data extraction from documents (earnings calls, filings)<br />
      • Standard pitch deck creation from templates<br />
      • Basic financial data visualizations<br />
      • Simple DDQ/RFP question answering<br />
      • Due diligence document analysis (AI highlights risks and summarizes according to a structured review)<br />
    - Is there a systematic process to identify AI solutions available on the market?<br />
    - Has the firm deployed an enterprise chatbot assistant like ChatGPT Enterprise or Copilot, with integrations to firm
      data?<br />
    - Has the firm deployed a general purpose agent assistant for users like Claude Cowork (in isolated sandbox)?<br />
    - Has the firm deployed a technology assistant like Claude Code for developers?<br />
    - Has the firm investigated what team members are already doing with cut and paste 'manual' AI workflows?<br />
    - Has the firm inventoried grunt work pain points that could be assisted with AI?<br />
    - Has the firm inventoried high-friction and high activation-energy pain points that could be alleviated with AI to increase
      velocity?<br />
  - level: 3
    name: Run
    questions:
    - Has the firm moved pilots from previous stage to production?
    - |
      Has the firm selected a few specific manual, contained workflows for human-in-the-loop agentic pilots?<br />

      Example use cases:<br />
      • Daily/weekly/monthly meeting agendas from structured/unstructured data<br />
      • Investment research copilots (analyst specifies elements to research, iterates on sections of report)<br />
      • DDQ/RFP response systems (AI drafts responses, human reviews)<br />
      • Portfolio or market commentary generation (AI drafts based on significant news and portfolio changes)<br />
      • Meeting preparation assistants (AI summarizes context, human prepares strategy)<br />
      • Compliance monitoring alerts (AI flags issues, leads through structured review checklist)<br />
      • Investor communication drafting (AI creates draft, human adds data/edits, head of IR edits/sends)<br />
      • Research report writing assistants (AI drafts sections, analyst refines)<br />
      • Contract review and red-lining (AI highlights issues, legal reviews)<br />
      • Market intelligence / alt data aggregation (AI compiles, analyst synthesizes)<br />
      • IT help desk ticket triage and routing based on topic, VIP tag<br />
    - Are there a system to solicited and evaluate use cases?
  - level: 4
    name: Fly
    questions:
    - Are autonomous agents handling end-to-end business processes?
    - |
      Have candidate workflows for end-to-end automation been identified? Imagine you could hire 1000 junior interns, which use cases would you re-engineer?<br />

      Example use cases:<br />
      • Daily trade reconciliation and exception handling workflow (autonomous matching, escalate only errors)<br />
      • Reference data management and updates (autonomous validation, data cleaning and updates)<br />
      • Regulatory filing preparation and submission (end-to-end automation with audit trail)<br />
      • Portfolio rebalancing proposals (within predefined parameters)<br />
      • NAV calculation, validation, distribution (autonomous daily processing)<br />
      • Client portal content updates (autonomous data refresh and publishing)<br />
      • Cash management and forecasting (autonomous monitoring and optimization)<br />
      • Vendor invoice processing and payment (autonomous matching and initial approval)<br />
      • Employee onboarding workflows (multi-step autonomous coordination)<br />
      • IT help desk ticket triage and resolution for frequent issues like backup restore, password reset (autonomous L1 support)<br />
      • Data room population for fundraising (autonomous document organization)<br />
      • Research distribution and tracking (autonomous delivery and metrics)<br />
- id: 11
  short: External Partnerships
  full: External Partnerships
  description: Access ecosystem partnerships, vendor relationships, and industry collaboration, to fuel innovation and to
    access external data and expertise.
  helpText: Assesses strategic partnerships with AI vendors, academic institutions, and industry consortia.
  levels:
  - level: 1
    name: Crawl
    questions:
    - Are vendor relationships limited to consumer SaaS subscriptions?
    - Is there no formal engagement with AI vendors or partners?
    - Is external AI expertise not being leveraged?
  - level: 2
    name: Walk
    questions:
    - Are designated team members systematically looking for 3rd party solutions, consultants, implementation partners for long-term relationships?
    - Are enterprise agreements in place with major AI platforms and vendors?
    - Is there due diligence and oversight of third-party AI vendors (including their supply chains) to ensure compliance,
      data security, and business alignment?
    - Do all third-party contracts document no model training and zero-day retention of your data, with periodic attestations
      of compliance with all contractual provisions?
  - level: 3
    name: Run
    questions:
    - Are strategic partnerships established with key technology providers?
    - Is the organization involved in industry consortia and working groups?
    - Are academic partnerships in place for research collaboration?
  - level: 4
    name: Fly
    questions:
    - Is the organization contributing to open-source projects and co-developing AI solutions with partners?
    - Are there joint ventures or investments in AI startups?
    - Does the organization influence partner roadmaps and standards?
- id: 12
  short: Process Implementation
  full: Business Integraion and Process Implementation
  description: 'Process integration, change management, and workflow optimization with AI.
    Organizations get maximum benefits not when they use AI to optimize one part within existing workflow, but when they re-engineer end-to-end workflows; not when you replace a horse with a car but when you re-engineer the transportation network.
    GenAI technology advances rapidly, but organizational change within enterprises happens more slowly, creating a speed
    limit to adoption and value creation. The winner
    is not the smartest or the strongest but the most adaptable to change, with the tightest OODA loop.'
  helpText: Evaluates how AI is integrated into business processes, from informal usage to fully automated workflows.
  levels:
  - level: 1
    name: Crawl
    questions:
    - Are AI tools used informally without adapting workflows and deep process integration?
    - Are workflows changing, but driven by individual contributors, without structured evaluation, explicit goals and evidence
      of achieving them?
  - level: 2
    name: Walk
    questions:
    - Has the firm implemented AI as assistant, through enterprise chatbots?
    - Has the firm identified 2-4 meaningful-impact, low-complexity, high-learning, measurable use cases to demonstrate value
      quickly?
    - Has the firm documented their business workflows to enable re-engineering them?
    - Has the firm launched targeted pilot projects with structured evaluations against SMART objectives?
    - Has the team begun to identify broader value creation opportunities from AI, enterprise competencies required to realize
      them?
    - Has the firm assessed where humans need to be in the loop for oversight and what constitutes acceptable risk and responsible
      use of AI?
    - Has the firm considered hackathons for innovative pilots, and other ways of encouraging grass-roots experimentation
      and learning while managing risk?
  - level: 3
    name: Run
    questions:
    - Does the firm have a playbook to operationalize processes as assistants with humans in the loop, scalably, robustly, with proper controls and business continuity/disaster
      recovery?
    - During the development cycle, is the firm creating re-usable, scalable components that can accelerate implementations?
    - During deployment and businesss integration, is the firm creating standardized playbooks that capture repeatable processes and accelerate future
      rollouts
    - Is AI explainable, so users know why it made a choice, and can rely on it, or take appropriate action and improve the
      process if the explanation is inadequate?
    - Is the firm considering when assistants are too creative and high risk, and does their creativity ceiling need to be
      lowered by breaking them down into predictable steps and adding checks, human intervention and fault-tolerance constraints?
    - Is the firm considering when assistants are insufficiently creative, and guardrails can be loosened?
    - Are assistants evaluated with data driven best practices, and known good/bad responses recorded for test suites to improve
      responses and prevent regressions?
    - Is the firm shifting design mindset from automating tasks within an existing process to
      reinventing the entire process with human and agentic coworkers?
    - 'Is the firm democratizing innovation: allowing bottom-up grass roots innovaton close to the business, letting rank and file workers reimagine
      how jobs are done, transforming operations, subject to data-driven review of the effect on operational performance, costs, risks,
      guardrails etc.'
    - Is there change management to insure new models and tools are deployed in a controlled and tested manner, without disruptions
      or regressions and measuring improvements?
    - Is there continuous process improvement driven by AI insights?
    - Are documentation and training materials updated on new AI-enhanced processes, with AI assistance?
  - level: 4
    name: Fly
    questions:
    - Is the firm systematically identifying issues which must be dealt with by the human in the loop, with a view to automating?
    - Has the firm built autonomous agent infrastructure, which can let agents notify humans and pause when intervention is required, so human can adjust/approve and continue?
    - Can agents use asynchronous, concurrent, parallel tasks and subagents to do multiple things at once?
    - Can agents try many solutions and pick the best one, or synthesize all responses? (bagging)
    - Can agents iteratively criticize their outputs, identify deficiencies, and correct them? (boosting)
    - Has the firm inventoried autonomous agent patterns and implemented a proof of concept for each one?
    - Can autonomous agents use reasoning, planning, long and short-term memory, swarms with inter-agent communication?
    - Can the firm implement agents as autonomous co-worker in Teams/Slack channels?
    - Is the firm not just optimizing existing workflows with AI, but redesign work to fully take advantage of AI (as if you had infinite interns)?
    - Has the firm established a structured process for continuous improvement, with comprehensive monitoring, inventorying
      new vendor capabilities, and establishing feedback loops with end-users?
    - Is AI a core enabler of growth, efficiency, and innovation?


# ==============================================================================
# Preserved comments from original file
# ==============================================================================

# Line 620: 1 Automate various repetitive human data entry or document analysis task.
# Line 623: WALK (Level 2): Initial Use Cases - Structured Learning & Pilots
# Line 627: - Meeting and earnings call transcription and basic summarization
# Line 628: - Document Q&A chatbots for internal policies, compliance manuals, and investment guidelines
# Line 629: - Simple investment memo drafting assistance (outline generation, formatting)
# Line 630: - Basic market research and news summarization
# Line 634: - DDQ and RFP response assistants (pre-filling standard responses)
# Line 635: - Marketing content generation for quarterly letters and client updates
# Line 636: - CRM note-taking and basic contact management automation
# Line 640: - Document digitization (OCR/NLP extraction from PDFs and scanned documents)
# Line 641: - Expense report automation and basic T&E processing
# Line 642: - Simple regulatory reporting assistance
# Line 643: - Contract review for standard terms
# Line 647: - Developer coding assistants for IT teams (GitHub Copilot, basic code completion)
# Line 648: - IT help desk chatbots for common employee questions
# Line 649: - Email drafting and response suggestions
# Line 650: - Meeting notes and action item extraction
# Line 653: RUN (Level 3): Advanced Use Cases - Operational Scale with Governance
# Line 657: - Investment research copilots integrated with filings, earnings calls, and proprietary data
# Line 658: - Alternative data analysis (web scraping, satellite data, sentiment analysis across multiple sources)
# Line 659: - Automated investment memoranda generation from research notes and data
# Line 660: - Meeting and earnings call analysis with sentiment tracking and key metrics extraction
# Line 661: - Sector-specific research automation with multi-document synthesis
# Line 665: - Automated sales and RFP response systems integrated with firm knowledge bases
# Line 666: - Client communication workflow automation (personalized updates, performance commentary)
# Line 667: - CRM systems with predictive insights and relationship scoring
# Line 671: - Trade order reconciliation and monitoring automation
# Line 672: - Cash management and treasury workflow automation
# Line 673: - Risk and reporting dashboards with anomaly detection and early warning systems
# Line 674: - Fund accounting automation for standard monthly/quarterly processes
# Line 678: - Compliance monitoring with automated documentation review
# Line 679: - Communications surveillance integrated with email, chat, and call systems
# Line 680: - Contract lifecycle management (drafting, review, renewals) with clause extraction
# Line 681: - Legal and regulatory Q&A systems connected to internal docs and regulations
# Line 685: - Data science copilots for analytics, modeling, and stress testing
# Line 686: - Infrastructure automation (cloud provisioning, pipeline monitoring)
# Line 687: - Cybersecurity automation (threat detection, log analysis, incident response)
# Line 690: FLY (Level 4): Very Advanced Use Cases - AI-Native Operations
# Line 692: Autonomous Investment Workflows
# Line 694: - Agentic portfolio construction assistants that can reason across multiple data sources, propose trades, and explain rationale
# Line 695: - Multi-agent research systems that autonomously monitor companies, identify signals, synthesize findings, and generate actionable
# Line 697: - Real-time market monitoring agents with dynamic adaptation to emerging trends and events
# Line 698: - Autonomous due diligence agents for private equity and venture capital that research companies, analyze financials, and draft investment
# Line 703: - Fully automated DDQ/RFP workflows with human oversight only at approval stage
# Line 704: - Autonomous regulatory reporting systems that prepare, review, and file reports with minimal human intervention
# Line 705: - Investor relations agents that proactively identify opportunities for client engagement and draft personalized communications
# Line 706: - Autonomous workflow orchestration across deal lifecycle (sourcing → diligence → investment committee → closing → monitoring)
# Line 710: - Multi-agent investment committee preparation systems (research agent + risk agent + compliance agent + presentation agent)
# Line 711: - Self-improving risk models that continuously learn from new data and adapt parameters
# Line 712: - Agentic operational AI for trade reconciliation, cash management, and NAV calculation with autonomous exception handling
# Line 713: - AI-led knowledge management systems that curate, synthesize, and distribute insights across the organization
# Line 717: - Fine-tuned models trained on proprietary investment data and firm-specific language
# Line 718: - Agentic AI mesh architecture coordinating multiple specialized agents (research, compliance, risk, operations)
# Line 719: - AI agents as core collaborators in investment decision-making with explainable reasoning
# Line 720: - Continuous model improvement pipelines with automated evaluation and deployment
# Line 724: - Proprietary AI capabilities offered as value-add to portfolio companies or LPs
# Line 725: - AI-native investment processes that fundamentally reshape how research and decisions are made
# Line 726: - Autonomous agents handling 80%+ of repetitive analytical and operational tasks
# Line 727: - Real-time personalized portfolio insights and client communication at scale
# Line 731: - Walk: Productivity enhancement within existing workflows
# Line 732: - Run: Process optimization and integration across systems
# Line 733: - Fly: Fundamental process reinvention with autonomous agents at the core
# Line 918: "merge with use cases?",
# Block comment from line 868:
#   run
#   - Develop clear "graduation criteria" to determine when pilots are ready for broader deployment, preventing projects from getting stuck in
#   the "pilot" phase. it's ok to close pilots with lessons learned and choose new underlying tech, architectures, and objectives
#   - Kick off a select number of high-impact agentic AI–driven workflow transformations in core business areas, while laying the groundwork
#   for an agentic AI technology foundation
#   "Deploy AI solutions into production, embed them within core business workflows, scale them robustly, securely, with strong governance",
#   "Move into more integrated, data-intensive AI solutions, such as predictive analytics in finance or HR and machine learning for forecasting",
#   - Key indicators that call for reinvention include high coordination overhead, rigid sequences that delay responsiveness, frequent human
#   intervention for decisions that could be data driven, and opportunities for dynamic adaptation or personalization
#   - Create standardized playbooks that capture learnings and accelerate future rollouts, while building reusable components that speed implementation
#   - Organizations must shift to a cross-functional delivery model, anchored in durable transformation squads composed of business domain
#   experts, process designers, AI and MLOps engineers, IT architects, software engineers, and data engineers
#   - Organizations must now shift to an industrialized delivery model, in which solutions are designed from the outset to scale, both
#   technically and financially
#   - The real breakthrough comes in the vertical realm, where agentic AI enables the automation of complex business workflows involving
#   multiple steps, actors, and systems
#   - Keep a dynamic backlog of opportunities, adopt structured forums to identify new AI use cases, use a robust prioritization framework
#   'fly'
#   - Realizing AI's full potential requires a shift in design mindset—from automating tasks within an existing process to reinventing the
#   - Opportunity now lies not in optimizing isolated tasks but in transforming entire business processes by embedding agents throughout the
#   value chain entire process with human and agentic coworkers
#   - AI agents can automate complex business processes by combining autonomy, planning, memory, and integration, shifting gen AI from a
#   reactive tool to a proactive collaborator
#   - move from reactive, tactical AI deployments to proactive, enterprise-wide transformation
#   - AI is part of the business model and firmly anchored within the company's DNA and corporate strategy. Products, services, and internal
#   processes have been adapted permanently by applying intelligent systems
#   '1nfinite interns'
#