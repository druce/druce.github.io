---
id: 7122
title: 'Numbers With Wings: A Modern Data Stack-In-A-Box'
date: 2023-01-22T01:01:01+00:00
author: Druce Vertes
layout: post
guid: /?p=7122
permalink: /2023/01/modern_data_stack
categories: datascience
tags: datascience

---
>*Not everything that counts can be counted, and not everything that can be counted counts. - Albert Einstein*

>*There are three kinds of people: those who can count, and those who can't. - Source unknown*
>

> ![NYC Subway entries](/assets/2023/mta_entries.png)

<!--more-->


Here is a longer post on creating a performant pipeline for 'pretty big' data, on a single box, instance, laptop, or container. The first part is, why do it, what are the components and considerations, and some general rants. Then we'll set up a stack to analyze MTA turnstile data, which is on the order of 1GB of data per per year.

# 0. Why a data stack-in-a-box?

Some might say, it's not *big* data if it fits on single box, like my laptop. One definition of big data used to be, data that is too big to be reasonably transformed or analyzed on a single box.

When Google first came out, the explosive growth of the Web was reducing the fraction that could be indexed, in most cases to the top couple of pages of a site. What really blew my mind was not just that Google had an order-of-magnitude bigger index and better relevance. But when you ran a search, the results gave not just a URL, but *the excerpt on the page* that matched your search. Google was *caching the text of the entire Web in RAM*. A cluster farmed out your search to dozens of stripped down PCs, cheap circuit boards zip-tied in place, each of which kept a shard of the index in RAM. A query was routed to a cluster, which got the top-ranking result from each shard, also looked up the relevant page and located the relevant excerpt, then sorted all this and assembled it into a results page. In a fraction of a second. 

Google clusters were a radical new computing paradigm. They expanded the realm of what was possible. 

Today, times have changed. You can fit *a lot* on a single box. I have a 2019 Intel MacBook Pro with 32GB of RAM; the latest [MacBook Pros](https://www.apple.com/newsroom/2023/01/apple-unveils-macbook-pro-featuring-m2-pro-and-m2-max/) have up to 96GB. Which is 'pretty big' data.  With the right stack, you can effectively analyze decades of MTA data down to individual turnstiles. 

Is the relevance of cluster computing waning? 

Modern CPU architectures like [Zen](https://en.wikipedia.org/wiki/Zen_4) and Apple's M1 and now M2 change a lot, if not everything. We almost always run multiple virtual servers on a single physical server. It is *cheaper* to run a giant [AMD Epyc box](https://aws.amazon.com/blogs/aws/new-amazon-ec2-c6a-instances-powered-by-3rd-gen-amd-epyc-processors-for-compute-intensive-workloads/) with 192 threads and 384GB of RAM than equivalent compute on individual servers. You can even rent an [8-socket Intel box with 24 TB of RAM](https://aws.amazon.com/ec2/instance-types/high-memory/)! Big data, at least in the sense of tabular data too big to fit on a single box in RAM, hardly even exists outside Big Tech.[^1] Today CPUs are so powerful that the pendulum may be swinging away from clusters. 

Giant multi-core CPUs and systems-on-a-chip (SOCs) change the way software is architected. For super high performance lightweight microservices you may want to [pin each thread to a core](https://twitter.com/alexxubyte/status/1588203762945884160), keep everything in CPU cache, and never context-switch. [The secret](https://debugger.medium.com/why-is-apples-m1-chip-so-fast-3262b158cba2) of Apple's M1 (now M2) is that it's not just a CPU, it also puts memory, I/O, GPU on a single chip for massive bandwidth. As Moore's law sunsets, expanding the envelope is more about giant chips with lots of cores that do everything.

If you can reasonably fit your data on one box, the juice may not be worth the squeeze to build a cluster. Just leverage a big box with many cores, lots of RAM, and massive bus bandwidth. 

Vertical is the new horizontal. 

Hence, the data stack-in-a-box. 

# 1. Components of data pipelines and stacks

We can divide what the data stack-in-a-box will do into 3 main tasks, with sub-tasks:

- Data warehouse, to persist and manage the data, and let us query it and aggregate it efficiently, taking full advantage of all the RAM and cores. Can be called a data warehouse, a DBMS, data lake, lakehouse etc.

- Extract, Load, and Transform (ELT) tools, to move data into the data warehouse and :

	- Tools to fetch data and ingest from REST APIs, CSVs, Parquet etc. via standard protocols. Built-in schemas for e.g. Zoom or Salesforce data and APIs would be nice.

	-  Scheduling / orchestration tools to detect new data and trigger jobs when needed: on a schedule, monitoring a local directory, polling a remote directoryor database, receiving an email, other resources and signals.

	- Tools to manage dependency graphs ([DAGs](https://en.wikipedia.org/wiki/Directed_acyclic_graph)) to trigger jobs like DB transformation / build, data quality, ML training, inference, notifications downstream when jobs succeed or fail.

- BI: Front end frameworks to create dataviz, reports, dashboards and end-user apps.

There are also [additional pieces](https://i.redd.it/pdnuk1r0yjf71.jpg), like [monitoring pipelines](https://medium.com/databand-ai/data-pipeline-performance-monitoring-what-it-is-and-why-its-here-c792f7cbc1e8) in production, [data quality](https://greatexpectations.io/), [data governance](https://www.collibra.com/us/en/products/data-governance). You can go [pretty deep](https://mattturck.wpenginepowered.com/wp-content/uploads/2021/12/2021-MAD-Landscape-v3.pdf). But if you've handled these concerns, you have a pretty good start.

An aside: Why ELT and not ETL? Earlier online analytic processing (OLAP) manifestations processed data into [cubes](https://en.wikipedia.org/wiki/OLAP_cube), sort of like a spreadsheet with a pivot table within a database, to batch pre-process aggregations and enable real-time drilldowns. Modern data stacks just keep data in raw form (or close to it) and leverage parallel processing to transform on the fly, obviating the need for preprocessing and determining up front what dimension will need to be aggregated. We give each core a shard of the data in RAM. Then each node can do its part of aggregations and drilldowns and send the results to a controller for final compilations, [MapReduce](https://en.wikipedia.org/wiki/MapReduce) paradigm style. In contrast to a client/server paradigm which moves data around disks and network, we marry each slice of data with nearby dedicated compute. Thus ELT in contrast to old-school ETL. In practice, however, we usually end up with ETLT: extract; clean up a little; load to our data warehouse in a clean, but unaggregated form; do drill-down and aggregation on the fly.

How could we build a basic [modern data stack](https://www.getdbt.com/blog/future-of-the-modern-data-stack/) with [MTA turnstile source data](https://data.ny.gov/Transportation/Turnstile-Usage-Data-2020/py8k-a8wg) to power dashboards like the [NYS Comptroller site](https://www.osc.state.ny.us/reports/osdc/impact-covid-19-pandemic-subway-ridership-new-york-city) or [this](https://toddwschneider.com/dashboards/nyc-subway-turnstiles/) or [this](https://www.subwayridership.nyc/)?

One way is an enterprise-ish 'on-prem' approach (typically in private or public cloud these days):
- Storage + compute: Spark cluster 
- Data pipeline management: Airflow, dbt, Fivetran, Stitch, Airbyte 
- Dataviz: PowerBI, Tableau, Superset
- Apps: Django web framework, Appsmith low-code framework

Another approach is 'cloud-native', using pay-as-you-go SaaS cloud services connected over public Inernet:
- Storage + compute: Cloud lakehouse like Snowflake or Databricks hosted Spark
- Cloud pipeline management: Astronomer hosted Airflow, Prefect, Dagster Cloud, dbt Cloud
- Cloud analytics service, Tableau Cloud, PowerBI SaaS, Preset hosted Superset
- Retool, Appsmith Cloud, with hosted Postgres to deliver end-user apps

With a cloud-native stack, SMBs can run their business more or less completely on SaaS services like Salesforce, Square, NetSuite, Mailchimp, Twilio, Qualtrics, Zoom. Who needs devs and system admins and MSPs? When you want to build a workflow app, for instance to manage a big conference, create some Zoom meetings, send some emails, texts, Slack/Discord messages, and get paid, build a front end in low-code platforms like Retool to talk to all those SaaS services. And then Tableau or PowerBI SaaS for dashboards and reporting.

# 2. Our data stack-in-a-box: DuckDB, dbt, Superset

But I'm not going to build any of these heavyweight solutions, I want a lightweight stack to pull 'pretty big' data, aggregate along various dimensions, and display a dashboard on my MacBook or in a container, for a data stack-in-a-box. Tech we will leverage:

- [Duckdb](https://duckdb.org/): like SQLlite, but for column-oriented data. It's a lightweight data warehouse that does high-performance multithreaded aggregation using SQL. [Columnar databases are faster for OLAP than row-oriented OLTP databases like Postgres](https://loonytek.com/2017/05/04/why-analytic-workloads-are-faster-on-columnar-databases/).[^2] When we have e.g. millions of rows of e.g. double-digit columns, we usually get orders of magnitude improvement in size/speed using a columnar format like Parquet vs. CSV with binary storage and compression.

- [dbt](https://www.getdbt.com/blog/future-of-the-modern-data-stack/): the database build tool. From your data warehouse's perspective, dbt is simply a SQL client. From the standpoint of a dev writing SQL transformations within your data warehouse, it's a structured scripting framework on top of SQL. dbt or sort of like PHP and Wordpress are to HTML. dbt makes SQL more dynamic and structured. You get almost for free: DAG workflow; logging; self-documentation of every table's provenance; parameterized SQL templates for modularity and re-use, and the ability to point any script to dev / test / production environments. 

- [Superset](https://superset.apache.org): an open source version of Tableau or PowerBI to run a dashboard.

This works pretty well ([code here](https://github.com/druce/MTA)). It seems to be an order of magnitude faster than raw pandas on 'pretty big' data. It's pretty cool, if you grew up in a world where Google didn't exist, to compute Google-style on your laptop or a $20 Hetzner or AWS box. It's similar enough to production-ready processes that when the time comes to move from your laptop to 'real' enterprise infrastructure, whether SaaS or 'on-prem', you will be using pretty much the same paradigm. 

DuckDB has some limitations. It is single-user. It writes to a single file. But it leverages all your RAM and cores in SQL aggregation and other operations. If you swap out DuckDB for Spark which will also run fine on your single box or laptop, this data-warehouse-in-a-box starts is an OK starting point for an enterprise stack.

If you are a normie business, just get Databricks (hosted Spark) or Snowflake (SQL data warehouse).  No offense to others but those are the dominant SaaS solutions. There are [a lot of ways](https://www.moderndatastack.xyz/stacks) to slice the salami. This dude even has a ['post-modern' data stack](https://blog.devgenius.io/modern-data-stack-demo-5d75dcdfba50). 

Today, rolling your own on-prem production modern data stack from open source is for teams with scale. Or penniless startups and hobbyists who don't want to pay for SaaS services. Which is probably not most teams. Regardless, in the rest of this post, we stand up our own small environment for a data app.

# 3. Requirements and initial DuckDB ingestion

### Hardware

 - The size of the data you can process fast will scale with RAM.
 - The speed at which you can process data will scale with cores.
 - I am running on a 2019 MacBook Pro with 32GB and 8 cores. If you run on a cloud instance, I would recommend starting with something like [r6a.xlarge](https://aws.amazon.com/ec2/pricing/on-demand/): AMD CPU, 32GB RAM, 4 cores. 

### Software

 - I will run everything on my Mac in a [conda virtualenv](https://conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html).
 - Ubuntu 20.04 or any OS that runs Anaconda should work, Windows, Linux, or Mac.  The Amazon [Deep Learning AMI](https://docs.aws.amazon.com/dlami/latest/devguide/overview-conda.html) is nice for machine learning in general but we are not going to use ML or GPU.
 - Python 3.8, I don't expect all modules to be 100% debugged and tested on the latest Python 3.9 and Ubuntu 22.04 as of this writing.

### Install

 - [Clone the repo](https://github.com/druce/MTA) into a local directory.
~~~ shell
git clone https://github.com/druce/MTA
~~~
- Download and install the appropriate [Anaconda](https://www.anaconda.com/products/distribution) for your OS.

  ~~~ shell
  $ wget https://repo.anaconda.com/archive/Anaconda3-2022.10-MacOSX-x86_64.sh 
  $ ./Anaconda3-2022.10-MacOSX-x86_64.sh 
  $ conda init           # (optional) update .bashrc etc.
  $ conda update conda   # (optional) check for updates to base 
  ~~~
  
- Make a virtualenv and install DuckDB

  ~~~ shell
  $ conda create -n mds python=3.8
  $ conda activate mds
  $ pip install duckdb duckcli pandas 
  ~~~
  
- If using Jupyter, [add SQL magic for DuckDB](https://duckdb.org/docs/guides/python/jupyter.html) and add `mds` virtualenv as a Jupyter kernel
  
  ~~~ shell
  $ pip install duckdb-engine ipython-sql sqlalchemy ipykernel
  $ python -m ipykernel install --user 
  ~~~

- [Download](https://github.com/druce/MTA/blob/main/0-download_data.py) and [load](https://github.com/druce/MTA/blob/main/1-ingest_data.py) the data into a DuckDB database mta.db. 
~~~ shell
  $ python 0-download_data.py
  $ python 1-ingest_data.py
~~~

# 4. Pipeline overview

At this point you should be able to do `jupyter notebook` and open [MTA-DuckDB.ipynb](https://localhost:8888/notebooks/MTA/MTA-DuckDB.ipynb) . In a nutshell:

- Load raw data into DuckDB
  - We have 1 row per turnstile per station per approximately 4-hour period
- Do some basic cleanup 
  - Make clean unique turnstile and station labels each in a single column
  - We have cumulative turnstile counters, so we first-difference them after grouping by turnstile, to get net entries and exits by period
  - Fix large outliers where presumably maintenance was done and counters were reset, remove negative or anomalously large entries/exits
- Compute aggregations like total entries by date, and visualize

# 5. dbt initial build / transform

### Install, configure, and run dbt

- Install
~~~ shell
  $ pip install dbt-duckdb
~~~

- Configure: edit dbt's profiles.yml config file to point to the directory where you cloned the MTA repo. Put it in your home directory under `~/.dbt/`. dbt  keeps these credentials and connection profiles here by default to avoid putting credentials in git repos.

- Run (these shell commands are in `build.sh`; edit your directory name)

~~~ shell
  $ # environment variables for python models that load data
  $ export BASEDIR="/Users/user/projects/MTA"
  $ export DBFILE="mta.db"
  $ export DATADIR="downloads"
  $ cd dbt_mta
  $ dbt debug  # check connection profile
  $ dbt seed   # load small static map tables from dbt/seeds
  $ dbt run    # download data and run build pipeline
  $ dbt test   # run tests
~~~

- On my Mac, this takes about 4 minutes to run on 5GB of data, not including the HTTP download but including the initial DB ingestion from .txt files.
- For comparison, the first version of the analysis that I wrote in pure Pandas without DuckDB takes about 30 minutes. With [Modin](https://modin.readthedocs.io/en/stable/) multi-threaded dataframes as a drop-in replacement for Pandas, this is cut to about 20 minutes. The comparison is not a valid benchmark since the transforms are not the same. Someone should do a proper benchmark.[^3] But this should give an idea of the benefits of a modern data stack.

### Full dbt walkthrough

The above uses the repo's fully set-up dbt project. Let's build it step by step from scratch so you can see what goes on under the hood and get a quick tour of dbt. 

- Back up the existing dbt directory
~~~ shell
  $ mv dbt_mta dbt_mta.sav
~~~

- In the MTA directory, run the below to create the scaffold of the project:
~~~ shell
  $ dbt init dbt_mta
  $ cd dbt_mta 
  $ ls  # inspect contents
  # At a minimum we need profiles.yml in ~/.dbt, dbt_project.yml and models in models directory
  $ dbt debug
~~~
- You should see `All checks passed`. If not, debug the `.dbt/profiles.yml` and the `dbt_project.yml` so everything is in sync. 
- Set up seeds (small static tables)
~~~
  cd seeds
  cp ../../dbt_mta.sav/seeds/*.csv .
  cp ../../dbt_mta.sav/seeds/schema.yml .
  # maybe inspect those files, seeds/schema.yml has some schemas, docstrings, tests
  cd ..
  dbt seed
  cd ..
  duckcli mta.db
  show tables
  select * from station_label_override
  exit
~~~
- You should see that 3 small tables have been created from the csv and schema files: `station_label_override`, `division_label_override`, `station_map`
- Now we create our models
~~~ shell
  cd dbt_mta/models
  cp ../../dbt_mta.sav/models/*.py .
  cp ../../dbt_mta.sav/models/*.sql .
  cp ../../dbt_mta.sav/models/*.yml .
  # maybe inspect those files, models/config.yml has some model schemas, docstrings, tests
  cd ..
  dbt run --select download_data
  dbt run --select ingest_data
~~~
- dbt doesn't talk HTTP or load data natively, it runs SQL transforms within the data warehouse. It expects raw data to be pre-loaded. For now, we have a couple of Python scripts to download and ingest, as models in the models directory. These `dbt run` commands just run the Python scripts within the dbt context.
- .sql files in the models directory run sql queries to create tables according to the model name
- .sql models refer to existing tables with `{% raw %}{{ref}}{% endraw %}` Jinja tags, creating a DAG defining the order of execution.
- To run and test the whole pipeline, type:
~~~ shell
dbt run
dbt test
~~~
- To view documentation run:
~~~ shell
  $ dbt docs generate
  $ dbt docs serve
~~~
- This allows you to browse the project documentation, and if you click 'View Lineage Graph' at bottom right on the Database tab, you should see something like:
![dbt lineage graph](/assets/2023/dbt_graph.png)

- There is more to dbt:
  - [dbt Cloud](https://www.getdbt.com/signup/) - dashboard for all your dbt projects
  - [Jinja](https://jinja.palletsprojects.com/en/3.1.x/) to parameterize SQL
  - Macros
  - Tests for data quality
  - Pre-hooks, post-hooks before and after loading tables.
  
dbt is probably something everyone should try for non-trivial pipelines. There isn't anything wrong with writing a pipeline based on a sequence of python scripts. But over time you will tend to standardize your script and turn it into a template. Then at some point maybe you will want to make a change everywhere. You might end up with a lot of scripts, and some data engineers who are better at SQL than Python, and it could get messy. dbt provides a standard framework out of the box that gives you a lot of stuff almost free: documentation, logging, modular templates. It's lightweight, there's not much lock-in, [most places](https://www.moderndatastack.xyz/stacks) use it.

# 6. Superset dashboard

I install Superset into my Mac conda virtualenv. The install varies a bit by platform, see [the instructions](https://superset.apache.org/docs/installation/installing-superset-from-scratch/). I found I had to pin a couple of module versions, which was a drag to figure out. You can install [a Docker image](https://superset.apache.org/docs/installation/installing-superset-using-docker-compose/), which should be straighforward if you have Docker, just mount the directory where the mta.db DuckDB database lives.

~~~ shell
# prereqs
# install or upgrade Xcode 
xcode-select --install

# install or upgrade Homebrew https://brew.sh/
brew upgrade
brew install readline pkg-config libffi openssl mysql postgres

# make virtualenv
conda create --name superset python=3.8
conda activate superset

pip install --upgrade setuptools pip

export LDFLAGS="-L$(brew --prefix openssl)/lib"
export CFLAGS="-I$(brew --prefix openssl)/include"

# install
pip install apache-superset

# at this writing these versions must be pinned
pip install --force-reinstall cryptography==38.0.4
pip install --force-reinstall wtforms==2.3.3
pip install --force-reinstall flask==2.1
pip install duckdb-engine Pillow

# Create an admin user in your metadata database (use `admin` as username to be able to load the examples)
export FLASK_APP=superset
superset fab create-admin

# Create default roles and permissions
superset init

# To start a development web server on port 8088, use -p to bind to another port
superset run -p 8088 --with-threads --reload --debugger
~~~

After a some wrangling, I get a dashboard that looks like this (2022 filter applied):

![Superset dashboard](/assets/2023/mta-dashboard.jpg)

Superset is a poor man's Tableau. It's powerful but complex and quirky. It was not as straightforward to create this as I expected. That being said, we can filter by a date range and see a comparison with pre-pandemic and pandemic periods, we can filter by Manhattan below 63 St, by day of the week, or anything reasonable, and see a comparison with corresponding days during pandemic and pre-pandemic. We can make a map widget. We can sort any table column to see top/bottom stations.

Here is stuff I don't love about Superset

- Instant regret from having a complex install and finding out I need to override versions of some components after the intall. Just use the containerized install. A downside is that Docker containers on my laptop use a lot of resources, make the fans run. An upside is it makes it easier to productionize.
	
- In my opinion I should be able to point a base install at a data source, import a dashboard, and it should just work. it's not that simple, I have to faff around in the web UI, edit some config files, put in some symbolic links because somehow when I moved some things around and some directories ended up hard-coded deep in the config DB. 
	
- I don't have confidence I can really customize every chart down to tick sizes etc.
	
- I don't love the native filtering UI and I don't see how to put custom widgets on a dashboard to e.g. filter by dragging a slider.
	
- Generally too many UX surprises.
	
But I've spent enough time for a 'weekend project' and I've seen enough for now. Since I love [Plotly](https://plotly.com/) and can code up nice charts in Python, I might go with Plotly [Dash](https://plotly.com/dash/) for dashboards, where I feel way more confidence I can configure every label and tickmark's style and size without going down a rabbit hole. The Dash gallery has nice filter widget and is a little more impressive. But you may not want to code with Plotly to make a dashboard.  For point-and-click no-code BI I might go with Tableau or PowerBI. Superset is not going to power state-of-the-art NY Times dataviz. But Superset is pretty good for a lot of purposes and has a lot of potential. For real-time dashboards I might go with [Grafana](https://grafana.com/) which is known for real-time monitoring. 

# 7. Wrapup

Some additional things we might do.

- Sub Dash for Superset.

- [Dagster](https://dagster.io/blog/duckdb-data-lake#-the-limitations-of-duckdb) for data download and ingestion. Airflow is also very popular to manage pipelines, but maybe more complex/less modern. We would like a layer that senses when remote data is available and runs the ELT, or runs it on a schedule, and lets us monitor the pipeline. This is such a simple pipeline that the full Dagster template is overkill and the simplest Dagster template doesn't exercise Dagster.

- Containerize the data stack-in-a-box. Make a [Dockerfile](https://github.com/druce/MTA/blob/main/Dockerfile) with installs of DuckDB, dbt, and Superset and other tools. By the time I got everything in there that I wanted, it became a 20GB image and that's kind of an abuse of Docker. 

- Use Spark for the ELT

- [Meltano](https://meltano.com) is a CLI to manage data pipelines, that can ingest data via [Singer taps](https://www.singer.io/#taps) and [Airbyte connectors](https://airbyte.com/connectors). It's the closest thing I've seen to an API of all APIs. I implemented it for the MTA data but it was much slower than just native DuckDB ingestion.

- Great Expectations data quality. Good DQ can detect possible data issues with high precision and recall. But the data quality issues here are challenging, It seems pretty easy for a turnstile to go from dozens of entries to hundreds if for instance an entrance is shut down and people are redirected, there is a big event etc. Insight into the real-world process and shoe leather tracking down process issues is inevitably the big part of the data quality function. And you have to do a reasonable amount of work relative to the benefit. If you are predicting overall system usage, then odd data from one turnstile may not matter, if you are redesigning a station then it might be telling you something. 

- Data governance, having the ability to have firmwide global 'library catalog' like [Amundsen](https://www.amundsen.io/) or [DataHub](https://datahubproject.io/) and processes to ensure regulatory complicance. Beyond the scope of a data stack-in-a-box.

This endeth my initial foray into modern data engineering. It's quite a rabbit-hole. This is one of those posts you publish out of exhaustion rather than satisfaction in the end-result. There may be future in-place updates.

In the meantime, may all your pipelines be unclogged!

<iframe width="560" height="315" src="https://www.youtube.com/embed/jGw1ZPgyEaE" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>

### The Bongos: Numbers With Wings
<iframe width="560" height="315" src="https://www.youtube.com/embed/XdPNmHJNQpc" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>

[^1]: Unstructured image, video, audio data is a different matter.

[^2]: Part of me feels like, if you're a relational database, you had one job, provide a robust, performant abstraction to tabular data. It feels code-smelly to have to use a totally different RDBMS engine for column-oriented vs row-oriented. SQL Server has had COLUMNSTORE for 10 years, even if big data folks don't consider it a true data warehouse. Maybe the traditional RDBMS will incorporate better column-oriented functionality and keep all the good stuff like transactions and query optimization. Or maybe everyone will have to query a [plethora of database engines](https://db-engines.com/en/ranking) and data lakes and we'll need a SQL layer on top of them all to complicate our lives with federated joins and distributed transactions. Oy. Possibly that's the niche that tools like [Trino](https://trino.io/) (f/k/a Presto) are looking to fill. Data seems like the mother of all leaky abstractions, you don't get great  performance if you don't know a fair amount about what's going on under the hood. DBAs and data engineers are maybe not going away. Or maybe it's so hard that a lot of people will just put the data into SaaS solutions optimized for their use case.

[^3]: If I was writing the article (maybe I will if I don't find a good one), I would benchmark vanilla pandas, Modin with different back-ends as an (almost) drop-in replacement for pandas, DuckDB SQL, and [Polars](https://towardsdatascience.com/getting-started-with-the-polars-dataframe-library-6f9e1c014c5c), which is a dataframe replacement for pandas written in Rust and using the Arrow in-memory dataframe data structure, and presumably not a fully drop-in replacement for pandas. Maybe R too.
