---
id: 7122
title: 'Numbers With Wings: A Modern Data Stack-In-A-Box'
date: 2023-01-09T01:01:01+00:00
author: Druce Vertes
layout: post
guid: /?p=7122
permalink: /2023/01/modern_data_stack
categories: datascience
tags: datascience

---
>*There are three kinds of people: those who can count, and those who can't. - Source unknown*
>
>*Not everything that counts can be counted, and not everything that can be counted counts. - Albert Einstein*
> ![NYC Subway entries](/assets/2023/mta_entries.png)

<!--more-->


Here is a work-in-progress mega-post on creating a highly performant pipeline for 'pretty big' data, on a single box, instance, laptop, or container. The first part is some general discussion / rants. Then we'll set up a stack to analyze MTA turnstile data, which is on the order of 25MB a week, 1GB per year in round numbers.

# 0. Why a data stack-in-a-box?

Some might say, it's not *big* data if it fits on single box, like my laptop. One definition of big data used to be, data that is too big to be reasonably transformed or analyzed on a single box.

But times have changed. You can fit *a lot* on a box. I have a 2019 Intel MacBook Pro with 32GB of RAM; the latest MacBook Pros have up to 96GB. Which is 'pretty big' data.  With the right stack, you can effectively analyze decades of MTA data down to individual turnstiles. 

Are clusters decreasingly relevant? Modern CPU architectures like [Zen](https://en.wikipedia.org/wiki/Zen_4) change many things, if not everything. When Google first came out, the part that really blew my mind was not just that it had an order-of-magnitude bigger index and better relevance. But when you ran a search, the results gave not just a URL, but *the excerpt on the page* that matched your search. Google was *caching the text of the entire Web in RAM*. A cluster farmed out your search to dozens of PCs (really some circuit boards zip-tied together), each of which kept a shard of the index in RAM, got the top-ranking result from each shard, also looked up the relevant page and located the relevant excerpt, then sorted all this and assembled it into a results page. In a fraction of a second. Google clusters were a radical new computing paradigm that expanded the realm of what was possible. 

Today the pendulum has swung the other way. We almost always run multiple virtual servers on a single physical server. It is *cheaper* to run a giant [AMD Epyc box](https://aws.amazon.com/blogs/aws/new-amazon-ec2-c6a-instances-powered-by-3rd-gen-amd-epyc-processors-for-compute-intensive-workloads/) with 192 cores and 384GB of RAM than equivalent compute on individual servers. You can even rent an [8-socket Intel box with 24 TB of RAM](https://aws.amazon.com/ec2/instance-types/high-memory/)! Big data, defined as tabular data too big to fit on a single box in RAM, hardly even exists outside Big Tech. Machines have grown even faster than tabular datasets.[^1]

Giant CPUs and SoCs change the way software is built. For super high performance lightweight microservices you may want to [pin each thread to a core](https://twitter.com/alexxubyte/status/1588203762945884160), keep everything in CPU cache, and never context-switch. As Moore's law flies into the sunset, expanding the envelope is more about giant dies for systems-on-a-chip (SoCs). That's [the secret](https://debugger.medium.com/why-is-apples-m1-chip-so-fast-3262b158cba2) of Apple's M1, it's not just a CPU, it also puts memory, I/O, GPU on a single chip for massive bandwidth. 

If you can reasonably fit your data on one box, the juice may not be worth the squeeze to make a cluster. Just leverage a big box with many cores, lots of RAM, and massive bus bandwidth. 

Vertical is the new horizontal. 

Hence, the data stack-in-a-box. 

# 1. Components of data pipelines and stacks

We can divide what the data stack-in-a-box will do into 6 tasks:

1. Data storage to manage the data and let us query it and aggregate it efficiently, taking full advantage of all the RAM and cores: a DBMS, data warehouse, data lake, lakehouse etc.

2. Tools to fetch data and ingest from REST APIs, CSVs, Parquet etc. via standard protocols. Built-in schemas for e.g. Zoom or Salesforce data and APIs would be nice.

3. A scheduler / orchestrator to check for new data and trigger jobs when needed: maybe on a schedule, maybe monitoring a local directory, maybe polling a remote directory, maybe receiving emails or other signals.

4. Tools to manage dependency graphs ([DAGs](https://en.wikipedia.org/wiki/Directed_acyclic_graph)) to trigger jobs like DB transformation / build, data quality, ML training, inference, notifications downstream when jobs succeed or fail.

5. Tools to munge data, extract, load, and transform (ELT).

6. Front end frameworks to create dataviz, reports, dashboards and end-user apps.

This is just one way to slice the salami. It might be simpler to break down into data warehouse, ELT, BI/front end. In practice especially the middle 2-5 can overlap but there are often multiple products. There are also [additional pieces](https://i.redd.it/pdnuk1r0yjf71.jpg), like [monitoring pipelines](https://www.acceldata.io/article/what-is-data-pipeline-monitoring) in production, [data quality](https://greatexpectations.io/), [data governance](https://www.collibra.com/us/en/products/data-governance). You can go [pretty deep](https://mattturck.wpenginepowered.com/wp-content/uploads/2021/12/2021-MAD-Landscape-v3.pdf). But if you've handled these concerns, you have a pretty good start.

An aside: Why ELT and not ETL? Earlier online analytic processing (OLAP) manifestations processed data into [cubes](https://en.wikipedia.org/wiki/OLAP_cube), sort of like a spreadsheet with a pivot table within a database, to batch pre-process aggregations and enable real-time drilldowns. Modern data stacks just keep data in raw form (or close to it) and leverage parallel processing to transform on the fly. We give each core a shard of the data in RAM. Then each node can do its part of aggregations and drilldowns and send the results to a controller for final compilations, [MapReduce](https://en.wikipedia.org/wiki/MapReduce) paradigm style. In contrast to a client/server paradigm which moves data around disks and network, we marry each slice of data with nearby dedicated compute. Thus ELT in contrast to old-school ETL. In practice, however, we usually end up with ETLT: extract; clean up a little; load to our data warehouse in a clean, but unaggregated form; do major aggregation on the fly without needing to determine up front what dimensions we might want to aggregate as in an OLAP cube.

How could we build a basic [modern data stack](https://www.getdbt.com/blog/future-of-the-modern-data-stack/) with [MTA turnstile source data](https://data.ny.gov/Transportation/Turnstile-Usage-Data-2020/py8k-a8wg) to power dashboards like the [NYS Comptroller site](https://www.osc.state.ny.us/reports/osdc/impact-covid-19-pandemic-subway-ridership-new-york-city) or [this](https://toddwschneider.com/dashboards/nyc-subway-turnstiles/) or [this](https://www.subwayridership.nyc/)?

One way is an enterprise-ish 'on-prem' approach (but typically in private or public cloud these days):
- Storage + compute: Spark cluster 
- Data pipeline management: Airflow, dbt, Fivetran, Stitch, Airbyte 
- Dataviz: PowerBI, Tableau, Superset
- Apps: Django web apps, Appsmith low-code

Another approach is 'cloud-native', using pay-as-you-go SaaS cloud services connected over public Inernet:
- Storage + compute: Cloud lakehouse like Snowflake or Databricks hosted Spark
- Cloud pipeline management: Astronomer hosted Airflow, Prefect, Dagster Cloud, dbt Cloud
- Cloud analytics service, Tableau Cloud, PowerBI SaaS, Preset hosted Superset
- Retool, Appsmith Cloud, with hosted Postgres to deliver end-user apps

With a cloud-native stack, SMBs can run their business more or less completely on SaaS services like Salesforce, Square, NetSuite, Workday, Mailchimp, Twilio, Zoom. Who needs devs and system admins and MSPs? When you want to build a workflow app, for instance to manage a big conference, create some Zoom meetings, send some emails, texts, Slack/Discord messages, and get paid, build a front end in low-code platforms like Retool to talk to all those SaaS services. And then Tableau or PowerBI SaaS for dashboards and reporting.

# 2. Our data stack-in-a-box: DuckDB, dbt, Superset

But I'm not going to build any of these heavyweight solutions, I want a lightweight stack to pull 'pretty large' data, aggregate along various dimensions, and display a dashboard on my MacBook or in a container, for a data stack-in-a-box. Tech we will leverage:

- [Duckdb](https://duckdb.org/): like SQLlite, but for column-oriented data. It's a lightweight module that does high-performance multithreaded aggregation using SQL. [Columnar databases are faster for OLAP than row-oriented OLTP databases like Postgres](https://loonytek.com/2017/05/04/why-analytic-workloads-are-faster-on-columnar-databases/).[^2] When we have e.g. millions of rows of e.g. double-digit columns, we usually get orders of magnitude improvement in size/speed using a columnar format like Parquet vs. CSV with binary storage and compression.

- [dbt](https://www.getdbt.com/blog/future-of-the-modern-data-stack/): the database build tool. From your data warehouse's perspective, dbt is simply a SQL client. But when you write your SQL scripts within the dbt framework, you get almost for free: DAG workflow; logging; self-documentation of every table's provenance; parameterized SQL templates for modularity and re-use, and the ability to point any script to dev / test / production environments. 

- [Superset](https://superset.apache.org): an open source version of Tableau or PowerBI to run a dashboard.

- [Meltano](https://meltano.com): a CLI to manage data pipelines, that can ingest data via [Singer taps](https://www.singer.io/#taps) and [Airbyte connectors](https://airbyte.com/connectors). It's the closest thing I've seen to an API of all APIs.

This works pretty well ([code here](https://github.com/druce/MTA)). It seems to be an order of magnitude faster than raw pandas on 'pretty big' data. It will analyze data fast, and it's a nice way to understand how the data sausage is made. It's awesomely cool, if you grew up in a world where Google didn't exist, to be able to compute Google-style on your laptop or a $20 Hetzner or AWS box. It's close to production-ready when the time comes to move from your laptop to 'real' enterprise infrastructure, whether SaaS or 'on-prem'. 

DuckDB has limitations. It is single-user. It writes to a single file. If your data does not fit in RAM on your single machine, it will fail. But if you sub Spark for DuckDB, this data-warehouse-in-a-box starts is a decent starting point for an enterprise stack.

But of course, if you are a normie business, just get Databricks (hosted Spark) or Snowflake (SQL data warehouse).  No offense to others but those are the dominant SaaS solutions. There are [a lot of ways](https://www.moderndatastack.xyz/stacks) to skin this cat. This dude even has a ['post-modern' data stack](https://blog.devgenius.io/modern-data-stack-demo-5d75dcdfba50). 

At this point, rolling your own modern data stack is for teams with scale. Or penniless startups and hobbyists who don't want to pay for SaaS services. Which is probably not most teams. Regardless, in the rest of this post, we stand up our own small environment for a data app.

# 3. Requirements and initial DuckDB ingestion

### Hardware

 - The size of the data you can process fast will scale with RAM.
 - The speed at which you can process data will scale with cores.
 - I am running on a 2019 MacBook Pro with 32GB and 8 cores. If you run on a cloud instance, I would recommend starting with something like [r6a.xlarge](https://aws.amazon.com/ec2/pricing/on-demand/): AMD CPU, 32GB RAM, 4 cores. 

### Software

 - I will run everything on my Mac in a [conda virtualenv](https://conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html).
 - Ubuntu 20.04 or any OS that runs Anaconda should work, Windows, Linux, or Mac.  The Amazon [Deep Learning AMI](https://docs.aws.amazon.com/dlami/latest/devguide/overview-conda.html) is nice for machine learning in general but we are not going to use ML or GPU.
 - Python 3.8, I don't expect all modules to be 100% debugged and tested on the latest Python 3.9 and Ubuntu 22.04 as of this writing.

### Install

 - [Clone the repo](https://github.com/druce/MTA) into a local directory.
~~~ shell
git clone https://github.com/druce/MTA
~~~
- Download and install the appropriate [Anaconda](https://www.anaconda.com/products/distribution) for your OS.

  ~~~ shell
  $ wget https://repo.anaconda.com/archive/Anaconda3-2022.10-MacOSX-x86_64.sh 
  $ ./Anaconda3-2022.10-MacOSX-x86_64.sh 
  $ conda init           # (optional) update .bashrc etc 
  $ conda update conda   # (optional) check for updates to base 
  ~~~
  
- Make a virtualenv and install DuckDB

  ~~~ shell
  $ conda create -n mds python=3.8
  $ conda activate mds
  $ pip install duckdb duckcli pandas 
  ~~~
  
- If using Jupyter, [add SQL magic for DuckDB](https://duckdb.org/docs/guides/python/jupyter.html) and add `mds` virtualenv as a Jupyter kernel
  
  ~~~ shell
  $ pip install duckdb-engine ipython-sql sqlalchemy ipykernel
  $ python -m ipykernel install --user 
  ~~~

- [Download](https://github.com/druce/MTA/blob/main/0-download_data.py) and [load](https://github.com/druce/MTA/blob/main/1-ingest_data.py) the data into a DuckDB database mta.db. 
~~~ shell
  $ python 0-download_data.py
  $ python 1-ingest_data.py
~~~

# 4. Pipeline overview

At this point you should be able to do `jupyter notebook` and open [MTA-DuckDB.ipynb](https://localhost:8888/notebooks/MTA/MTA-DuckDB.ipynb) . In a nutshell:

- Load raw data into DuckDB
  - We have 1 row per turnstile per station per approximately 4-hour period
- Do some basic cleanup 
  - Make clean unique turnstile and station labels each in a single column
  - We have cumulative turnstile counters, so we first-difference them after grouping by turnstile, to get net entries and exits by period
  - Fix large outliers where presumably maintenance was done and counters were reset, remove negative or very large entries/exits
- Compute aggregations like total entries by date, and visualize

# 5. dbt initial build / transform

### Install, configure, and run dbt

- Install
~~~ shell
  $ pip install dbt-duckdb
~~~

- Configure: edit dbt's profiles.yml config file to point to the directory where you cloned the MTA repo. Put it in your home directory under `~/.dbt/`. dbt  keeps these credentials and connection profiles here by default to avoid putting credentials in git repos.

- Run (these shell commands are in `build.sh`; edit your directory name)

~~~ shell
  $ # environment variables for python models that load data
  $ export BASEDIR="/Users/login/projects/MTA"
  $ export DBFILE="mta.db"
  $ export DATADIR="downloads"
  $ cd dbt_mta
  $ dbt debug  # check connection profile
  $ dbt seed   # load small static mapping tables from dbt/seeds
  $ dbt run    # download data and run build pipeline
~~~

- On my Mac, this takes about 4 minutes to run on 5GB of data, not including the HTTP download but including the initial DB ingestion from .txt files.
- For comparison, the first version of the analysis that I wrote in pure Pandas without DuckDB takes about 30 minutes. With [Modin](https://modin.readthedocs.io/en/stable/) multi-threaded dataframes as a drop-in replacement for Pandas, this is cut to about 20 minutes. The comparison is not a valid benchmark since the transforms are not the same. Someone should do a proper benchmark.[^3] But this should give an idea of the benefits of the modern data stack.

### Full dbt walkthrough

The above uses the repo's fully set-up dbt project. Let's build it step by step from scratch so you can see what goes on under the hood and get a quick tour of dbt. 

- Back up the existing dbt directory
~~~ shell
  $ mv dbt_mta dbt_mta.sav
~~~

- In the MTA directory, run the below to create the scaffold of the project:
~~~ shell
  $ dbt init dbt_mta
  $ cd dbt_mta 
  $ ls  # inspect contents
  # At a minimum we need profiles.yml in ~/.dbt, dbt_project.yml and models in models directory
  $ dbt debug
~~~
- You should see `All checks passed`. If not, debug the `.dbt/profiles.yml` and the `dbt_project.yml` so everything is in sync. 
- Set up seeds (small static tables)
~~~
  cd seeds
  cp ../../dbt_mta.sav/seeds/*.csv .
  cp ../../dbt_mta.sav/seeds/schema.yml .
  # maybe inspect those files
  cd ..
  dbt seed
  cd ..
  duckcli mta.db
  show tables
  select * from station_label_override
  exit
~~~
- You should see that 2 small tables have been created from the csv and schema files: `station_label_override` and `division_label_override`
- Now we create our models
~~~ shell
  cd dbt_mta/models
  cp ../../dbt_mta.sav/models/*.py .
  cp ../../dbt_mta.sav/models/*.sql .
  cp ../../dbt_mta.sav/models/*.yml .
  # maybe inspect those files
  cd ..
  dbt run --select download_data
  dbt run --select ingest_data
~~~
- dbt doesn't talk HTTP or load data natively, it expects raw data to be loaded into our database. For now, we have a couple of python scripts to download and ingest, as models in the models directory. These `dbt run` commands just run the python scripts within the dbt context. In the future we may want to use [Dagster](https://dagster.io/blog/duckdb-data-lake#-the-limitations-of-duckdb) for data download and ingestion. Airflow is also very popular to manage pipelines, but it is complex, and more modern alternatives seem to be gaining traction.
- .sql files in the models directory run sql queries to create tables according to the model name
- .sql models refer to existing tables with `{% raw %}{{ref}}{% endraw %}` Jinja tags, creating a DAG defining the order of execution.
- To run the whole pipeline type:
~~~ shell
dbt run
~~~
- models/config.yml has model schemas and documentation
- To view documentation run:
~~~ shell
  $ dbt docs generate
  $ dbt docs serve
~~~
- This allows you to browse the project documentation, and if you click 'View Lineage Graph' at bottom right on the Database tab, you should see something like:
![dbt lineage graph](/assets/2023/dbt_graph.png)

- There is more to dbt:
  - [dbt Cloud](https://www.getdbt.com/signup/) - dashboard for all your dbt projects
  - [Jinja](https://jinja.palletsprojects.com/en/3.1.x/) to parameterize SQL
  - macros
  - tests for data quality
  - pre-hooks, post-hooks before and after loading tables.
  
dbt is probably something everyone should try for non-trivial pipelines. There isn't anything wrong with writing a pipeline based on a sequence of python scripts. But over time you will tend to standardize your script and turn it into a template. Then at some point maybe you will want to make a change everywhere. You might end up with a lot of scripts, and some data engineers who are better at sql than python. dbt provides a standard framework out of the box that gives you a lot of stuff almost for free: documentation, logging, modular templates. It's lightweight, there's not much lock-in, [most places](https://www.moderndatastack.xyz/stacks) use it.

# 6. Superset dashboard

I install Superset into my Mac conda virtualenv. The install varies a bit by platform, see [the instructions](https://superset.apache.org/docs/installation/installing-superset-from-scratch/). I found I had to pin a couple of module versions, which was a drag to figure out. You can install [a Docker image](https://superset.apache.org/docs/installation/installing-superset-using-docker-compose/), which should be straighforward if you have Docker, but just mount the directory where the mta.db DuckDB database lives.

~~~ shell
# prereqs
# install or upgrade Xcode 
xcode-select --install

# install or upgrade Homebrew https://brew.sh/
brew upgrade
brew install readline pkg-config libffi openssl mysql postgres

# make virtualenv
conda create --name superset python=3.8
conda activate superset

pip install --upgrade setuptools pip

export LDFLAGS="-L$(brew --prefix openssl)/lib"
export CFLAGS="-I$(brew --prefix openssl)/include"

# install
pip install apache-superset

# at this writing these versions must be pinned
pip install --force-reinstall cryptography==38.0.4
pip install --force-reinstall wtforms==2.3.3
pip install duckdb-engine Pillow

# Create an admin user in your metadata database (use `admin` as username to be able to load the examples)
export FLASK_APP=superset
superset fab create-admin

# Create default roles and permissions
superset init

# To start a development web server on port 8088, use -p to bind to another port
superset run -p 8088 --with-threads --reload --debugger
~~~

After a fair amount of wrangling, I get a dashboard that looks like this:

![Superset dashboard](/assets/2023/mta-dashboard.jpg)

Superset is a poor man's Tableau. It's powerful but complex and quirky. It was not as straightforward to create this as I expected. That being said, we can filter by a date range and see a comparison with pre-pandemic and pandemic periods, we can filter by Manhattan below 63 St, or outside this area, or everything. We can filter by day of the week and see a comparison with those days during pandemic and pre-pandemic. We can sort any table column. 

I feel like this dashboard could use a little more work, formatting, color-code maps by entries/growth etc. But I've spent enough time and I've seen enough. For real-time dashboards I might go with Grafana which is known for real-time. (Superset is designed for real-time updates via Apache Druid). Since I like Plotly and can code up nice charts in Python, I might go with Plotly Dash for dashboards. For point-and-click BI I might go with Tableau or PowerBI. Superset is not going to put NY Times dataviz to shame. But Superset is pretty good for a lot of purposes and has a lot of potential.

Still to come:

- part 7, conclusion, speedrun other stuff, like Meltano, Airflow, Great Expectations data quality

In the meantime, may all your pipelines be unclogged:

<iframe width="560" height="315" src="https://www.youtube.com/embed/jGw1ZPgyEaE" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>

### The Bongos: Numbers With Wings
<iframe width="560" height="315" src="https://www.youtube.com/embed/XdPNmHJNQpc" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>

[^1]: Unstructured image, video, audio data is a different matter.

[^2]: Part of me feels like, if you're a relational database, you had one job, provide a robust, performant abstraction to tabular data. It feels code-smelly to have to use a totally different RDBMS engine for column-oriented vs row-oriented. SQL Server has had COLUMNSTORE for 10 years, even if big data folks don't consider it a true data warehouse. Maybe the traditional RDBMS will incorporate better column-oriented functionality and keep all the good stuff like transactions and query optimization. Or maybe everyone will have to query a [plethora of database engines](https://db-engines.com/en/ranking) and data lakes and we'll need a SQL layer on top of them all to complicate our lives with federated joins and distributed transactions. Oy. Possibly that's the niche that tools like [Trino](https://trino.io/) (f/k/a Presto) are looking to fill. Data seems like the mother of all leaky abstractions, you don't get really good performance if you don't know a fair amount about what's going on under the hood. DBAs and data engineers are maybe not going away. Or maybe it's so hard that a lot of people will just put the data into SaaS solutions optimized for their use case.

[^3]: If I was writing the article (maybe I will if I don't find a good one), I would benchmark vanilla pandas, Modin with different back-ends as an (almost) drop-in replacement for pandas, DuckDB SQL, and [Polars](https://towardsdatascience.com/getting-started-with-the-polars-dataframe-library-6f9e1c014c5c), which is a dataframe replacement for pandas which is written in Rust and using the Arrow in-memory dataframe data structre, and presumably not really a drop-in replacement for pandas. Maybe R too.

