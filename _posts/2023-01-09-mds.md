---
id: 7122
title: 'Numbers With Wings: A Modern Data Stack-In-A-Box'
date: 2023-01-09T01:01:01+00:00
author: Druce Vertes
layout: post
guid: /?p=7121
permalink: /2023/01/modern_data_stack
categories: politics
tags: politics

---
>*Not everything that counts can be counted, and not everything that can be counted counts. - Albert Einstein*

>*There are three kinds of people: those who can count, and those who can't.*

With apologies to [The Bongos](https://www.youtube.com/watch?v=XdPNmHJNQpc)

This is going to be a mega-post on creating a highly performant pipeline for 'pretty big' data, on your laptop, or maybe on a cloud inatance or container. The first part is some general discussion / rants. Then we'll set up a stack to analyze MTA turnstile data, which is on the order of 25MB a week, 1GB per year in round numbers.

You might say, it's not big data if it fits on a laptop. One definition of big data used, data that is too big to be handled on a single box.

But times have changed. I have a 2019 Intel MacBook Pro with 32GB of RAM. So with the right stack, I can effectively analyze maybe 20 years of data down to individual turnstiles. Which is 'pretty big data.

Are clusters decreasingly relevant? Modern CPU architectures like [Zen](https://en.wikipedia.org/wiki/Zen_4) change a lot. When Google first came out, the part that truly blew my mind was not just that it had an order-of-magnitude bigger index and better relevance. But when you ran a search, the results gave not just a URL, but *the excerpt on the page* that matched your search. Google was *caching the entire Web in RAM*. A cluster farmed out your search to dozens of PCs (really some circuit boards zip-tied together), each of which kept a shard of the index in RAM, got the top-ranking result from each shard, also got the relevant page and searched for the relevant excerpt, then sorted all this and assembled it into a results page. In a fraction of a second. Google clusters were an unprecedented new paradigm in computing. 

Today the pendulum has swung the other way and we almost always run multiple virtual servers on a single CPU. It is usually *cheaper* to run a giant [AMD Epyc box](https://aws.amazon.com/blogs/aws/new-amazon-ec2-c6a-instances-powered-by-3rd-gen-amd-epyc-processors-for-compute-intensive-workloads/) with 192 cores and 384GB of RAM than equivalent compute on individual servers. You can even rent an [8-socket Intel box with 24 TB of RAM](https://aws.amazon.com/ec2/instance-types/high-memory/)! Big data, in the sense of data too big to fit on a single box in RAM, hardly even exists outside Big Tech. Machines have grown even faster than tabular datasets. Giant CPUs also change the way software is built, for super high performance services you may want to [pin each thread to a core](https://twitter.com/alexxubyte/status/1588203762945884160), keep everything in on-CPU cache, and never context-switch . As Moore's law sunsets, computing is more about giant dies for systems-on-a-chip. That's the secret of Apple's M1, it's not a CPU, it puts everything on 1 chip for massive bandwidth. 

For a lot of data analysis, the juice is not worth the squeeze to make a cluster, if you can properly take advantage of a big box's memory, many cores, and massive bus bandwidth. For a lot of apps, vertical is the new horizontal. 

We need 6 things to run a data pipeline:

1. Data storage to manage the data and let us query it and aggregate it efficiently, taking full advantage of all the RAM and cores: a DBMS, data warehouse, data lake, lakehouse etc.

2. A scheduler / orchestrator to check for new data and trigger jobs when data is available: maybe on a schedule, maybe monitoring a local directory, maybe polling a remote directory, maybe receiving emails or other signals.

3. Tools to fetch data from REST APIs, CSVs, etc via standard protocols. Built-in schemas for e.g. Zoom or Salesforce data and APIs would be nice.

4. Tools to manage dependency graphs ([DAGs](https://en.wikipedia.org/wiki/Directed_acyclic_graph)) to trigger jobs like DB transformation / build, data quality, ML training, inference, notifications downstream when jobs succeed or fail.

5. Tools to munge data, extract, load, and transform (ELT). (1) (2)

6. Front end frameworks to create dataviz, reports, dashboards and end-user apps

This is just one way to slice the salami. It might be simpler to break down into ELT, data warehouse, BI/front end. In practice 1, 2, and 3 can overlap but there are usually multiple products. There are also [additional pieces](https://i.redd.it/pdnuk1r0yjf71.jpg), like [monitoring pipelines](https://www.acceldata.io/article/what-is-data-pipeline-monitoring) in production, [data quality](https://greatexpectations.io/), [data governance](https://www.collibra.com/us/en/products/data-governance). You can go [pretty deep](https://mattturck.wpenginepowered.com/wp-content/uploads/2021/12/2021-MAD-Landscape-v3.pdf). But this is a reasonable starting framework.

An aside: Why ELT and not ETL? Earlier OLAP manifestations processed data into cubes, sort of like spreadsheets within a database, to batch pre-process aggregations and enable real-time drilldowns. Modern data stacks keep data in close to raw form and leverage parallel processing to transform on the fly. We give each core a shard of the data in RAM. In contrast to a client/server paradigm which moves data around disks and network, we each slice of data together with its dedicated compute. Then each node can do its part of aggregations and drilldowns and send the results to a controller for final compilations, [MapReduce](https://en.wikipedia.org/wiki/MapReduce) paradigm style. Thus ELT in contrast to old-school ETL. In practice, however, we usally end up with ETLT: extract; clean up a little; load to our data warehouse in a clean, but minimally transformed form; do major aggregation on the fly without needing to determine up front what aggregation we need.

How can we build a basic [modern data stack](https://www.getdbt.com/blog/future-of-the-modern-data-stack/) with [MTA turnstile source data](https://data.ny.gov/Transportation/Turnstile-Usage-Data-2020/py8k-a8wg) to do something like [this](https://toddwschneider.com/dashboards/nyc-subway-turnstiles/) or [this](https://www.subwayridership.nyc/)?

One way is an enterprise-ish 'on-prem' approach (but typically in private or public cloud these days):
- Storage + compute: Spark cluster 
- Pipeline management: Airflow, dbt, Fivetran, Stitch, Airbyte 
- Dataviz: PowerBI, Tableau, Superset
- Apps: Django web apps, Appsmith low-code

Another approach is to 'cloud-native', using pay-as-you-go SaaS cloud services
- Storage + compute: Cloud lakehouse like Snowflake or Databricks hosted Spark
- Cloud pipeline management: Astronomer hosted Airflow, Prefect, Dagster Cloud, dbt Cloud
- Cloud analytics service, Tableau Cloud, PowerBI SaaS, Preset hosted Superset
- Retool, Appsmith Cloud, with hosted Postgres to deliver end-user apps

With a cloud-native stack, SMBs can run their business more or less completely on SaaS services like Salesforce, Square, NetSuite, Workday, Mailchimp, Twilio, Zoom. Who needs devs and system admins and MSPs? When you want to build a workflow app, for instance to organize a big conference, create some Zoom meetings, send some emails and text messages, and get paid, build it in Retool to talk to all those SaaS services. 

But I'm not going to build any of these heavyweight solutions, I want a lightweight stack to pull 'pretty large' data, run an analysis, and display a dashboard on my MacBook or in a container, for a data stack-in-a-box. Tech we will leverage:

- [Duckdb](https://duckdb.org/) is like SQLlite, but for column-oriented data. It's a lightweight package that does high-performance multithreaded aggregation using SQL. [Columnar databases are faster for OLAP](https://loonytek.com/2017/05/04/why-analytic-workloads-are-faster-on-columnar-databases/) . We can usually get orders of magnitude improvement in size/speed using a columnar format like Parquet vs. CSV with binary storage and compression.

- [dbt](https://www.getdbt.com/blog/future-of-the-modern-data-stack/), the database build tool, from your data warehouse's perspective, is simply a SQL client. But when you write your SQL scripts within the dbt framework, you get almost for free: DAG workflow; logging; self-documentation of every table's provenance; parameterized SQL templates for modularity and re-use, and the ability to point any script to dev / test / production environments. 

- [Superset](https://superset.apache.org) is an open source version of Tableau or PowerBI to run a dashboard.

- [Meltano](https://meltano.com), an open source project from Gitlab, is a CLI to manage data pipelines, that can ingest data via [Singer taps](https://www.singer.io/#taps) and [Airbyte connectors](https://airbyte.com/connectors). It's the closest thing I've seen to an API of all APIs.

This works pretty well (code here). It's an order of magnitude faster than raw pandas. It's also more production-ready if you want to move for your laptop to 'real' servers. It will analyze data fast, and it's a nice way to understand how the data sausage is made. It's undeniably cool, if you grew up in a world where Google didn't exist, to be able to compute Google-style on your laptop or a $20 Hetzner or AWS box. And you can do real work fast with any data source.

But of course, if you are a normie business, just get Databricks (hosted Spark) or Snowflake (SQL data warehouse). I mean, there are [a lot of ways](https://www.moderndatastack.xyz/stacks) to skin this cat. No offense to others but those are the dominant SaaS solutions. 

If you want to roll your own, there are [a lot of ways](https://www.moderndatastack.xyz/stacks) to skin this cat. This dude even has a ['post-modern' data stack](https://blog.devgenius.io/modern-data-stack-demo-5d75dcdfba50). But at this point, rolling your own modern data stack is for teams with scale which is maybe not most teams. Or penniless startups and hobbyists who want to see how the sausage is made and put up a small container with a data app, which is what we did here.

On the other hand, if you sub Spark for DuckDB, this hobbyist data-warehouse-in-a-box starts to look like it could be a decent model for an enterprise stack.

Still to come over next few weeks:

- part 2, DuckDB and ingestion walk-through
- part 3, dbt and load walkthrough
- part 4, Superset dashboarding walk-through
- part 5, conclusion, speedrun other stuff, like Meltano, Airflow, Great Expectations data quality

									 (3) footnote: part of me feels like, if you're a relational database, you had one job, provide a robust, performant abstraction to tabular data. It feels code-smelly to have to use a totally different RDBMS engine for column-oriented vs row-oriented. SQL Server has had COLUMNSTORE for 10 years, even if big data folks don't consider it a true data warehouse. Maybe the traditional RDBMS will incorporate better column-oriented functionality and keep all the good stuff like transactions and query optimization. Or maybe everyone will have to query a [plethora of database engines](https://db-engines.com/en/ranking) and data lakes and we'll need a SQL layer on top of them all to complicate our lives with federated joins and distributed transactions. Oy. Possibly that's the niche that tools like [Trino](https://trino.io/) (f/k/a Presto) are looking to fill. Data seems like the mother of all leaky abstractions, you don't get good performance if you don't know a fair amount about what's going on under the hood. DBAs and data engineers are maybe not going away. Or maybe it's so hard that a lot of people will just put the data into managed cloud services optimized for their use case.

