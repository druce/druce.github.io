---
id: 7122
title: 'Numbers With Wings: A Modern Data Stack-In-A-Box'
date: 2023-01-09T01:01:01+00:00
author: Druce Vertes
layout: post
guid: /?p=7122
permalink: /2023/01/modern_data_stack
categories: datascience
tags: datascience

---
>*There are three kinds of people: those who can count, and those who can't. - Source unknown*
>
>*Not everything that counts can be counted, and not everything that can be counted counts. - Albert Einstein*
<!--more-->

# The Bongos: Numbers With Wings
<iframe width="560" height="315" src="https://www.youtube.com/embed/XdPNmHJNQpc" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>

Here is a work-in-progress mega-post on creating a highly performant pipeline for 'pretty big' data, on a single box, instance, or laptop. The first part is some general discussion / rants. Then we'll set up a stack to analyze MTA turnstile data, which is on the order of 25MB a week, 1GB per year in round numbers.

# 0. Why a data stack-in-a-box?

You might say, it's not big data if it fits on single box like my laptop. One definition of big data used to be, data that is too big to be handled on a single box.

But times have changed. I have a 2019 Intel MacBook Pro with 32GB of RAM. So with the right stack, I can effectively analyze maybe 20 years of MTA data down to individual turnstiles. Which is 'pretty big' data.

Are clusters decreasingly relevant? Modern CPU architectures like [Zen](https://en.wikipedia.org/wiki/Zen_4) change many things, if not everything. When Google first came out, the part that truly blew my mind was not just that it had an order-of-magnitude bigger index and better relevance. But when you ran a search, the results gave not just a URL, but *the excerpt on the page* that matched your search. Google was *caching the text of the entire Web in RAM*. A cluster farmed out your search to dozens of PCs (really some circuit boards zip-tied together), each of which kept a shard of the index in RAM, got the top-ranking result from each shard, also looked up the relevant page and located the relevant excerpt, then sorted all this and assembled it into a results page. In a fraction of a second. Google clusters were an unprecedented new paradigm in computing. 

Today the pendulum has swung the other way and we almost always run multiple virtual servers on a single physical server. It is usually *cheaper* to run a giant [AMD Epyc box](https://aws.amazon.com/blogs/aws/new-amazon-ec2-c6a-instances-powered-by-3rd-gen-amd-epyc-processors-for-compute-intensive-workloads/) with 192 cores and 384GB of RAM than equivalent compute on individual servers. You can even rent an [8-socket Intel box with 24 TB of RAM](https://aws.amazon.com/ec2/instance-types/high-memory/)! Big data, in the sense of (tabular) data too big to fit on a single box in RAM, hardly even exists outside Big Tech. Machines have grown even faster than tabular datasets.[^1]

Giant CPUs even change the way software is built. For super high performance services you may want to [pin each thread to a core](https://twitter.com/alexxubyte/status/1588203762945884160), keep everything in on-CPU cache, and never context-switch. As Moore's law flies into the sunset, expanding the envelope is more about giant dies for systems-on-a-chip. That's [the secret](https://debugger.medium.com/why-is-apples-m1-chip-so-fast-3262b158cba2) of Apple's M1, it's not a CPU, it puts everything on 1 chip for massive bandwidth. 

For a lot of data analysis, the juice is not worth the squeeze to make a cluster, if you can properly take advantage of a big box's memory, many cores, and massive bus bandwidth. For a lot of apps, vertical is the new horizontal. 

Hence, the data stack-in-a-box. 

# 1. Components of data pipelines and stacks

We can divide what the data stack-in-a-box will do into 6 tasks:

1. Data storage to manage the data and let us query it and aggregate it efficiently, taking full advantage of all the RAM and cores: a DBMS, data warehouse, data lake, lakehouse etc.

2. Tools to fetch data from REST APIs, CSVs, etc via standard protocols. Built-in schemas for e.g. Zoom or Salesforce data and APIs would be nice.

3. A scheduler / orchestrator to check for new data and trigger jobs when data is available: maybe on a schedule, maybe monitoring a local directory, maybe polling a remote directory, maybe receiving emails or other signals.

4. Tools to manage dependency graphs ([DAGs](https://en.wikipedia.org/wiki/Directed_acyclic_graph)) to trigger jobs like DB transformation / build, data quality, ML training, inference, notifications downstream when jobs succeed or fail.

5. Tools to munge data, extract, load, and transform (ELT).

6. Front end frameworks to create dataviz, reports, dashboards and end-user apps.

This is just one way to slice the salami. It might be simpler to break down into data warehouse, ELT, BI/front end. In practice especially the middle 2-5 can overlap but there are usually multiple products. There are also [additional pieces](https://i.redd.it/pdnuk1r0yjf71.jpg), like [monitoring pipelines](https://www.acceldata.io/article/what-is-data-pipeline-monitoring) in production, [data quality](https://greatexpectations.io/), [data governance](https://www.collibra.com/us/en/products/data-governance). You can go [pretty deep](https://mattturck.wpenginepowered.com/wp-content/uploads/2021/12/2021-MAD-Landscape-v3.pdf). But if you've handled these concerns, you have a pretty good start.

An aside: Why ELT and not ETL? Earlier OLAP manifestations processed data into [cubes](https://en.wikipedia.org/wiki/OLAP_cube), sort of like a spreadsheet with a pivot table within a database, to batch pre-process aggregations and enable real-time drilldowns. Modern data stacks just keep data in raw form (or close to it) and leverage parallel processing to transform on the fly. We give each core a shard of the data in RAM. Then each node can do its part of aggregations and drilldowns and send the results to a controller for final compilations, [MapReduce](https://en.wikipedia.org/wiki/MapReduce) paradigm style. In contrast to a client/server paradigm which moves data around disks and network, we marry each slice of data with nearby dedicated compute. Thus ELT in contrast to old-school ETL. In practice, however, we usually end up with ETLT: extract; clean up a little; load to our data warehouse in a clean, but minimally transformed form; do major aggregation on the fly without needing to determine up front what dimensions we might want to aggregate.

How can we build a basic [modern data stack](https://www.getdbt.com/blog/future-of-the-modern-data-stack/) with [MTA turnstile source data](https://data.ny.gov/Transportation/Turnstile-Usage-Data-2020/py8k-a8wg) to power dashboards like [this](https://toddwschneider.com/dashboards/nyc-subway-turnstiles/) or [this](https://www.subwayridership.nyc/)?

One way is an enterprise-ish 'on-prem' approach (but typically in private or public cloud these days):
- Storage + compute: Spark cluster 
- Pipeline management: Airflow, dbt, Fivetran, Stitch, Airbyte 
- Dataviz: PowerBI, Tableau, Superset
- Apps: Django web apps, Appsmith low-code

Another approach is 'cloud-native', using pay-as-you-go SaaS cloud services connected over public Inernet:
- Storage + compute: Cloud lakehouse like Snowflake or Databricks hosted Spark
- Cloud pipeline management: Astronomer hosted Airflow, Prefect, Dagster Cloud, dbt Cloud
- Cloud analytics service, Tableau Cloud, PowerBI SaaS, Preset hosted Superset
- Retool, Appsmith Cloud, with hosted Postgres to deliver end-user apps

With a cloud-native stack, SMBs can run their business more or less completely on SaaS services like Salesforce, Square, NetSuite, Workday, Mailchimp, Twilio, Zoom. Who needs devs and system admins and MSPs? When you want to build a workflow app, for instance to manage a big conference, create some Zoom meetings, send some emails, texts, Slack/Discord messages, and get paid, build it in Retool to talk to all those SaaS services. 

# 2. Our data stack-in-a-box: DuckDB, dbt, Superset

But I'm not going to build any of these heavyweight solutions, I want a lightweight stack to pull 'pretty large' data, run an analysis, and display a dashboard on my MacBook or in a container, for a data stack-in-a-box. Tech we will leverage:

- [Duckdb](https://duckdb.org/): like SQLlite, but for column-oriented data. It's a lightweight module that does high-performance multithreaded aggregation using SQL. [Columnar databases are faster for OLAP than row-oriented OLTP databases like Postgres](https://loonytek.com/2017/05/04/why-analytic-workloads-are-faster-on-columnar-databases/).[^2] We can usually get orders of magnitude improvement in size/speed using a columnar format like Parquet vs. CSV with binary storage and compression.

- [dbt](https://www.getdbt.com/blog/future-of-the-modern-data-stack/): the database build tool. From your data warehouse's perspective, dbt is simply a SQL client. But when you write your SQL scripts within the dbt framework, you get almost for free: DAG workflow; logging; self-documentation of every table's provenance; parameterized SQL templates for modularity and re-use, and the ability to point any script to dev / test / production environments. 

- [Superset](https://superset.apache.org): an open source version of Tableau or PowerBI to run a dashboard.

- [Meltano](https://meltano.com): a CLI to manage data pipelines, that can ingest data via [Singer taps](https://www.singer.io/#taps) and [Airbyte connectors](https://airbyte.com/connectors). It's the closest thing I've seen to an API of all APIs.

This works pretty well ([code here](https://github.com/druce/MTA)). It's an order of magnitude faster than raw pandas on 'pretty big' data. It will analyze data fast, and it's a nice way to understand how the data sausage is made. It's awesomely cool, if you grew up in a world where Google didn't exist, to be able to compute Google-style on your laptop or a $20 Hetzner or AWS box. It's close to production-ready when the time comes to move from your laptop to 'real' enterprise infrastructure, whether SaaS or 'on-prem'. If you sub Spark for DuckDB, this data-warehouse-in-a-box starts is a decent starting point for an enterprise stack.

But of course, if you are a normie business, just get Databricks (hosted Spark) or Snowflake (SQL data warehouse).  No offense to others but those are the dominant SaaS solutions. There are [a lot of ways](https://www.moderndatastack.xyz/stacks) to skin this cat. This dude even has a ['post-modern' data stack](https://blog.devgenius.io/modern-data-stack-demo-5d75dcdfba50). 

At this point, rolling your own modern data stack is for teams with scale. Or penniless startups and hobbyists who don't want to pay for SaaS services. Which is probably most teams. In the rest of this post, we stand up a small environment for a data app.

# 3. Requirements and initial DuckDB ingestion

### Hardware

 - The size of the data you can process fast will scale with RAM.
 - The speed at which you can process data will scale with cores.
 - I am running on a 2019 MacBook Pro with 32GB and 8 cores. If you run on a cloud instance, I would recommend starting with something like [r6a.xlarge](https://aws.amazon.com/ec2/pricing/on-demand/): AMD CPU, 32GB RAM, 4 cores. 

### Software

 - I will run everything on my Mac in a [conda virtualenv](https://conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html).
 - Ubuntu 20.04 or any OS that runs Anaconda should work, Windows, Linux, or Mac.  The Amazon [Deep Learning AMI](https://docs.aws.amazon.com/dlami/latest/devguide/overview-conda.html) is nice for machine learning in general but we are not going to use ML or GPU.
 - Python 3.8, I don't expect all modules to be 100% debugged and tested on the latest Python 3.9 and Ubuntu 22.04 as of this writing.

### Install

 - [Clone the repo](https://github.com/druce/MTA) into a local directory.
~~~ shell
git clone https://github.com/druce/MTA
~~~
- Download and install the appropriate [Anaconda](https://www.anaconda.com/products/distribution) for your OS.

  ~~~ shell
  $ wget https://repo.anaconda.com/archive/Anaconda3-2022.10-MacOSX-x86_64.sh 
  $ ./Anaconda3-2022.10-MacOSX-x86_64.sh 
  $ conda init
  $ conda update conda
  ~~~
  
- Make a virtualenv and install DuckDB

  ~~~ shell
  $ conda create -n mds python=3.8
  $ conda activate mds
  $ pip install duckdb 
  ~~~
  
- If using Jupyter, [add SQL magic for DuckDB](https://duckdb.org/docs/guides/python/jupyter.html) and add `mds` virtualenv as a Jupyter kernel
  
  ~~~ shell
  $ pip install duckdb-engine ipython-sql sqlachemy ipykernel
  $ python -m ipykernel install --user 
  ~~~

- [Download](https://github.com/druce/MTA/blob/main/0-download_data.py) and [load](https://github.com/druce/MTA/blob/main/1-ingest_data.py) the data into a DuckDB database mta.db. 
~~~ shell
  $ python 0-download_data.py
  $ python 1-ingest_data.py
~~~

# 4. Overview

At this point you should be able to do `jupyter notebook` and open [MTA-DuckDB.ipynb](https://localhost:8888/notebooks/MTA/MTA-DuckDB.ipynb) . In a nutshell:

- Load raw data into DuckDB
- Do some basic cleanup 
  - We have 1 row per turnstile per station per approximately 4-hour period
  - Make clean turnstile and station labels each in a single column
  - We have cumulative turnstile counters, so we first-difference them after grouping by turnstile, to get net oentries and exits by period
  - Fix outliers where presumably maintenance was done and counters were reset, so we have negative or very large entries/exits
- Compute some aggregations and run some charts

# 5. dbt initial build / transform

### Install, configure, and run dbt

- Install
~~~ shell
  $ pip install dbt-duckdb
~~~

- Configure: edit dbt's profiles.yml config file to point to the directory where you cloned the repo. Put it in your home directory under `~/.dbt/`. dbt  keeps these credentials and connection profiles here by default to avoid risk of putting credentials in git repos.

- Run

~~~ shell
  $ cd dbt
  $ dbt debug  # check connection profile
  $ dbt run
~~~

- On my Mac, this takes about 4 minutes to run on 5GB of data, including the initial DB load from .txt files.
- For comparison, the first version of the analysis that I wrote in pure Pandas without DuckDB takes about 30 minutes. With [Modin](https://modin.readthedocs.io/en/stable/) multi-threaded dataframes as a drop-in replacement for Pandas, this is cut to about 20 minutes. The comparison is not a strictly valid benchmark since the transforms are not exactly the same, I cleaned a lot of things up as I want along. But it should give an idea of the benefits of the modern data stack.

### Full dbt walkthrough

- initial dbt configure, dbt debug
- seeds, dbt seed
- python models, dbt run
- sql models, dbt run
- docs, dbt generate, dbt serve

Still to come

- part 6, Superset dashboarding walk-through
- part 7, conclusion, speedrun other stuff, like Meltano, Airflow, Great Expectations data quality

In the meantime, may all your pipelines be unclogged:

<iframe width="560" height="315" src="https://www.youtube.com/embed/jGw1ZPgyEaE" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>

[^1]: Unstructured image, video, audio data is a different matter.

[^2]: Part of me feels like, if you're a relational database, you had one job, provide a robust, performant abstraction to tabular data. It feels code-smelly to have to use a totally different RDBMS engine for column-oriented vs row-oriented. SQL Server has had COLUMNSTORE for 10 years, even if big data folks don't consider it a true data warehouse. Maybe the traditional RDBMS will incorporate better column-oriented functionality and keep all the good stuff like transactions and query optimization. Or maybe everyone will have to query a [plethora of database engines](https://db-engines.com/en/ranking) and data lakes and we'll need a SQL layer on top of them all to complicate our lives with federated joins and distributed transactions. Oy. Possibly that's the niche that tools like [Trino](https://trino.io/) (f/k/a Presto) are looking to fill. Data seems like the mother of all leaky abstractions, you don't get really good performance if you don't know a fair amount about what's going on under the hood. DBAs and data engineers are maybe not going away. Or maybe it's so hard that a lot of people will just put the data into SaaS solutions optimized for their use case.

