---
id: 5840
title: Rethinking the marketplace of ideas
date: 2017-03-30T22:06:24+00:00
author: Druce Vertes, CFA
layout: post
guid: /?p=5840
permalink: /2017/03/rethinking-the-marketplace-of-ideas/
medium_post:
  - 'O:11:"Medium_Post":11:{s:16:"author_image_url";s:68:"https://cdn-images-1.medium.com/fit/c/200/200/0*tLekueVp7unnAXxY.jpg";s:10:"author_url";s:25:"https://medium.com/@druce";s:11:"byline_name";N;s:12:"byline_email";N;s:10:"cross_link";s:2:"no";s:2:"id";s:12:"df0189a52a14";s:21:"follower_notification";s:3:"yes";s:7:"license";s:19:"all-rights-reserved";s:14:"publication_id";s:2:"-1";s:6:"status";s:6:"public";s:3:"url";s:74:"https://medium.com/@druce/rethinking-the-marketplace-of-ideas-df0189a52a14";}'
post-views:
  - "4180"
dsq_thread_id:
  - "5682275309"
categories:
  - Uncategorized
---
I was recently listening to Fred Wilson and Howard Lindzon talk about, among others, news sources and curation, which is a topic dear to my heart. (Which I wrote about before [here](/2016/11/on-fake-news-market-designs-and-the-fascistlibertarian-nexus/) and [here](/2017/02/come-back-kelly-evans-well-be-good-i-promise/)).

Curation is hard, people tend to fall into confirmation bias circle jerks, and we are in a crisis of media legitimacy. (More generally, the legitimacy of the establishment and objective reality.)

If you think the moon is made of green cheese, you have a problem. If a lot of people think the moon is made of green cheese, society has a much bigger problem. (Or that climate change is a hoax invented by the Chinese, or that vaccinations cause autism, etc.)

The marketplace for ideas is faltering. I&#8217;m not sure if it&#8217;s only the arrival of the unwashed masses, Facebook likes, adverse selection, filtering tools that aren&#8217;t fit for purpose, or manipulation and &#8216;fake news.&#8217;

What seems to have gone missing is the sharing ethic that existed when the blogosphere and Twittersphere and social bookmarking were young.

What works to surface quality content is human curation, like [Memeorandum](http://www.memeorandum.com/), [Abnormal Returns](https://abnormalreturns.com/), and an engaged community that actively promotes and shares quality and feels that it&#8217;s important&#8230; like [AVC](http://avc.com/), [Hacker News](https://news.ycombinator.com/), the better [subreddits](https://www.reddit.com/r/Economics/), [Open Culture](https://twitter.com/openculture), [Arts and Letters Daily](http://www.aldaily.com/), [Brain Pickings](https://www.brainpickings.org/), etc.

[Ray Dalio](http://www.businessinsider.com/ray-dalio-interview-henry-blodget-1-2017) seems to think a new market design with some kind of regulatory component is needed. I don&#8217;t know what he has in mind, but one could imagine a self-regulatory body that agrees on journalistic standards like [CFA ethics](https://www.cfainstitute.org/ethics/codes/ethics/Pages/index.aspx): clearly separating fact from opinion, having a reasonable basis for any statement of fact or conclusion, showing your sources and investigative processes, promptly correcting errors, etc.

And then people and media sources that meet those standards get a seal of approval, and it investigates violations, censures or even expels people and organizations that don&#8217;t meet the standard. Sort of a credit rating agency for journalists.

Government regulation of the press isn&#8217;t compatible with the USA&#8217;s Constitution and even press self-regulation wouldn&#8217;t be in line with American&#8217;s traditional sense of the free press. It&#8217;s not clear that the most prominent publications e.g. the New York Times and Wall Street Journal would submit to a regime like that or that it would have much sway without them.

But Dalio is not wrong either, we need better consensus on what we expect from journalists and tools to signal credibility and hold people accountable. If the New York Times wants to be credible, it can&#8217;t rely purely on prestige, it has to be open about its standards and practices and [uphold what it means for something to be published in the newspaper of record](http://www.philly.com/philly/blogs/attytood/The-big-problem-with-NY-Times-climate-isnt-what-you-think.html).

[David Siegel](http://www.businessinsider.com/two-sigma-david-siegel-on-fake-news-and-infinite-personalization-2017-2) and [Cathy O&#8217;Neil](https://www.bloomberg.com/view/articles/2017-01-24/if-fake-news-fools-you-it-can-fool-robots-too) think personalization and big data are the problem.

Siegel frames the problem as one of micro-personalization. The picture I have is, in the old days everyone watched Cronkite and read the New York Times, and elite media institutions set the agenda. With the Internet and cable TV, the media fragmented. Everyone is more receptive toward media outlets that reflect their own values and point of view, and gravitates towards outlets that reflect them. But the more you hear mostly news and points of view that confirm your own biases, the stronger those biases get. And personalization and news recommendations are the ultimate silo or filter bubble. You only hear the news you like to hear. The end result is a singularity of polarization, where anything that doesn&#8217;t toe a narrow line triggers [cognitive dissonance](http://theoatmeal.com/comics/believe) and a strong emotional reaction of &#8216;OMG mainstream media bias&#8217;/&#8217;fake news&#8217;.

Siegel&#8217;s argument as I understand it seems completely plausible. But the root issue is fragmentation, not algorithms per se. Eliminate algorithms and you still have the problem that some people get all their information from [Democratic Underground](https://www.democraticunderground.com/) and some from [Free Republic](http://www.freerepublic.com/tag/*/index). You can make an algorithm optimize for whatever you want, for instance try to [surface quality](https://www.bloomberg.com/news/articles/2017-04-25/google-rewrites-its-powerful-search-rankings-to-bury-fake-news) from a variety of points of view. The algorithm genie is not going back in the bottle. To the extent there&#8217;s an algorithm problem, the answer is to improve the algorithm.

O&#8217;Neil&#8217;s &#8220;Weapons of Math Destruction&#8221; point is that any manipulation that can fool humans can fool the algorithms. We&#8217;re stuck with misinformation, and the solution is to turn to trusted sources. But what happens if sinister forces fool your trusted source, or make you think a source can be trusted when in fact they are bought and sold?

Furthermore, if I only believe stuff once it&#8217;s fully fact-checked and published on Bloomberg or in the New York Times, I&#8217;m ignoring a lot of potentially useful information and sources. It&#8217;s a [bias/variance problem](/2016/11/everyone-lives-in-a-bubble-and-all-models-are-overfitted/). I want a news filter that is sensitive enough to surface the viral story about [United Airlines](http://www.businessinsider.com/video-shows-passenger-forcibly-dragged-off-united-airlines-plane-2017-4) if I happen to own their stock, while being selective enough to bury [hoaxes](https://www.nytimes.com/2015/06/07/magazine/the-agency.html?_r=0).

If the argument is that any signal a filter might use can be gamed by fake news information operations, that is generally true, but only up to a point.

Indeed, [Goodhart&#8217;s law](http://lesswrong.com/lw/1ws/the_importance_of_goodharts_law/) is in play, and any statistical regularity that let&#8217;s one filter fake news will tend to collapse once pressure is applied on it for control purposes and it starts being gamed.

But good predictors don&#8217;t usually collapse to zero. Even though just about anything can be faked with sufficient skill, museums are probably not mostly full of fakes, even though the incentives to manufacture forgeries are enormous. Our relative safety lies in the fact that it&#8217;s hard to commit a perfect crime. The entropy of reality is impossibly complex to forge. With enough fact-checkers, lies are generally detectable. With enough resources you can make anything go viral, but you can&#8217;t make anything withstand scrutiny. You can fool all the people some of the time and some of the people all the time, but you cannot fool all the people all the time. And (fake news alert!) Lincoln [never actually said that](http://quoteinvestigator.com/2013/12/11/cannot-fool/).

The crude manipulation of the form &#8216;Pope endorses Trump&#8217; is straightforward for the top quartile of readers to detect and debunk. [So it is feasible for algorithms to filter that out](https://shift.newco.co/how-to-detect-fake-news-in-real-time-9fdae0197bfd) based on where the story originated, the ratio of credible sources spreading it, spam flagging. Just as Google filters spam, news algorithms can and should filter egregiously fake news.

Platforms like Facebook and Google are going to build algorithms, and it&#8217;s incumbent on them to build ones that can&#8217;t be fooled all of the time. To [cry censorship](http://www.zerohedge.com/news/2017-01-21/exposing-fake-news-censorship-industry) is a bad-faith, burn-it-all-down argument. The answer to bad algorithms is better algorithms. There is no argument for letting people vote based on &#8216;Pope endorses Trump&#8217;-level news. Either participate in truth-based news and policy, or GTFO, because democracy needs to be nurtured, and the crude distortions of reality, reminiscent of Nazis and Soviets, herald the end of democracy.

Powerful forces will use big data and machine learning to manipulate you. But big data and machine learning algorithms are also very powerful tools to fight manipulation and help surface quality. And we should be doing that instead of decrying personalization, and building a market design which is more resistant to manipulation and provides tools to surface quality.

I think Cathy O&#8217;Neil is right in the sense that the process has to be connected to human curation. Giving the filtering process over to machine seems to result in a sort of clickbait fetishism, vaguely akin to Marx&#8217;s commodity fetishism, where fundamentally human relations and processes are abstracted and subordinated to relationships between commodities and markets. The value of true information gets buried by clickbait that triggers the reptilian brain, endorphin release, and bias confirmation.

I think if we could somehow build a pervasive &#8216;pay it forward&#8217; karma and reputation ecosystem that rewarded people for sharing quality and burying fake news and garbage, and somehow get back some of the old sharing ethic, it would go a long way. That&#8217;s what I&#8217;d be thinking about if I were a VC or online community entrepreneur. Tools to let people signal quality, find data exhaust that separates spam, disinformation and noise from truth, build trust, and fight the noise machines.

If credible news organizations open their work, tag facts vs. opinions, open up source materials like interview transcripts, link to sources, then a crowdsourced fact-checking Wikipedia / [Genius](https://rap.genius.com/) could do a pretty good job debunking stuff that&#8217;s complete garbage. Contributors recognized as fair fact-checkers could get karma and monetary rewards.

Something like [WikiTribune](https://www.wired.co.uk/article/jimmy-wales-wikitribune) seems like a promising approach. Maybe even an [industry news integrity initiative](http://www.niemanlab.org/2017/04/the-news-integrity-initiative-is-taking-a-cross-industry-approach-to-fixing-the-news-trust-problem/).

It&#8217;s a huge problem and solving it is critical for democracy and free market capitalism.

That being said, if you have a large chunk of people who want to tear everything down, you&#8217;re not going make progress building trust and credibility.

And fish rots from the head. If you&#8217;re led by someone with a habit of lying, denying reality, and calling everything he doesn&#8217;t like &#8220;fake news,&#8221; you&#8217;re going to continue to have a crisis of legitimacy and reality-based institutions.