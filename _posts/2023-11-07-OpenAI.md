---
id: 7127
title: 'OpenAI Dev Day'
subtitle: ''
date: 2023-11-07T01:01:01+00:00
author: Druce Vertes
layout: post
guid: /?p=7127
permalink: /2023/11/OpenAI-Dev-Day
categories: datascience
tags: datascience

---

> OpenAI is moving rapidly and extending their lead.
<iframe width="560" height="315" src="https://www.youtube.com/embed/U9mJuUkhUzk?si=iKPA30LObMbOSpZ_" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>

<!--more-->

OpenAI held their first dev day, with the anticipation formerly attending Steve Jobs iPhone announcements. Key notes on the keynote:

- GPT4 Turbo with major feature improvements and updates to the foundation model, at less than half the price of the previous top-of-the-line model.

- API improvements including the Assistant API which builds some of the most essential and popular capabilities of Langchain and LlamaIndex frameworks directly into the OpenAI API.

- GPTs - think of this as, the feature dropdown is dead, long live the dropdown; basically anyone can build a custom chatbot with access to specific tools, knowledge, and system prompts. Most every product will come with its own chatbot for docs, question answering, and to perform API tasks.


Foundation model enhancements
- [GPT4 Turbo now available](https://openai.com/blog/new-models-and-developer-products-announced-at-devday)
	- In beta as *gpt-4-1106-preview*, and now driving the main ChatGPT chatbot UI
	- 128K context window vs. 32K
	- Knowledge up to April 2023 cutoff
	- Less than 1/2 the price of GPT-4 8K context, and 1/4 the price of GPT-4 32K token context: $0.01 per 1,000 input tokens and $0.03 per 1,000 output tokens, vs. previous $0.03 and $0.06 (8K context). *It's quite remarkable to get a 2-page summary of a 200-page document for about $1.*
	- Doubled tokens per minute rate limits ([Check your rate limits](https://platform.openai.com/account/limits))
	- Multi-modal inputs: images. 
	- Updated Whisper V3 text-to-speech model. Aside: The Sky voice in the ChatGPT mobile app is ScarJo and you can't tell me otherwise.
	- Function calling update: One API call can return multiple function calls in response to a prompt, such as, 'look up a track in Spotify', 'play the track'
	- JSON mode will always return valid JSON. Previously telling GPT-4 to return JSON was hit-or-miss. But you could trick it to return valid JSON by inventing a fictitious function signature for it to call.
	- Reproducible outputs by specifying a random seed. Previous model was stochastic and you would potentially get a different answer each time.
	- Return probabilities for completions, which is useful for knowing how certain the model is of its answer, or e.g. providing multiple alternatives for autocomplete.
	- Fine-tuning GPT-4 using your own content and tasks is now available in experimental mode on request.
	- Custom models - for a high price, like 7 figures, OpenAI will develop a [custom GPT-4 based on data and training parameters you specify](https://openai.com/form/custom-models)
	- Copyright Shield: OpenAI will indemnify enterprise users and app developers against liability for infringement claims based on the usage of OpenAI APIs.


- API enhancements. Background: There is an [emerging stack for LLM applications](https://a16z.com/emerging-architectures-for-llm-applications/). Beyond the OpenAI question-answering API, app developers may need conversational memory storage and retrieval and contextual compression to remember and use conversations, embedding storage and retrieval and contextual compression, logging/observability, prompt management, UI wrappers, etc. etc. Two important frameworks that provide integrations between LLMs and other DBs and SaaS services and APIs are [LangChain](https://python.langchain.com/docs/get_started/introduction) and [LlamaIndex](https://docs.llamaindex.ai/en/stable/). They are a great roadmap to how to implement complex LLM flows and you're not a serious LLM dev if you don't have a passing familiarity with them. 
  
  But in most cases all the real magic is performed by the underlying calls to the LLM, e.g. OpenAI. And for simplicity and stability you are often better off writing straight to the OpenAI API, like you might be better off using a simple JAM (JS, APIs, Markup) web dev stack vs. full-blown MERN framework (MongoDB, Express.js, React.js, Node.js).
  
  With the latest update, OpenAI has stolen a page from LangChain and LlamaIndex and started to integrate some of the functionality higher in the stack, directly into the OpenAI API.
  
	- Persistent infinite threads - OpenAI handles conversational memory
	- Code interpreter - OpenAI handles calling functions in a sandbox. When you use the function calling API to describe functions and when to use them, and then a prompt returns a function signature to call, OpenAI can  run the function in a sandbox (for $.03 per sandbox per hour). 
	- Retrieval - Add documents to the context for prompting, and OpenAI handles retrieval-augmented generation (RAG). 
	- Another way of viewing the Assistants API is to expose the capabilities  of the ChatGPT Advanced Data Analysis dropdown to devs in the API
	- 4 new model APIs
		- Image input (in beta in the *gpt-4-vision-preview* model, expected to be in the final GPT-4 release)
		- DALL•E 3 image generation via the API
		- Text to speech via *tts-1* and  *tts-1-hd* models . ElevenLabs was previously one way to do this, but these models are cheaper and most likely better.
		- Speech to text with updated Whisper v3


- [GPTs.](https://openai.com/blog/introducing-gpts) ChatGPT previously had a dropdown where you had to choose the flavor of GPT-4 you wanted to use: one with plug-ins you had to specify, or the ability to browse the Web, or advanced data analysis with code interpreter and ability to upload docs, or DALL•E 3. That dropdown is going away and the default GPT-4 Turbo model will have access to all those functionalities.
  
On the other hand, now you or any vendor, will have the ability to create a custom GPT chatbot with access to the data and tools and system prompts of your choice, and publish it in the GPT store.

I would expect any product going forward will come with a GPT to teach you to use it and answer questions, and to talk to its API to perform tasks.

Takeaways
- Impressive keynote. didn't announce GPT-5 or that AGI has been achieved internally, but shows rapid maturation of the product and of OpenAI as an organization.
- Changes the environment for a number of players: Anthropic, ElevenLabs, LangChain, LlamaIndex, and vertical RAG startups.
- API maturing rapidly.
- Expect an explosion in niche-specific GPTs.
