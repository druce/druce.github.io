---
id: 7125
title: 'Truth, Lies, and ChatGPT'
subtitle: 'On the fine line between clever and stupid, between BS, cargo cults, and great science; the Dari&eacute;n Gap between wordcel AI and shape-rotator AI, what AI might mean for social construction of reality, and what we should do about it.'
date: 2023-09-15T01:01:01+00:00
author: Druce Vertes
layout: post
guid: /?p=7125
permalink: /2023/09/chatgpt
categories: datascience
tags: datascience

---

>*That was just bullshit, Joel. - Miles, in [Risky Business (1983)](https://getyarn.io/yarn-clip/60423ed3-f7a6-4807-a43d-bf71c85214d1)*

![greatimposterposter.jpg](/assets/2023/greatimposterposter.jpg)
<!--more-->

What does ChatGPT really know? Where does it falter? Where do we stand on the journey to Artificial General Intelligence (AGI)? Are we approaching the [singularity](https://en.wikipedia.org/wiki/The_Singularity_Is_Near)? What does generative AI mean for increasingly computer-mediated [human reality](https://en.wikipedia.org/wiki/The_Social_Construction_of_Reality)?

Every new generation of media and tech since TV seems to make us dumber. How do we break the cycle and harness AI that makes us smarter, by helping us sort through the glut of information and misinformation and bullshit? 

Generative AI can manufacture infinite 'truthy' bullshit at near-zero cost, including 'deepfake' audio and video. And then it could potentially further train itself on the ones that go viral, concentrating fake 'truth' deep in our information infrastructure and knowledge frameworks. The singularity might prove a 'singularity of bullshit'.

How do we engineer our knowledge ecosystem to disrupt bullshit instead of perpetuating and magnifying it, and further intensifying the acid bath our shrinking attention spans soak in? How do we deal with deepfakes, misinformation, and other forms of AI-assisted fraud and bullshit? 

First let's explore what ChatGPT currently knows, and how it might evolve. Then let's think about the nature of human bullshit and its ecosystem, and what humans know vs. what we think we know. Finally, let's discuss some takeaways and possible policies.

### ChatGPT and bullshit

>*There are three kinds of lies: lies, damned lies, and statistics. - Mark Twain*

![liarliar.png](/assets/2023/liarliar.png)

ChatGPT is remarkably good at bullshit. For example, let's ask it to summarize the story of the movie "Titanic" in various styles, like a literary reviewer, or a Gen Z slacker, or Snoop Dogg.

![/assets/2023/titanic-critic.png](/assets/2023/titanic-critic.png)

![/assets/2023/titanic-genz.png](/assets/2023/titanic-genz.png)

![/assets/2023/titanic-snoop.png](/assets/2023/titanic-snoop.png)

ChatGPT excels as a pattern matcher and mimic, ranking in the [top 0.1% in verbal IQ](https://www.scientificamerican.com/article/i-gave-chatgpt-an-iq-test-heres-what-i-discovered/) by some measures. You can ask ChatGPT to respond in the voice of a Harvard history professor, an FT or WSJ journalist, a Goldman Sachs research analyst with deep knowledge of economics, finance, and investment management, or a McKinsey management consultant with expertise in corporate strategy and operations, and it will do a remarkable job. It can even write decent [Harvard papers](https://www.slowboring.com/p/chatgpt-goes-to-harvard), which is of course the bullshit Olympics. If you want, you could tell ChatGPT you want all replies written in the language of [incel rage bait](https://twitter.com/nivi/status/1683621899254001665), or a fictional persona like Holden Caulfield.

It seems likely that, within a few years, most professional writing and creative work will be done with the assistance of ChatGPT. (For simplicity, let's use 'ChatGPT' as a shortcut for generative AI as a whole). Feed ChatGPT your corporate style guide and examples of tone, voice and branding nuances, and it will [speed up and improve](https://www.science.org/doi/10.1126/science.adh2586) writing and [knowledge work tasks](https://www.oneusefulthing.org/p/centaurs-and-cyborgs-on-the-jagged). Its verbal mastery is [essentially indistinguishable from human writing](https://www.science.org/doi/10.1126/sciadv.adh1850). 

Fine artists might have to take a page from mountain climbers who characterize their ascents as 'without oxygen' or 'free solo', and specify what assistive AI tech was used. Everybody can now be a [James Michener](https://www.erroluys.com/covenantassignment1.html) or [Edward Stratemeyer](https://daily.jstor.org/the-secret-syndicate-behind-nancy-drew/), prolific authors who used teams of assistants to operate content mills.

You can use the latent logic and reasoning inherent in language understanding and generation to create remarkable workflows: ask an [agent](https://agentgpt.reworkd.ai/) to identify the best [coffee grinder](/assets/2023/coffeegrinder.pdf) according to your criteria, fetch information from Google, verify it, condense it, make a recommendation based on data, and then succinctly summarize and explain it. Most complex tech will probably integrate ChatGPT assistants, supplementing or possibly to some extent replacing existing UIs, APIs, and DSLs. An AI assistant like [Microsoft Copilot](https://www.microsoft.com/en-us/microsoft-365/blog/2023/09/21/announcing-microsoft-365-copilot-general-availability-and-microsoft-365-chat/), that knows you and your workflows and content creation style well, might become the UI of first choice to many different applications, abstracting the UI from application layer.

Skeptics might say ChatGPT doesn't have deep knowledge, and it's all surface level bullshit, which has a kernel of validity. ChatGPT doesn't actually 'program' like a computer science student does, which is to make a mental model of a novel problem and write code to solve it based on deep understanding of data structures and algorithms. What ChatGPT can do is read all the code in existence, which a computer science student cannot. And then, if there is code out there similar to what you are trying to write, if will create something possibly [amazing](https://www.reddit.com/r/ChatGPT/comments/16tso7j/chatgpt_can_now_code_from_a_whiteboard_drawing_wow/), or possibly pretty good but badly flawed. It might even make up modules and APIs it thinks it needs to perform a task. If you give it a novel [Leetcode](https://www.techinterviewhandbook.org/grind75) hard problem that it has never seen before, it [will not solve it](https://twitter.com/cHHillee/status/1635790330854526981). ChatGPT has extensive knowledge but not deep understanding. It gives you ['infinite interns'](https://www.ben-evans.com/benedictevans/2023/7/2/working-with-ai), but you have to give them very detailed direction, and they sometimes go off the rails regardless. The glaring [Brandon Hunter screwup](https://searchengineland.com/microsoft-brandon-hunter-useless-ai-obituary-432008) bodes poorly for AI journalism and office automation. It's just a matter of time before the first major IT disaster fueled by generative AI, like when the Mars probe [blew up due to a metric/Imperial error](http://www.cnn.com/TECH/space/9909/30/mars.metric.02/). 

But ChatGPT does know a hell of a lot. Go to a [random Wikipedia page](https://wikiroulette.co/) and quiz ChatGPT on a fact, like who won the 1995 world women's handball championship, and it will probably know the answer. And the [GPT-4 paper](https://arxiv.org/abs/2303.08774) shows it passed all these tests (note the poor math scores, we will come back to that):

![/assets/2023/gpt/TestScores.png](/assets/2023/gpt/TestScores.png)

Carl Sagan said, "If you wish to make an apple pie from scratch, you must first invent the universe." Similarly, if you wish to predict Shakespeare's next token from scratch you must first fully model Shakespeare's universe. A sufficiently immense statistical knowledge graph of linguistic relationships inherently captures a great deal of human knowledge.  Just as constructing fluent language requires latent rhetorical reasoning, it also requires implicit factual knowledge.

On the other hand, it's also possible to overestimate ChatGPT, to be lured by its linguistic fluency into the trap of thinking ChatGPT is smarter than it is. Smart people ask it to tackle complex questions like, make a psychological profile of this person based on this writing, rate this resume against this job description. It will happily generate wrong, highly confident answers. It's good at first-order language understanding and generation, information extraction, summarization. You can give it a second-order question like, what is the difference between a Les Paul Standard and a Les Paul Custom, and it will be helpful. You can give it chain-of-thought reasoning tasks like, here are a bunch of annual reports, give me compound annual growth rate in revenues for last 5 years, especially if you prompt it to think step by step. But chain-of-thought prompting can sometimes veer into telling ChatGPT what you want it to say.

Expecting ChatGPT to answer deep analytical questions at the level of a thoughtful, trained, experienced scientist is unrealistic. LLMs [struggle at complex reasoning](https://aiguide.substack.com/p/can-large-language-models-reason). Of course, you can tell ChatGPT it’s a chemistry professor with deep knowledge of flavors and fragrances. It might sound like one, and be very confident and persuasive. But that doesn’t make it one. I’d be a bit cautious consuming its recipes. 

ChatGPT's ability to bullshit is a trap. We will get a ton of terrible, directed thinking, that sounds really persuasive and is supported by beautiful but misleading data visualization from the [Advanced Data Analysis / Code Interpreter](https://www.coursera.org/learn/chatgpt-advanced-data-analysis) plugin.

ChatGPT is an AI [Ferdinand Waldo Damara](https://www.businessinsider.com/ferdinand-waldo-demara-greatest-con-artist-ever-great-imposter-2016-9#the-brave-and-generous-surgeons-fame-spread--all-the-way-back-to-the-real-dr-joseph-cyr-15). The exploits of Damara, dubbed [The Great Impostor](https://www.imdb.com/title/tt0053879/?ref_=fn_al_tt_1), exceed the [possibly exaggerated frauds of Frank Abagnale](https://nypost.com/2023/03/13/catch-me-if-you-can-conman-frank-abagnale-lied-about-his-lies/).  Damara had a photographic memory and an exceptional IQ, and undoubtedly had the ability to be a legitimate doctor, or civil engineer, if not the character and temperament. Are you a surgeon because you have degrees and appointments, or because you [save lives](https://nowiknow.com/the-doctor-is-in-we-think/)? To bullshit well, you have to know a lot about what you are doing. 

Maybe what ChatGPT shows us is that the line between deceit, bullshit, cargo-culting and good research is finer than we think.

ChatGPT excels at plausible, ['truthy' explanations](https://www.cc.com/video/63ite2/the-colbert-report-the-word-truthiness). It will happily make stuff up if you ask it about fictional characters or APIs. Fortunately, it's such a people-pleaser that, if you merely instruct it to use only a specific source of truth, or to think step by step and be careful not to say anything that isn't true, then it will generally avoid making stuff up. 

If you can keep ChatGPT grounded in reality and avoid situations where it goes off the rails, it is truly a force multiplier for knowledge work. It's a huge step towards Artificial General Intelligence (AGI). But it's not AGI yet. Things that a fully-developed AGI would want to do:

- **Sensory Perception.** Cameras are great but human eyes are better, maybe 500 megapixels, wide dynamic range, heuristics to detect and track objects and integrate over time.

- **Filtering.** Human senses transmit roughly [1GB/s of information to the brain](https://hasanoviz.medium.com/visualization-principles-a8c6e46ddc5f). Most of the data is filtered before we become aware of it. We can only process maybe [120 bits of conscious awareness per second](https://www.fastcompany.com/3051417/why-its-so-hard-to-pay-attention-explained-by-science), on a [half-second lag behind real-time](https://www.tandfonline.com/doi/full/10.1080/14681360500487470). The preprocessing by the optic nerve and subconscious [integration of various senses](https://www.youtube.com/watch?v=YvnOtS4V-Pg) mean that by the time we are aware of something, it already makes sense. To play soccer like Messi, you have to train until your brain [OODA-loops](https://en.wikipedia.org/wiki/OODA_loop) rings around your opponent without conscious thought.

- **Memory and representation**. AGI would fit new facts into existing frameworks, and create new theories, models and frameworks to fit facts that don't fit old patterns.

- **Abstraction**. Using inductive reasoning, it would extrapolate from concrete examples to general principles and world models.

- **Deductive reasoning.** It would use general models and principles to solve specific problems, identifying the best model and inputs to the model.

- **Autonomy.** It would independently determine high-priority tasks and execute them without external prompts, based on its own values and objectives. 

- **Adaptation.** It would constantly learn and improve its policies.

- **Self-reflection.**  It would use introspection to reason about itself, evaluate its own progress, determine what it needs to learn and practice to achieve its objectives.

- **Flexibility.** Creative solutions to complex novel problems.

- **Empathy.** The ability to model other intelligent agents.

- **Systems thinking.** The ability to model systems of agents, think strategically, and determine individual policies aligned with shared objectives. Ethical reasoning is basically strategic thinking, on a long timeframe, taking into account multi-agent policy equilibria.

- **Communication.** The ability to explain and communicate its thinking and actions to other intelligent agents, achieving cooperation and collaboration for shared objectives.

- **Physical interaction.** Robotic manipulation of complex novel environments.

- **Sentience.** Defining consciousness seems a fool's errand. We can experience it directly, but there is no test to know if an elephant or dolphin or octopus is sentient. It's a poignant element of the human condition that we are social creatures, language creatures, but our most direct experiences are ones about which we can say and share very little. [Whereof one cannot speak, etc.](https://www.brainyquote.com/authors/ludwig-wittgenstein-quotes)

ChatGPT is a giant leap forward. Unlike humans, computers are fungible and once you create one intelligence, you can create millions. Then they can communicate with each other at much higher rates than humans, self-improving rapidly. The [singularity](https://en.wikipedia.org/wiki/The_Singularity_Is_Near) starts to come into focus. We're maybe only a couple of ChatGPT-size leaps away, and the leaps might get easier because we can use the machine learning tools we built before to make the next leap. We're not there, but maybe we can start to see it from here.

### The Dari&eacute;n gap between poets and quants, wordcels and shape rotators

![sheinstein.png](/assets/2023/sheinstein.png)

ChatGPT often fails at [obvious riddles](https://www.scientificamerican.com/article/i-gave-chatgpt-an-iq-test-heres-what-i-discovered/), like “What is the first name of the father of Sebastian’s children?”.

![/assets/2023/John.png](/assets/2023/John.png)
![/assets/2023/Sebastian.png](/assets/2023/Sebastian.png)
![/assets/2023/Milarus.png](/assets/2023/Milarus.png)

If you ask ChatGPT to play tic-tac-toe, it might find creative ways to win:

![/assets/2023/tictactoe.png](/assets/2023/tictactoe.png)

ChatGPT isn't reliable at simple math problems. 

![/assets/2023/gptmathfail.png](/assets/2023/gptmathfail.png)

In the first question, the final answer is correct but the first paragraph is incorrect and unnecessary. In the second question, the correct calculation is 41x67x83=228001. 

As mentioned above, ChatGPT gets low scores on the AP Calculus test and on the American Mathematics Competition tests. Computers are supposed to be good at math. Why does ChatGPT struggle at math?

If you try to travel from North America to South America, you can't complete the entire journey by ground transportation. There's a segment that is impassable to vehicles, for reasons of history, geography, economy, and politics. That’s the [Dari&eacute;n gap](https://en.wikipedia.org/wiki/Dari%C3%A9n_Gap).

We're in a liminal state, a transitional phase where computers excel at computation, and excel at language, but are missing something to bridge the gap. The latest iPhones can perform up to 15 trillion floating point operations per second. Wolfram tools perform brilliantly at symbolic integration and solving systems of differential equations. But ChatGPT can struggle to translate word problems into the right representation to solve them, even when given access to Python or Wolfram tools.

If you want to navigate a road network from point A to point B, there is more than one way to do it. One method is to know your current GPS coordinates and the GPS coordinates of your destination, determine which direction on the road you are currently on takes you closer to the destination, and keep moving toward your destination, continuously updating your position based on the direction and velocity of travel and making turns and possibly backtracking as necessary. This dead reckoning approach is, I suppose, the way people with a keen sense of direction navigate. (I am not one of them, I'm more like [Captain Sobel in Band of Brothers](https://www.youtube.com/watch?v=W0scF0yxpNU).) Another approach is to construct the graph of all the roads and intersections and apply [Dijkstra's algorithm](https://en.wikipedia.org/wiki/Dijkstra%27s_algorithm) to find the shortest route. Having long memorized the graph of NYC roads and public transport, some kind of approximate Dijkstra is second nature to me. A third way is to follow text directions from Google Maps, or from a human, like the ['rutters'](https://en.wikipedia.org/wiki/Rutter_(nautical)) or pilot books of the early explorers.

ChatGPT is language-oriented. It's a poet, or a [wordcel](https://roonscape.ai/p/a-song-of-shapes-and-words).

To solve systems of differential equations, you need a quant, a coder, a shape rotator who models problems visually and numerically.

[Richard Feynman](https://chem.fsu.edu/chemlab/isc3523c/feyn_surely.pdf) explained his 'shape rotator' mode of thinking:

*I had a scheme, which I still use today when somebody is explaining something that I’m trying to understand: I keep making up examples. For instance, the mathematicians would come in with a terrific theorem, and they’re all excited. As they’re telling me the conditions of the theorem, I construct something which fits all the conditions. You know, you have a set (one ball) – disjoint (two balls). Then the balls turn colors, grow hairs, or whatever, in my head as they put more conditions on. Finally they state the theorem, which is some dumb thing about the ball which isn’t true for my hairy green ball thing, so I say, ‘False!’*

...

*I can’t understand anything in general unless I’m carrying along in my mind a specific example and watching it go. Some people think in the beginning that I’m kind of slow and I don’t understand the problem, because I ask a lot of these “dumb” questions: “Is a cathode plus or minus? Is an an-ion this way, or that way?” But later, when the guy’s in the middle of a bunch of equations, he’ll say something and I’ll say, “Wait a minute! There’s an error! That can’t be right!” The guy looks at his equations, and sure enough, after a while, he finds the mistake and wonders, “How the hell did this guy, who hardly understood at the beginning, find that mistake in the mess of all these equations?” He thinks I’m following the steps mathematically, but that’s not what I’m doing. I have the specific, physical example of what he’s trying to analyze, and I know from instinct and experience the properties of the thing. So when the equation says it should behave so-and-so, and I know that’s the wrong way around, I jump up and say, “Wait! There’s a mistake!*

Humans seem to follow multiple paths of reasoning simultaneously and cross-check between them as part of 'common sense'. There is more than one path to enlightenment, and the more paths you use, the better. ChatGPT is a one-trick pattern-matching pony. And sometimes generative AI generates more fingers and teeth than what's anatomically accurate. It seems plausible to combine Stable Diffusion with another 'fix it in post' AI and instruct it to please ensure humans have five fingers and their teeth match well-known human anatomy. For instance, [ideogram.ai](https://ideogram.ai/) will now let you specify text to appear in an AI-generated image, where other AIs struggle and write nonsense.

There can be a 'brittleness' to deep learning solutions. 'Adversarial' in 'Generative Adversarial Networks' means you are training one AI to make images that another 'adversarial' AI can't detect as fakes. And yet a simple finger count or dental inspection defeats it. Similarly, a simple bit of analysis and adversarial trickery helped [a human defeat AlphaGo](https://www.vice.com/en/article/v7v5xb/a-human-amateur-beat-a-top-go-playing-ai-using-a-simple-trick). It seems that a bit of model heterogeneity and diversification might go a long way.

A few ChatGPT improvements we might see in the not-too-distant future:

ChatGPT is reportedly architected as a [mixture-of-experts model](https://pub.towardsai.net/gpt-4-8-models-in-one-the-secret-is-out-e3d16fd1eee0). So adding an expert model optimized for math word problems, trained by [rewarding it for each successful step](https://openai.com/research/improving-mathematical-reasoning-with-process-supervision), will help it perform better. Think of this as a horizontal scaling approach: add many more specialized models and use the best one.

Another approach is to decompose problems into components and solve them sequentially. Using an 'agent' workflow with [function calling, code interpreter, data analytics](https://www.oneusefulthing.org/p/what-ai-can-do-with-a-toolbox-getting), ChatGPT can translate questions into a function call, or SQL, or Wolfram Mathematica, or a REST API, get answers back, make further calls as necessary and finally translate the result into English. Think of this as a vertical scaling approach: solve problems using deep chains of models.

You can combine these approaches and send questions to multiple expert models that try to solve things in different ways and use external tools, search engines, SQL, or [REST APIs](https://gorilla.cs.berkeley.edu/) they have trained on, cross-check each other and fix errors, and then return the explanation back to the user. (It's an interesting question whether ensembles work better if they are totally independent, or if they have some channel to check each other...my suspicion is, fuzzy models might work better when the models communicate.)

ChatGPT is evolving. The LLM itself becomes sort of a glue code generator; it solves impedance mismatches between different systems. You can take a natural language question, translate part of it into SQL, part of it into a search engine query, a REST API, whatever, and then combine the results into a natural-language answer. 

Updating an LLM with new information currently presents major challenges. You have to retrain it from scratch on a new corpus at great cost, [or use fine-tuning, or use retrieval-augmented generation](https://towardsdatascience.com/rag-vs-finetuning-which-is-the-best-tool-to-boost-your-llm-application-94654b1eaba7). There is a transfer learning problem here that seems solvable. If we had an understanding of how LLMs represent a knowledge graph, we could abstract the knowledge graph from the capacity for language. We could train the LLM against multiple unrelated knowledge graphs, in a way that generalizes to unknown future knowledge graphs. This approach could allow us to potentially swap knowledge in and out. It might turn out that everything is so intermingled, there is no way to disentangle the language capability from the factual representation. It just feels like retraining a whole model from scratch is inefficient, and we might be able to represent a knowledge base in a transformer network which could be transferred into a separately trained LLM, and vastly reduce the amount of training to add new data. Then you could update ChatGPT every day with new information from news, instead of having data through January 2022 as of late 2023.

ChatGPT has mastered language, in the process gaining a great deal of knowledge and at least some basic reasoning ability. I expect a very long runway for ChatGPT to get a lot smarter, with transfer learning, wider mixture-of-expert models, deeper models that chain multiple heterogenous experts, smart bagging and boosting where they cross-check each other and fix each other's errors. 

There is a long way to go to fill out the AGI capabilities listed above, to master human-like ability to use reason, intelligence, creativity to solve problems in novel, complex, constantly-changing environments. But machines are pretty good at most of the activities involved in AGI. The challenge is to build functionality that can bridge the Dari&eacute;n gap(s), and integrate all the different aspects of human intelligence.

Then AI can pass the Steve Wozniak [coffee test](https://www.fastcompany.com/1568187/wozniak-could-computer-make-cup-coffee). Can a robot assistant go into a random house, locate the essential tools and ingredients, and make a decent cup of coffee?

<iframe width="560" height="315" src="https://www.youtube.com/embed/RNLL0yjWLgE?si=nK2xfivaDPpOuCz7" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>

### What Is Bullshit?

![/assets/2023/bullshit.gif](/assets/2023/bullshit.gif)

>*History is a set of lies agreed upon. - Napoleon Bonaparte*

>*Bullshit is the glue that binds us as a nation. - George Carlin*

Let's switch gears a bit and delve into a different topic...bullshit. When I was growing up, there was the Weekly World News featuring [Batboy](https://www.google.com/search?sca_esv=566955665&rlz=1C5CHFA_enUS901US901&sxsrf=AM9HkKm_xifndRXVykSvR-Ef8V208t6Bqg:1695222613180&q=batboy&tbm=isch&source=lnms&sa=X&ved=2ahUKEwiKvrGtvLmBAxUImIkEHeanBHMQ0pQJegQIBxAB&biw=1772&bih=978&dpr=2) and other half-human cryptids like the love child of [Hillary Clinton and lizard alien P'Lod](https://www.motherjones.com/media/2015/04/weekly-world-news-clintons-aliens/). But Alex Jones's large, well-monetized community for whom the Sandy Hook massacre was a psyop and the grieving parents are crisis actors, is a more sinister development. 

The Internet is to bullshit what the [Broad Street pump](https://en.wikipedia.org/wiki/1854_Broad_Street_cholera_outbreak) was to cholera. It turns the deluded mind of a sick individual like Alex Jones into a systemic problem for everyone. 

On the positive side, social media connects quirky communities of people with niche interests, such as shared love of nerdy hobbies like LARPing. They can find each other and make connections outside families and communities that may not be welcoming. More controversially (but who am I to judge), it enables communities of furries and [dog haters](https://www.theatlantic.com/ideas/archive/2023/09/people-who-hate-dogs-reddit-dogfree/675372/) alike. And then at the extremes we get conspiracy theorists and hate groups who create self-reinforcing alternate reality echo chambers. (Sometimes it can be hard to tell the difference between the extremists and the LARPers, and when the extremists are busted, the extremists will say they are just joking, just asking questions, just having some good old-time fun. [Why are you so serious?](https://www.goodreads.com/quotes/7870768-never-believe-that-anti-semites-are-completely-unaware-of-the-absurdity))

In a couple of generations, we went from church every Sunday to [bowling alone](https://en.wikipedia.org/wiki/Bowling_Alone) to 'extremely online'. Sometimes, we have extreme audience segmentation, and sometimes we have [context collapse](https://en.wikipedia.org/wiki/Context_collapse), where everyone sees the same thing, but some people [interpret](https://www.thedailybeast.com/elon-musks-attacks-on-a-las-vegas-reporter-are-a-new-low-in-his-thuggery) it totally differently based on the context they see it in, or lack of context, or their own preconceptions or agendas.

Enter ChatGPT, which is a force-multiplier for bullshit, even more than for honest knowledge work, which is constrained by reality. 

AI-generated bullshit can help the toxicity of extremists go mainstream and overwhelm bland, objective, 'reality-based' communities.

[Brandolini's law](https://en.wikipedia.org/wiki/Brandolini%27s_law), the Bullshit Asymmetry Principle, says it takes humans an order of magnitude more energy to debunk bullshit than to create it. With AI generating bullshit, the asymmetry increases without bound.

Then we observe a [Gresham's law](https://fs.blog/mental-model-greshams-law/) race to the bottom, as people start to dismiss everything as bullshit, and just believe whatever they want. Even our self-appointed agenda-setter, the idiot savant Elon Musk [waxes conspiratorial](https://www.nytimes.com/2023/09/25/business/media/las-vegas-review-journal-bike-crash.html) and calls everything he doesn't like [a psyop](https://www.google.com/search?q=things+elon+musk+has+called+a+psyop&rlz=1C5CHFA_enUS901US901&oq=things+elon+musk+has+called+a+psyop&aqs=chrome..69i57j33i160j33i160i395l2.6848j1j4&sourceid=chrome&ie=UTF-8). 

This section doesn't have much to do with ChatGPT and can probably be safely skipped. The bottom line is, bullshit is everywhere, there is no effective test for AI-generated bullshit, and all we can do is attack it head-on using the same tech it uses to overwhelm our defenses.

[![/assets/2023/notarobot.png](/assets/2023/notarobot.png)](https://nautil.us/welcome-to-the-next-level-of-bullshit-237959/)

Following the pre-eminent scholar of bullshit, Harry Frankfurt, and [this unknown legend](https://www.reddit.com/media?url=https%3A%2F%2Fi.redd.it%2Ffwd0opvsjxz51.jpg), let's make a 'bullshit alignment matrix' according to the ground truth of a statement, and its intent.

|  Truth value    	| Good: Truthful Intent  	| Bullshit: Statements for Effect, Indifferent to Truth Value  	| Bad: Intended to Mislead 	|
|:---	|---	|---	|---	|
| **Factually True**    	| <em>Roses are red</em>  	| <em>My response to the Covid pandemic was amazing!</em> 	| <em>More doctors smoke Camels<br /> &nbsp; <br />I did not have sexual relations with that woman, Miss Lewinsky</em>  	| 
| **No Inherent Truth Value:** <br />ill-formed nonsense, oxymoron, non sequitur or empty puffery	  	| <em>This statement is false</em>  	| <em>Our Founding Fathers under divine guidance created a new beginning for mankind.&nbsp; <br />&nbsp; <br /> Starbucks provides an immersive ultra-premium coffee-forward experience. &nbsp; <br />&nbsp; <br />Nobody's bigger or better at the military than I am.</em>	|   <em>[We're not going to sit here and listen to you bad-mouth the United States of America!](https://www.youtube.com/watch?v=KLHFdduVDVg)</em> 	|
| **Factually False** | <em>[Sarcasm](https://philpapers.org/rec/HAITIC): Great blog post, Einstein <br /> &nbsp; <br /> Satire, parody </em>	|  <em>Obama was born in Kenya&nbsp; <br />&nbsp; <br />We had the biggest inauguration crowd ever.</em>	| <em><a href="https://www.ftc.gov/news-events/news/press-releases/2014/06/loreal-settles-ftc-charges-alleging-deceptive-advertising-anti-aging-cosmetics">Clinically proven to boost genes and make your skin visibly younger in just a week</a>&nbsp; <br />&nbsp; <br /> [Gish Gallop](https://en.wikipedia.org/wiki/Gish_gallop)</em>  	|

&nbsp; <br />
Frankfurt argues that bullshit lives in the middle column and is characterized by intent. Bullshit is speaking with indifference to the truth value of what you are saying, using speech as an instrument to achieve an effect or objective instead of clearly conveying sincerely held beliefs. According to Frankfurt, if you are trying to tell the truth as you honestly believe it, but you're misinformed or clueless, that's not bullshit. Also, if you know what you are saying is false and you are lying, that's deception. Bullshit is the middle, where truth doesn't matter, all the points are made up, and speech is merely instrumental.

But it's a fine distinction. If you start believing your own bullshit, <em>&agrave; la</em> [George Costanza](https://www.youtube.com/watch?v=vn_PSJsl0LQ), does it cease to be bullshit? And sufficiently advanced indifference to the truth, when you should know better, is indistinguishable from deception.

Consider the story of Iraq's WMD. Was it a lie, or was it bullshit? Maybe Cheney and associates sincerely felt they captured the deeper reality that Saddam Hussein would deploy and use WMD if he could. But they knew that the evidence was flimsy and possibly fabricated, and war is serious business, the burden of proof is on the invader.  When [people say](https://en.wikipedia.org/wiki/Reality-based_community) *“We're an empire now, and when we act, we create our own reality,”* it veers from directed reasoning into outright lying.

Was Obama's statement, “If you like your health insurance, you can keep it under Obamacare” a lie, or bullshit? At first glance, it was true. Obamacare did not materially change existing insurance or require people to switch plans. Obama probably sincerely believed that existing insurance plans would remain largely unchanged and the vast majority of people wouldn't be compelled to switch plans. They would simply have a choice to switch if Obamacare was a better plan and solution for them. But Obama's team is smart, and probably could have and should have anticipated the substantial disruptions, and that ultimately a significant number of people would need to switch plans. Also, it was poorly phrased and could be interpreted as, you can turn your current insurance plan into an Obamacare plan, which presumably was not the intent. It veered into bullshit, if not a 'lie of the year'.

Definitely bullshit: When Alex Jones claimed that the Sandy Hook massacre was a hoax, he didn’t care whether it was true or not. His goal was to get viewers and sell his nutritional supplements. Similarly bullshit: Donald Trump's claim that he was going to stop immigration by building a wall, or that he would balance the budget and pay off the national debt. He didn’t care at all if these things were true or possible, just that they made him sound great. The NYT made a [list of 'lies'](https://www.nytimes.com/interactive/2017/06/23/opinion/trumps-lies.html) but technically, they are really extremely high-grade concentrated bullshit. Trump is completely indifferent to truth; he aims to create a perception and stoke his ego and inflame his cult followers. Conveniently directed reasoners who say, [take him seriously but not literally](https://www.cnbc.com/2016/11/09/peter-thiel-perfectly-summed-up-donald-trump-in-one-paragraph.html), are saying, I know he's bullshitting and I don't care, because his agenda suits my extreme-right cult of 'disruption'.

Let's turn to the bottom left corner. Some statements are false or misleading, but well-intentioned. When asked how you liked a performer's music, you might say, "Your costume was stunning, you really sparkled up there!" It's a non-sequitur and lets the person smile and say thank you so much, but the meaning might be pretty clear. Or, sarcasm like, "you were brilliant today, great job Einstein," is technically false but probably communicates the speaker's genuine thoughts effectively.

Take Dr. Fauci's shifting stance on masks. He initially discouraged their use but later endorsed them, despite the evidence staying consistent. Was one of those stances bullshit? Maybe. There is good evidence for effectiveness of masks. On planes we saw people without masks get sick, while people with masks did not. In natural experiments where mask policies were imposed in some places and not others, we saw less contagion in masked populations. No one is calling for surgeons to stop wearing masks in operating rooms. The virus gets spread through droplets, and masks trap droplets. [Steve Scalise](https://www.reddit.com/r/NewOrleans/comments/16jbxll/steve_scalise_wearing_a_mask_because_of_cancer/) seems firmly pro-mask now that he is immune-compromised. But Fauci's job as a public health official is to give truthful guidance to help achieve public health objectives. It seems like he made the first statement for instrumental reasons, to avoid a run on masks that would take them from frontline workers who needed them most, and because there might not be sufficient compliance and benefit to make a difference. Later, when mask supply grew and people demanded more action, he changed his position. You could call it bullshit in the sense that it's speech intended for effect, something not strictly true but told with a good intent from the bottom left of the table. But if he is transparent that he is making a recommendation based on an overall public health objective it's not bullshit. 

Similarly, consider economists who don't critique aspects of capitalism lest they [give ammunition to the barbarians](https://rodrik.typepad.com/dani_rodriks_weblog/2007/05/are_there_barba.html). If it's directed reasoning, it's bullshit. If Fed chairman Powell is talking to calm markets and transparent about why, it's not necessarily bullshit.

Anyway, I have to call bullshit on Frankfurt's main claim here, that bullshit is purely the middle column of indifference to truth. Those are important forms of bullshit, but speech in bad faith and contrary to the correct order of things, can occur in other forms. Communication takes place on multiple levels, and can have multiple meanings depending on context. Fruit flies like a banana.

That 1990s modem dialup sound can be annoying noise and can also be conveying important signals, depending on context. Bullshit, like beauty, can be in the eye of the beholder, a property of the interaction between speaker and listener, and not inherently of the message itself. Something might be bullshit on one level but not on another.  And inherently a lot of communication contains elements of both bullshit and non-bullshit.

### The Fine Line Between Truth, Error, Fraud, Bullshit, Cargo Cult Science and Good Science

> *Everybody talking to their pockets <br /> Everybody wants a box of chocolates <br /> And a long-stem rose - Leonard Cohen*

> *Doubt is an uncomfortable condition, but certainty is a ridiculous one. - Voltaire*

> *In my country, we believe that the only things that separate us from the animals are pointless rituals and mindless superstitions. - [Latka Gravas](https://www.youtube.com/watch?v=z2HM-9uxsq8)*

If this isn't digressive enough, I want to talk about Feynman's [Caltech lecture on cargo cults](https://calteches.library.caltech.edu/51/2/CargoCult.htm). "The first principle is that you must not fool yourself—and you are the easiest person to fool." Also [Feynman](https://www.youtube.com/watch?v=p2xhb-SdK0g): “It doesn't matter how beautiful your theory is, it doesn't matter how smart you are. If it doesn't agree with experiment, it's wrong.”

You are the easiest person to fool, and you are incentivized to privilege sweet, self-serving bullshit over hard-to-swallow bitter truth.  Feynman used the example of cargo cults in Papua New Guinea after World War 2, when natives built fake airfields and control towers hoping to bring back American planes and 'cargo'. He used cargo cults to represent pseudo-scientific bullshit.

Cargo cults may be helpful bullshit of the bottom left variety. They can serve broader functions. The PNG cargo-cult chiefs got people out doing something productive to make things better, and everyone felt better. The colonial authorities aren't too upset that civil leaders are organizing and gathering the people for innocuous reasons. And it builds the community in ways that might let them do other, more productive stuff.

Does close order drill formation increase battlefield performance? Probably not directly. But it instills discipline and unit cohesion. Similarly, benign hazing, like making fraternity pledges carry a brick at all times, persists for social purposes of signaling that pledges are committed to join, enforcing connection among the pledges by sharing this pointless experience, and reinforcing group hierarchy.

Cargo cults might be the ultimate ['bullshit jobs.'](https://en.wikipedia.org/wiki/Bullshit_Jobs) "What would you say you do around here, using a pineapple as a radio mike?" The cargo cult rituals solve no first-order societal problems. They just pay respect to the social hierarchy and represent an effort to bring the universe into balance, without necessarily understanding how. 

Information-free economic forecasts and research reports may fall into a similar category. You have to make decisions about the future. It is coming willy-nilly, and you have to make choices that impact the future. The Economic Report of the President and the State of the Union address are a sort of collective ritual bull session about what the future might bring, and help form collective consensus about things we have no choice but to coordinate collectively.

Point forecasts and stock market 'price targets' make no sense. And yet there is demand for them. People are bad at probability and crave certainty. To demand certainty is to demand bullshit, and yet smart people demand and pay big bucks for bullshit all the time.

Feynman chose to talk about cargo cults instead of Western organized religion, because of course that would have been offensive. But self-evidently, talking snakes and burning bushes and body-and-blood-of-Christ rituals aren't all that different from cargo cults.

People I know say they don't believe church doctrine, they just like going to church. People get a benefit even if Scrooge is right that it’s all humbug, a/k/a bullshit. Collectively embracing a bit of bullshit helps the medicine go down, and lets us do what we need to do anyway. Scrooge saw Christmas as humbug (bullshit) because it was a bunch of posturing and virtue signaling preventing him doing what was the natural order of things, which was to work and make money. But the payoff is that Scrooge was missing the big picture, which is that work is necessary but it's not the purpose of life.

In the words of Cervantes: “When life itself seems lunatic, who knows where madness lies? Perhaps to be too practical is madness. To surrender dreams — this may be madness. Too much sanity may be madness — and maddest of all: to see life as it is, and not as it should be!”

A lot of science is borderline cargo cult. Most of machine learning is trying random stuff and seeing if it works. Freud had some cockamamie ideas that were pretty cargo-cultish. Nobody knows how the brain works, we try SSRIs because we know they impact neuron firing, they help some people, we don't really know why. We don't know the full story why some people have heart attacks. But we see they have blockages that have a lot of cholesterol, we give statins to reduce cholesterol, they reduce heart attacks. 

Even disregarding the disturbingly high incidence of non-replicability and outright fraud (exhibits [one](https://www.npr.org/2023/07/27/1190568472/dan-ariely-francesca-gino-harvard-dishonesty-fabricated-data), [two](https://www.vox.com/future-perfect/2023/8/9/23825966/francesca-gino-honesty-research-scientific-fraud-defamation-harvard-university), [three](https://www.scientificamerican.com/article/science-corrects-itself-right-a-scandal-at-stanford-says-it-doesnt/)), if scientists didn't publish bullshit, they would barely publish anything.

Sometimes you have to do the best you can, and the best you have is a bit cargo-cultish. You can't model the human body at the level of quantum electrodynamics. When you have very complicated things, even things that are pretty well understood, like aerodynamics, the explanations that help you reason about the system, like air flows faster over the top of the wing, so Bernoulli's law, yada yada yada, are oversimplifications that fail at the edge cases. They are a bit cargo-cultish. You can write a bunch of theories and computer models, but humans have to reduce daily practice to rules of thumb and checklists, that become cargo-cultish. Feynman has the skill and the luxury to think in purely scientific terms, but engineers, doctors and other practitioners invariably end up with some cargo-culting.

But if you try crazy cargo-cult things, and check the evidence, and you're not fooling yourself, it's no longer cargo cult. Even if you're just throwing spaghetti at the wall to see what sticks. The first step to doing good science is doing science that kind of sucks, and Feynman would surely agree. 

I’m not a follower of organized religion, but if it helps you be a happier better person than [vaya con dios, my darlings](https://www.youtube.com/watch?v=QqZ0Sdz_V40). Of course, one person's useful fiction is another's 'opium of the people.'

Bullshit is everywhere. The line between fraud, self-delusion, bullshit, cargo-cult, and solid evidence-based search for truth is finer than we might think.  There's an infinite supply of bullshit, and there's also a ton of human demand for it. People need bullshit to get through the day and the night. The eye sees what it is prepared to see and the mind believes what it wants to believe. 

But when it comes time to make high-stakes decisions, you have to make maximum effort to avoid fooling yourself. The Trump phenomenon and the widespread acceptance of facially ridiculous conspiracy theories does not bode well. There is going to be so much bullshit that it will drive out reality-based thinking, and not fooling ourselves will cease to be an option.

![/assets/2023/bullshit_tg.gif](/assets/2023/bullshit_tg.gif)

### The C.R.A.P. Framework, or a Tax on Bullshit

So far, we've noted that 1) generative AI is remarkably effective at generating bullshit; 2) there's an infinite supply and also inherently strong demand for bullshit. Furthermore, there's no effective test for human or AI-generated bullshit. 

If you could ask AI to check if something is AI-generated, you could probably tell an AI to create something that can defeat all the checks. AI detection seems like an 'AI-hard' problem.  [There's no Voight-Kampff test](https://bladerunner.fandom.com/wiki/Voight-Kampff_test). We currently have a situation where AI creates 6-fingered people, but given enough time and access to the test, a smart enough AI can keep fixing or regenerating content until it passes any given test. 

One wonders why the Tyrell Corporation didn't make a backdoor where you could just ask a replicant if it was human, or why X-rays didn't give it away. Matter is harder to fake then bits.

We have a bullshit problem. Software is eating the world. All our interactions are mediated online now with bits, and bits are pretty easy to fake. In a democracy, if we don't have a shared, reasonably well-grounded reality, if citizens are uninformed, if they don't have critical thinking, if they care more about being entertained by political reality-show bullshit than about the deadly serious consequences of shared decision-making, we're pretty screwed. 

We are starting to get ['Tank Man' selfies](https://futurism.com/the-byte/google-ai-generated-fake-tank-man) rewriting history. We have [Drake deepfakes](https://www.theverge.com/2023/4/18/23688141/ai-drake-song-ghostwriter-copyright-umg-the-weeknd), and will undoubtedly have new songs by long-dead artists. Not to mention [teenage boys making deepfake nudes of their classmates](https://www.bbc.com/news/world-europe-66877718). I want new Ray Charles and Aretha Franklin music as much as anybody but it's probably going to be bullshit and rewriting and cheapening their legacy.

A little game theory: if you have a 20% chance of getting busted for, say, taking contraband Cuban cigars through customs, the penalty must be more than 5x the benefit to be effective. When people don't pay a price for dishonesty, they are incentivized for more pervasive and extreme dishonesty, and then society goes to hell. Nobody trusts anyone because being untrustworthy pays. If the public sphere is a toxic stew of bullshit, there is no more shared reality. 

Which is what some people want, because it creates a vacuum where they can abuse their power unimpeded by democracy, rule of law and public opinion. Calling any speech you don't like a psyop and [inciting mobs against the speaker](https://www.nytimes.com/2023/09/18/opinion/trump-elon-musk-twitter.html) or planning to ['flood the zone with shit'](https://www.cnn.com/2021/11/16/media/steve-bannon-reliable-sources/index.html) gives the game away.

So what are we going to do about it?

There's a silver lining, which is that tech can (imperfectly) help filter bullshit. There is [more academic fakery](https://en.wikipedia.org/wiki/List_of_scientific_misconduct_incidents) than we would like to admit, but people are also [doing a better job catching it](https://www.wired.com/story/scientific-fraud-is-slippery-to-catch-but-easier-to-combat/). 

Suppose labs all used electronic notebooks with a cryptographically secure permanent audit trail? It would be a lot harder to alter data and photoshop X-rays.

We want to follow something like the [C.R.A.P. framework](https://www.linkedin.com/pulse/workplace-bullshit-crap-framework-cutting-down-pei-ying-chua/).

C - *Comprehend* the nature of the bullshit economy. *Who* is creating what bullshit and *why*. Especially the 'why', and what incentives exist around that bullshit.

R - *Recognize* bullshit when it starts stinking up the joint. Standards and probably legislation may be helpful.
- Social media should encourage the use of real names and privilege the reach of real people.
- In public speech there should be mandatory disclosure of AI-generated content. You can't just [generate the voice of a public figure saying stuff](https://www.politico.com/news/2023/07/17/desantis-pac-ai-generated-trump-in-ad-00106695) and pretend it's real. You can imitate and parody people, but you have to disclose that is what it is.
- When generative AI platforms are used to create images and videos, the content should contain metadata specifying that it is AI-generated and generate a proper audit trail of who created it and how. If you use open-source software, your PC or phone, you must similarly tag metadata. If you publicly post AI-generated content and don't tag it as AI-generated, it's fraud.
- Use the power of tech, real names, and tags to track how fake content emerges and spreads. It's possible to detect fake Astroturf virality by statistical anomalies in how promoted bullshit spreads vs. organic virality. The problem is 1) if it generates clicks, social media doesn't care if it's real or bullshit and 2) the focus shifts to creating clickbait that promotes itself.
- There's no foolproof signature for fake content, but the AI can make errors like spurious fingers and teeth. So, commit to the arms race to detect fakery.

A - *Act* to block bullshit. And the creators and spreaders have to be penalized. If you're a scientific faker, your papers are retracted. If you are a commercial faker, the profits have to get disgorged and those involved penalized. This means the whole value chain: The entity that sold fake stuff, the ad network / 'native advertising platform', and the media platform. The Taboola and Outbrain sidebar or footer of shame with scam medical ads, on ostensibly legit sites and even large company intranets and smartphone home screens, is a disgrace.

P - *Prevent* bullshit. Disrupt the economics of bullshit. The fundamental problem is that creating and distributing bullshit is free, and it pays big. The attention economy rewards clicks, and it doesn't care a whole lot if they are clicks on a toxic edgelord meme, or public-service investigative journalism.
- Tax all forms of online advertising. Attention-sucking bullshit is an externality. You have to internalize the public costs of bullshit.
- Liability for fraudulent bullshit all the way up and down the value chain.
- Support [anti-bullshit critical thinking education](https://www.callingbullshit.org/case_studies.html), [research](https://www.washingtonpost.com/technology/2023/09/23/online-misinformation-jim-jordan/), public-interest peer review of fake content, fact-checking of bullshit, other good-faith anti-bullshit programs.

Lies can be countered with the truth. Bullshit cannot, because it was never about truth in the first place. 'Bias' doesn't mean having a point of view, but having a belief that cannot be swayed by new information. The word 'meme' was invented by [Richard Dawkins](https://en.wikipedia.org/wiki/Meme) based on the 'selfish gene'. It's content that is highly adapted to arouse people to spread it. Humans see what we want to see, and good bullshitters leverage that, creating rage bait that confirms our priors. With generative AI, it's cheap to create infinite amounts of bullshit, and without penalties, bad actors are strongly incentivized to create it. Unless we build a marketplace of ideas with a better immune system, the singularity will end up being a singularity of bullshit.

There is no such thing as a 'free market.' If a market is designed by sellers, it will privilege winner's curse auction dynamics. If it's designed by buyers, it's not going to be bothered by buyer collusion. If it's designed by traders, it will privilege volatility. Markets are human institutions that need to be designed to solve problems and lead to efficient outcomes. Good markets have the maximum amount of freedom consistent with efficiency. The issue is not whether markets have rules, it's who will set the rules, and whether they will be Ticketmaster rules or Uber rules, which are cartels to extract the maximum out of the consumer, or crony capitalism rules or casino capitalism rules, or once in a blue moon some consumer-friendly rules. And we live in a democracy so how about we let voters decide?

(I am so tired of hearing people, it's not the Internet that led us here, it's human frailty. Or the old chestnut, guns don't kill people, people kill people. It's a motherfluffing system, dipshits. Oh yeah, there are no human-designed systems or systemic problems, only individual choices. God forbid we think about how systems impact individual choices and outcomes because that might lead to socialism, or something. WTF even is civilization and progress except figuring out how to cooperate and do better under better social and economic systems? Can't type more because my eyes just rolled back into my skull.)

The social project of producing knowledge inherently has a bias-variance problem. If it's too expensive to make and test and market new medical theories and therapies, resources and lives are wasted. If it's too easy to sell quackery, bullshit gains wide acceptance, and again resources and lives are wasted. I reject the idea that we can't work to get better, because it involves some kind of control of speech and thought. If you choose not to decide, you still have made a choice, and in this case your choice is bullshit. We should beware of unintended consequences, but we don't have any choice but to engineer online spaces for transparency and truth. We are responsible for the world we create.

It's hard to make predictions, especially about the future. When electricity was invented, at first we used it to replace candles, to do what you previously did a little better. Later, we invented washing machines, and air conditioners, and helped emancipate women and remake the geography of the South. With self-driving cars, at first you just replicate the human driver. But later, you can have cars that talk to each other and roads designed for them, and you can radically increase the density and throughput of the transportation system. Incremental change becomes systemic revolution.

Who knows what AI will be able to do in the near future? Some people say it might disrupt jobs, others say it will be no different from previous economic revolutions like electricity, and create as many jobs as it destroys. But in previous revolutions, technology changed but capital and labor remained highly complementary. Machines increase the demand for labor that can do what machines cannot. What happens if you can create a robot that is a perfect substitute for labor? Labor can't demand more in wages than it costs to rent the machine. Potentially, you get an economic singularity of robots building more robots, and laid-off workers living rough outside automated workplaces. You have to make sure the system is aligned with your values, or you end up with the [paperclip maximizer](https://medium.com/@happybits/paperclip-maximizer-405fcf13fc93) of [Nick Bostrom](https://www.youtube.com/watch?v=MnT1xgZgkpk), a line that keeps going up and to the right, but maximizing human misery.

Before AI we had a major bullshit problem. AI makes it much worse. The big new challenges will be hard to meet without the sort of clear-eyed reality-based thinking that AI itself disrupts, absent proper incentives.

![/assets/2023/bullshit2.gif](/assets/2023/bullshit2.gif)
