---
id: 1353
title: '&#8216;Big Data&#8217;'
date: 2013-01-05T07:24:47+00:00
author: Druce Vertes, CFA
layout: post
guid: /?p=1353
permalink: /2013/01/big-data/
post-views:
  - "5839"
dsq_thread_id:
  - "1003254948"
categories:
  - Uncategorized
---
If &#8216;The Graduate&#8217; were made today, Benjamin Braddock might hear a well-meaning uncle stage-whisper &#8216;Big Data&#8217; instead of &#8216;Plastics.&#8217; (Runners-up: &#8216;The Cloud&#8217;, &#8216;Social Discovery&#8217;, &#8216;Gamification&#8217;, the [list goes on](http://www.languagemonitor.com/high-tech-buzzwords/top-tech-buzzwords-everyone-uses-but-dont-quite-understand-2012/).) &#8216;Big data&#8217; is a buzzword that people throw around a lot. What does it mean? Large data sets are not new. The IRS, the Census, Walmart, money center banks have always had big data sets.

What&#8217;s changed?  
<!--more-->

  
Look at Google. You type &#8216;Justin,&#8217; and within milliseconds it offers &#8216;Bieber&#8217; as a completion. You hit enter, and it fires off a query to a bunch of computers (guessing low hundreds) that each have part of the Google index in RAM. They return answers, and the results are ranked. Then the page ids are fired off to a different set of computers that has the entire text of the relevant Web pages in RAM, and the relevant excerpts are retrieved. Finally, a Search Engine Results Page (SERP) gets built, with personalized ads etc. [All this happens in milliseconds](http://www.googleguide.com/google_works.html).

This was a revolutionary approach to working with large datasets. What&#8217;s changed is not just that there are more large datasets, but also, the tools Google pioneered have gone mainstream, and new computing stacks let you apply powerful algorithms in real time.

Near the bottom of the stack is the database. Oracle databases can handle any amount of data. Could Google have been built on top of Oracle? Probably. Should they have? No. First, Oracle&#8217;s business model isn&#8217;t designed for it, licensing for millions of servers would cost a fortune! Second, Oracle is poorly adapted to the problem. It can do complex queries across massive disks pretty efficiently. But if you want to split data across hundreds of servers and keep it in memory, you have to give up much of the functionality Oracle provides, and it would still have complexity and features that would take up resources, driving up the cost and degrading performance.

A traditional SQL database is built on [ACID](http://en.wikipedia.org/wiki/ACID) principles: (Atomicity, Consistency, Isolation, Durability), that make databases reliable for banks and accounting systems. A transaction either works or it doesn&#8217;t. It can&#8217;t be half-completed, where money is debited from one account and not credited to another. Database activity can be viewed as a single series of transactions and queries &#8211; any query will see the database in a consistent state in the chain. Each transaction is isolated from every other, even if they happen concurrently, they don&#8217;t affect each other, as if they happened sequentially. And once a transaction is committed, it stays committed, regardless of power loss, network failure, etc.

ACID imposes a cost in complexity and performance. There&#8217;s a computer science result called [Brewer&#8217;s CAP theorem](http://www.julianbrowne.com/article/viewer/brewers-cap-theorem) ([longer version, by Brewer](http://www.infoq.com/articles/cap-twelve-years-later-how-the-rules-have-changed)). Pick any two: _Consistency_, knowing a single up-to-date state of the data; high _Availability_ for writes; and _Partition_ tolerance, no impact from network failure. The CAP theorem means that when you have a distributed system that communicates over a network, you have to make tradeoffs. Â If you have a master database and a few slaves mirroring the master for reporting, web queries, there will always be an instant where the master has been updated and the slaves are still out of date. There is no perfect disaster recovery (DR) system, where two systems are 100% in sync and if one fails, you can immediately fail over to the other with a guarantee of no data loss. There is always that instant where if the network fails, things are not 100% in sync. ACID and distributed systems don&#8217;t mix perfectly.

But Google doesn&#8217;t need ACID. Nobody cares much if two similar queries at the same time yield slightly different Google results, or Facebook walls. They do care if they don&#8217;t get it back in milliseconds.

And networks have grown incredibly fast, large and reliable. Meanwhile, CPUs are still getting cheaper, but individual CPU cores are not gaining performance as quickly, and speeding them up is less energy-efficient than buying more low-power CPUs. So companies like Google, Facebook, etc., use a different set of non-ACID tools in big server farms that let lots of cheap servers connected over fast networks work together.

Wags sometimes refer to the new databases, in contrast to ACID, as &#8216;BASE&#8217;: _Basic Availability, Soft state, Eventually consistent_. &#8216;NoSQL&#8217; is more commonly used. Different NoSQL database systems are optimized for different use cases: key-value stores, document databases, graph databases. There has been an explosion of new database systems &#8211; the growing [blue area in this chart](http://gigaom.com/cloud/confused-by-the-glut-of-new-databases-heres-a-map-for-you/).

What does the Big Data stack look like?

  * Clusters of virtual machines on [Amazon Web Services](http://aws.amazon.com/), [Heroku](http://www.heroku.com/), Rackspace, etc.: PaaS (platform as a service)
  * NoSQL databases for non-ACID applications: MongoDB, Cassandra, CouchDB, Redis, Riak (a [popularity chart](http://architects.dzone.com/articles/graph-nosql-database-linkedin), [Google Trends](http://www.google.com/trends/explore#q=mongodb,couchdb,redis,hbase,nosql)).
  * Platforms to distribute algorithms over those clusters &#8211; [Hadoop](http://hadoop.apache.org/), [MapReduce](http://research.google.com/archive/mapreduce.html).
  * Analytical software to run complex machine learning algorithms on those platforms.

_The last bullet point is what people most commonly associate with &#8216;Big Data&#8217;._ Not only can Google return the most frequent completion as you type, it can update the most frequent searches in real time. More critically for the Google business model, it can predict which ad link you are most likely to click on, based on your recent activity, current location, everything it knows about your demographics and psychographics from your searches, your clicks on all the web sites in Google&#8217;s ad network, your emails in Gmail. Powerful machine learning algorithms like [support vector machines (SVM)](http://en.wikipedia.org/wiki/Support_vector_machine) can run on clusters in a highly parallel manner to model behavior using hundreds of features and make predictions in real time.

If you have all the parameters to make a decision, given enough data and training, machine learning will generally find a good model and make useful predictions. Nevertheless, ad networks still present Christian singles ads to old married non-Christians. If you don&#8217;t collect the right parameters and feed them in the right form, you get garbage in, garbage out. If the data you have isn&#8217;t predictive, or you have the wrong model, [having a lot of data isn&#8217;t going to help you](http://www.linkedin.com/today/post/article/20130103045241-25760-are-we-all-being-fooled-by-big-data)[<sup>1</sup>](#1).

Bottom line: A new computing stack, pioneered 15 years ago by Google, has gone mainstream. It consists of ultra-cheap small computers connected by fast networks, plus distributed algorithms, for new applications that run at web scale and don&#8217;t have strong consistency requirements, and can make sophisticated predictions and decisions in real-time.

People call it &#8216;Big Data&#8217;, but what&#8217;s new and big is the computing power that can be directed at data in real-time. The distinguishing feature of this paradigm is dealing with more data than can fit in a computer&#8217;s memory by splitting up the task over many computers working in parallel in real-time, instead of doing them sequentially on a disk in batch mode.

In that sense, a traditional SQL data warehouse with petabytes of transaction data, batch processing business intelligence for online analytic processing and data mining, is not in the new Big Data paradigm, in the same sense as a real-time machine learning ad-serving engine that may have a smaller database.

It&#8217;s not the data that got big, it&#8217;s the computers that got small, fast, very numerous, and very very clever.

  
&#8220;One word: Cat pictures. Wait, that&#8217;s two words.&#8221;

<a name="1"></a><sup>1</sup> <small>It&#8217;s undeniable that bad models were a signficant cause of the financial crisis. But it&#8217;s an error to say that <a href="/2011/07/fannie-freddie-and-the-financial-crisis/">the financial crisis</a> was caused by big data gone awry. First of all, the bad models just so happened to be <a href="http://mathbabe.org/2012/12/20/nate-silver-confuses-cause-and-effect-ends-up-defending-corruption/">the ones that made the bankers the most money in the short run</a>. It wasn&#8217;t so much bad big data as it was misaligned incentives, agent-principal problems, in some cases outright looting. Second, to my knowledge, there was exactly one firm that had a big data system that gave an integrated real-time picture of firm risks: Goldman Sachs, which averted life-threatening losses (although they were at risk if AIG or the whole system went down). Goldman&#8217;s system priced every instrument, however exotic, in real-time using the same curves, forward rates, volatilities, and correlation matrices, and allowed them to compute risk and test shocks against the whole firm. They were the only ones who said, we&#8217;re losing money every day on this when our model says we shouldn&#8217;t, let&#8217;s take a close look at this&#8230; and then cut losses and go short. Unlike Goldman Sachs, which grew organically, every other firm was cobbled together with mergers, and had different groups which did their own analytics in disparate systems which someone then struggled to integrate at the top level. They lost billions because, <em>unlike Goldman, they didn&#8217;t use a big data paradigm.</em> And also because <a href="http://nymag.com/news/business/46476/index6.html">âAt Goldman, &#8230;when they say get out, they get out. At [competitor], when [the president] says get out, people start negotiating.â</a></small>