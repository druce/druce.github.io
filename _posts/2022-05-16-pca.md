---
id: 7119
title: 'How I learned to stop worrying and love PCA: The optimal threshold for PCA dimensionality reduction'
date: 2022-05-16T01:01:01+00:00
author: Druce Vertes
layout: post
guid: /?p=7119
permalink: /2022/05/PCA
categories: datascience
tags: datascience

---
>A PCA refresher, and a principled algorithm for selecting how many principal components to keep for dimensionality reduction.
<figure>
  <img
  src="/assets/2022/fig1.png"
  alt="Figure 1.">
  <figcaption>Fig. 1. A random 2D data set with singular vectors scaled by singular values</figcaption>
</figure>
<!--more-->

I was refreshing my knowledge of Principal Component Analysis (PCA) and the Singular Value Decomposition (SVD) and came across this [excellent method](http://www.pyrunner.com/weblog/2016/08/01/optimal-svht/) for selecting an optimal rank for dimensionality reduction.

When we apply PCA for dimensionality reduction or compression, we have to pick a number of components to use for a reduced-rank approximation. The "PCA 101" best practice is to plot the singular values and look for an "elbow" where the significance of components stops dropping off rapidly, and which captures a large fraction of the variance. At that point we think we have captured the true rank of our data, and the remaining components are mostly noise.

Here is the more systematic approach:
- Perform PCA on white noise with some standard deviation $$\sigma$$ and model the singular value profile of white noise.
- If you assume your real data matrix has a true rank plus white noise, you can estimate the $$\sigma$$ of the noise in your data by fitting your least-significant singular values to the singular value profile of white noise.
- Using this estimate of the white noise in your data, pick a threshold below which additional ranks add more noise than data, and don't improve a linear regression against the PCA components.

This is the method described in [this paper](https://ieeexplore.ieee.org/abstract/document/6846297) by M. Gavish and D. L. Donoho, "The Optimal Hard Threshold for Singular Values is $$4/\sqrt {3}$$ ," in IEEE Transactions on Information Theory, vol. 60, no. 8, pp. 5040-5053, Aug. 2014, doi: 10.1109/TIT.2014.2323359.

In the rest of the post, we'll do a quick refresher on PCA and then apply this method in Python.

### PCA Review

- In my first job back in the 80s, PCA came up, and I asked my [legendary boss](https://www.stern.nyu.edu/faculty/bio/richard-berner) "what's PCA?". He answered, "it's a way to fit everything against everything at the same time". PCA, or more generally the SVD from which it derives, is not just dimensionality reduction, it's an X-ray of our data to find *all* the linear relationships. From an SVD decomposition, you can quickly do not just compression and dimensionality reduction, but regression, classification like facial recognition, anomaly detection, recommender systems. For pretty much anything you want to do with data, PCA and SVD can be a good starting point.
- Linear algebra review: Some texts teach algebra as the study of matrices and some teach it as vector fields and linear maps; these concepts are isomorphic. Data and transformations are interchangeable, they are both just matrices. Multiplying $$AB$$ can be viewed as applying transformation $$A$$ to matrix $$B$$ or left-multiplying transformation $$B$$ by the transpose of $$A$$. In other words, $$AB = (B^TA^T)^T$$. A matrix exists in a vector space, and so do linear transformations themselves. Geometrically, every linear algebra transformation is rotation, dilation, reflection or projection, and combinations thereof. 
- We can decompose or factorize any matrix in different ways. Suppose we have a data matrix with $$m$$ rows (observations, samples) and $$n$$ columns (features, predictors). Singular value decomposition factorizes any $$m \times n$$ matrix A as: $$A = U \Sigma V^T$$ where (don't worry too much if the bullets below are Greek):
	- $$U$$ is the $$m \times m$$ matrix of the eigenvectors of $$A^TA$$, the **left singular vectors**. They capture all the relationships between the **observation rows**. $$U$$ is an orthogonal matrix so it represents a rotation and possibly reflection of data in $$m \times m$$ row space.
	- $$V^T$$  is the $$n \times n$$ matrix of the eigenvectors of $$AA^T$$, the **right singular vectors**. They capture all the relationships between the **feature columns**. $$V^T$$ is an orthogonal matrix so it represents a rotation and possibly reflection of data in $$n \times n$$ column space.
	- $$\Sigma$$ is the diagonal dilation (scaling) matrix, the **singular values**. The left and right singular vector matrices are orthogonal with norm 1, so they don't contain any scaling data.  The singular values describe the scale of each principal component, connecting the unitary scale of the singular vectors to the true scale of the original data. The singular values are, by convention, sorted by size as you go down the diagonal. This makes the decomposition unique up to changes in sign. The eigenvalues are guaranteed to be real and positive because $$A^TA$$ and $$AA^T$$ are products of matrices by their transpose and therefore positive definite by construction.
	- The existence of this decomposition partly proves the first sentence of this bullet: any matrix transformation can be decomposed into a combination of a rotation in row space, a dilation scaling each column by a factor, and a rotation in column space. (Projection means mapping into a lower-dimensional subspaces)
- The principal components are defined as $$P = U\Sigma$$ so by substitution we can get the original data back as $$A=PV^T$$. Multiplying each side by $$V$$ we get $$P=AV$$. (The inverse of an orthogonal matrix is its transpose.) So matrix $$V$$ is the factor loadings. Each principal component is a weighted average of the feature columns, where the weights form an orthogonal matrix.[^1]
- PCA is therefore simply an orthogonal change of basis to a better basis for this data set. What is a good basis? Suppose you are analyzing a spiral galaxy. Some weird basis from a random point in random directions may not be a good basis. A good set of coordinates might be:
   - Set the origin at the center of the galaxy
   - Set the x-axis along the principal axis which maximizes the variance along the x-axis and minimizes the variance perpendicular to the x-axis (along the remaining axes). 
   - Set the y-axis orthogonal to the y-axis in the direction that maximizes the remaining variance
   - Set the z-axis to the only possible axis orthogonal to the first two, which is the shortest possible axis.
   - When you change the basis with this orthogonal matrix, the overall variance, relationships of data (all distances and angles) are conserved.
- Each principal component column is a linear combination of the original feature columns. You can think of each principal component as representing a *latent variable*. For instance, if I try to predict a recession using a large dataset of economic activity, I might find principal components corresponding to growth, inflation, monetary conditions, labor markets, consumer demand, investment demand, fiscal policy, the foreign sector. 
- Each principal component is *independent*. It has 0 correlation with every other principal component. The slope of a regression between any two principal components is 0.
- The labor market component might have big loadings on employment growth, wages, big negative loadings on unemployment rate and jobless claims, and subtracts enough of the other components to eliminate any correlation with any other component. The labor component's singular value would measure its overall contribution to the variance.

[^1]: The SVD also creates linear combinations of rows ordered by significance. Suppose we have a data set of observations of organisms, with feature columns like height, weight, body temperature, life expectancy etc. When we do PCA, we reformulate our columns as principal components $$A=PV^T$$. One principal component might strongly weight the animal-ish features and another the plant-ish features, and a maybe less-significant component strongly weights the remaining features that distinguish mammals from reptiles. We can also say $$P'=\Sigma V^T$$ and then $$A=UP'$$ where $$P'=U^TA$$. Now the $$P'$$ matrix is the principal components of $$A^T$$. If 'animal-ishness' is the most important latent variable explaining the variance, then the first component of $$P'$$ will be the linear combination of observations, weighted by 'animal-ishness'. The SVD collects similar rows exactly the same way it collects columns. If $$A=U\Sigma Vt$$, then $$A^T=V\Sigma U^T$$, which is the SVD of the transpose.
## An Example
Suppose we generate some random 2-dimensional data, with standard deviation 2 in the $$x$$ direction and 1 in the $$y$$ direction. Then we rotate it 30Â° counterclockwise. Now the data columns are correlated, each point is a mix of the original $$x$$ and $$y$$ directions.

<figure>
  <img
  src="/assets/2022/fig2.png"
  alt="Figure 2.">
  <figcaption>Fig. 2. A random 2D data set.</figcaption>
</figure>

Now we do the SVD on our data. If we multiply our right singular vectors (norm=1) by the singular values, we get vectors showing our principal axes, with their norms being the standard deviations we used to construct the data.

<figure>
  <img
  src="/assets/2022/fig1.png"
  alt="Figure 3.">
  <figcaption>Fig. 3. A random 2D data set, with singular vectors scaled by singular values.</figcaption>
</figure>

If we plot our principal components, we have our unrotated data back. The SVD or PCA analysis essentially just unscrambles or unf\*\*ks the basis.

<figure>
  <img
  src="/assets/2022/fig4.png"
  alt="Figure 4.">
  <figcaption>Fig. 4. Principal components of the random 2D data set.</figcaption>
</figure>

Suppose I take figure 4 and use the principal axis eigenvectors as my basis. In this coordinate system, the standard deviation ellipses become circles. Now, suppose I know some $$x$$ value and I want to predict $$y$$. The points with a known $$x$$ form a line (probably not horizontal or vertical in this basis). If I project the origin onto that line, I get the point closest to the origin in likelihood space and therefore, after I convert that point to coordinates in the original basis, I get the maximum likelihood estimate of $$y$$.  So, not only does the SVD provide a simple regression formula, it breaks down the data in such a way, that if I have $$n$$ known variables I can project the origin onto the subspace defined by those $$n$$ variables, and get the maximum likelihood estimate of the missing variables. That's what is meant by 'regressing everything against everything'. Any regression you might want to do falls out of the SVD via matrix multiplication.[^2]

[^2]: It's worth mentioning this small mathematical difference between PCA and ordinary least squares regression. When we do standard OLS regression with $$x$$ as the explanatory variable and $$y$$ as the dependent variable, we get a regression line defined by $$y=ax +b$$. We estimate the $$a$$ that minimizes the squared error in the $$y$$ direction. If we then fit $$x = cy +d$$ on the same training data, $$c$$ will not be exactly $$1/a$$, because we are minimizing the error in the $$x$$ direction. Even if you have a perfect normal distribution, you only get the same value if the slope is 1. There will also be a difference depending on the departure from normality, outliers etc. In other words, vanilla OLS linear regression minimizes the expected squared vertical distance between the regression line and target variable.  when you flip the axes to regress be against a, you are minimizing what was formerly the horizontal distance. PCA fits everything against everything simultaneously, and solves for a single slope which minimizes the square of the error as measured by the distance between points and the regression line, i.e. the perpendicular projection onto the regression line.


### Optimal PCA truncation

Let's generate an image with known rank of 2. To do this, we create $$U$$ and $$V$$ as $$500 \times 2$$ matrices:

For U, we choose these 2 principal components (we just want something a little complicated and weird to make an interesting image):

$$U_1 = cos(4x)$$

$$U_2 = e^{(x^2)} sin(4x)$$

<figure>
  <img
  src="/assets/2022/fig5.png"
  alt="Figure 5.">
  <figcaption>Fig. 5. U matrix (500 x 2).</figcaption>
</figure>

For V, we choose these 2 principal components:

$$V_1 = sin(8x)$$

$$V_2 = e^{(x^2)} cos(8x)$$

<figure>
  <img
  src="/assets/2022/fig6.png"
  alt="Figure 6.">
  <figcaption>Fig. 6. V matrix (500 x 2).</figcaption>
</figure>


and make a $$\Sigma$$ equal to the identity matrix and compute a $$500 \times 500$$ pixel image as $$U\Sigma V^T$$

<figure>
  <img
  src="/assets/2022/fig7.png"
  alt="Figure 7.">
  <figcaption>Fig. 7. Image of contrived 500 x 500 rank 2 matrix.</figcaption>
</figure>


Now let's create some white noise with $$\sigma=1$$ and add it to the image:

<figure>
  <img
  src="/assets/2022/fig8.png"
  alt="Figure 8.">
  <figcaption>Fig. 8. Image of contrived 500 x 500 rank 2 matrix.</figcaption>
</figure>

Now let's look at the singular value profiles for the original image, the white noise, and the combined noisy image.

<figure>
  <img
  src="/assets/2022/fig9.png"
  alt="Figure 9.">
  <figcaption>Fig. 9. Singular values of contrived rank-2 matrix.</figcaption>
</figure>

<figure>
  <img
  src="/assets/2022/fig10.png"
  alt="Figure 10.">
  <figcaption>Fig. 10. Singular values of white noise.</figcaption>
</figure>

<figure>
  <img
  src="/assets/2022/fig11.png"
  alt="Figure 11.">
	<figcaption>Fig. 11. Singular values of rank-2 matrix + noise.</figcaption>
</figure>

In Figure 11, it's easy to see in this contrived example where the rank-2 singular values run out of significance and the noise dominates.

In this example, we know the $$\sigma$$ of our noise and number of rows $$m$$, so according to the Gavish-Donoho paper, the truncation threshold can be computed as $$\frac{4}{\sqrt{3}}\sqrt m \sigma$$, approximately 51.6. We select all the singular values larger than 51.6 and add 1, so we can truncate our the rank to 2, in other words we take our original SVD
$$A=U \Sigma V^T$$
and we keep only the first 2 columns of U, the top left 2x2 part of $$\Sigma$$, and the first 2 rows of $$V^T$$. We recompute $$A$$ from the truncated matrix as
$$A_2=U_2 \Sigma_2 V_2^T$$. 

We can see that this does a great job of reconstituting our matrix and eliminating most of the noise.

<figure>
  <img
  src="/assets/2022/fig12.png"
  alt="Figure 12.">
	<figcaption>Fig. 12. Image of low-rank approximation of noisy matrix, using optimal truncation of noisy principal components.</figcaption>
</figure>


Alternatively, if we don't know the $$\sigma$$, Gavish-Donoho tells us how we can estimate it. Basically, we can model at what point we are adding more noise than signal, or at what point a loss function further down the pipeline, like a regression against PCA components, stops improving.

Finally, let's apply the same process to a photo, instead of a contrived test image:

<figure>
  <img
  src="/assets/2022/fig13.png"
  alt="Figure 13.">
	<figcaption>Fig. 13. Original uncompressed test image.</figcaption>
</figure>

We add a small amount of noise:

<figure>
  <img
  src="/assets/2022/fig14.png"
  alt="Figure 14.">
	<figcaption>Fig. 14. Test image + noise</figcaption>
</figure>

We compute our optimal PCA truncation rank as 29, which yields this image:

<figure>
  <img
  src="/assets/2022/fig15.png"
  alt="Figure 15.">
	<figcaption>Fig. 15. Optimally-truncated rank-29 image</figcaption>
</figure>

We compare our 'optimally-truncated' image to a rank-100 image:

<figure>
  <img
  src="/assets/2022/fig16.png"
  alt="Figure 16.">
	<figcaption>Fig. 16. Rank-100 image</figcaption>
</figure>

The rank-29 compression is acceptable. But it didn't seem to perform any noise reduction. And adding more dimensions does improve the image. It's not obvious that adding more ranks beyond the 'optimal' threshold adds more noise than data.

Maybe the brain is really good at filtering the noise.

Nevertheless, using this threshold as a starting point and adding additional ranks for a margin of safety seems likely to capture what is needed for facial recogination or image classifiation. Even tripling the size of the matrix adds only a modest improvement. 

The Gavish-Donoho optimal truncation method is a systematic quantitative way to eyeball the elbow and pick an appropriate threshold. Optimal truncation depends on what we are going to use the output for and what our loss function is. This is certainly a more systematic approach and a good starting point. But of course YMMV.

### A Final PCA Example - Stat Arb ###

Here's a fun appplication of PCA:
- Perform a PCA analysis of daily returns for all the stocks in the S&P for the last year. Don't truncate, keep the full-rank approximation of the PCA component loadings.
- The next day, at 10am, take today's intra-day returns and compute the intra-day return on all 500 principal components using the loadings estimated from the last year of data.
- Using today's intra-day returns for each of the 500 principal components, compute the return you would expect for each individual stock in the S&P. Subtract the estimated return from the actual return.
- If you rank the stocks by outperformance / underperformance vs. the expected return based on the other 499 stocks, then the outperforming stocks either have some stock-specific news, or there is some market pressure like a big buyer which might be expected to mean-revert. Similarly the underperforming stocks either have good news, or there is some market pressure like a big seller which might mean-revert.
- So, after applying some filter for stocks with market-moving news, a reasonable strategy might be to buy the highest-performing stocks and keep them for a week (or some backtested interval), short the worst-performing stocks and keep them for a week, and hedge any net exposure with S&P futures.
- This, in its simplest form, is statistical arbitrage.
