---
id: 7124
title: 'Thinking About ChatGPT'
date: 2023-03-29T01:01:01+00:00
author: Druce Vertes
layout: post
guid: /?p=7124
permalink: /2023/03/chatgpt
Ncategories: datascience
tags: datascience

---
>*I think it’s comparable in scale with the Industrial Revolution or electricity — or maybe the wheel. - Geoffrey Hinton*
>
>*Any sufficiently advanced technology is indistinguishable from magic. - Arthur C. Clarke*
<!--more-->

[Large Language Models](https://en.wikipedia.org/wiki/Large_language_model) like [GPT-4](https://openai.com/research/gpt-4) from [OpenAI's](https://openai.com/)  [Generative Pre-trained Transformer](https://en.wikipedia.org/wiki/Generative_pre-trained_transformer?ref=hackernoon.com) series, which power [Bing AI](https://www.bing.com/new) and [ChatGPT](https://chat.openai.com/), give computers the ability to understand and communicate with natural language at a near-human level. In most cases it's hard to tell if text was generated by ChatGPT or by a human. This will have far-reaching implications.

Here's a primer on LLMs and the latest versions of GPT. 

# History and Background

### A Short History of Natural Language Processing (NLP) 

- Statistical language models go back at least to [Claude Shannon's 1950 entropy paper](https://www.princeton.edu/~wbialek/rome/refs/shannon_51.pdf)
- Statistically if you play Wordle and H is the second letter, what is first letter mostly likely to be? I'm going to go with S (T and C might be other choices, Q would not.) 
- When I was a kid on the way to school in the subway I would see  this ad for [secretarial school](https://playingintheworldgame.com/2012/09/21/f-u-cn-rd-ths-if-you-can-read-this/) to learn speedwriting shorthand:
   ![](/assets/2023/gpt/Speedwriting.png)
- If all characters were equally likely, ignoring punctuation and spaces, the amount of information in English text would be log2(26)=4.7 bits per character. In fact people can fill in missing characters based on context because there is a lot of redundancy. Shannon found the true information rate to be between 0.6 and 1.3 bits per character, so you could probably compress English text to 1/4 the size, which is in fact close to what we find with Zip (73% reduction). The redundancy can be considered a form of 'parity stripe' that makes it easier to read error-free text.
- At a far more complex level (that implicitly encodes a mind-blowing amount of human knowledge), what ChatGPT does is: statistically predict the probability of tokens (words or word fragments or n-grams) and generate sequences of words based on the likelihood of being appropriate in context. This turns out to be [unreasonably effective.](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) 
- In the 1980s and 90s there was a lot of NLP work on taking sentences and
	- tagging parts of speech
	- stemming and lemmatizing words (dealing with suffixes like -ing and -ed)  
	- building syntax trees
	- see e.g. the [NLTK module](https://www.nltk.org/) and [An Introduction to Part-of-Speech Tagging and the Hidden Markov Model](https://www.freecodecamp.org/news/an-introduction-to-part-of-speech-tagging-and-the-hidden-markov-model-953d45338f24/)
- In the 2010s, users of Google Translate were shocked at a sudden improvement. Translations were suddenly much more natural. [Google had switched to an end-to-end machine learning model.](https://www.nytimes.com/2016/12/14/magazine/the-great-ai-awakening.html)
- A key NLP concept is the *embedding*. We represent each token (word, word fragment or n-gram) as a vector of 100s to 1000s of floats. The embedding represents a word by coordinates in a *latent space* that tell you something about how it relates to every other word. With an algorithm like *word2vec*, you can train an algorithm to predict the context each word is found in, or what words are nearby. Simultaneously with training the prediction algo, you train the embedding that best represents each word. You discover embeddings that encode surprisingly deep 'meaning' purely based on context. For instance, if you take the vector for 'France' - 'Italy' + 'Rome', the closest word to that vector in the latent space represents 'Paris'. Or 'dog' - 'puppy' = 'cat' - 'kitten'. [According to this post, ChatGPT-3 used embeddings of dimension 12288.](https://dugas.ch/artificial_curiosity/GPT_architecture.html#:~:text=Of%20course%2C%20the%20embedding%20dimensions,a%2012288%20dimension%20embedding%20vector.)
- It turns out that in order for an embedding to predict what context a word appears in, it has to capture all the word's varied meanings, connotations, and grammatical properties. These embeddings can be used by deep learning models in tasks like sentence completion, sentiment analysis, translation, etc.
- Embedding or *projecting* an object into a latent space by training an embedding model is an extremely important concept in machine learning. If you are an investor, you may be familiar with factor models. If you calculate the beta of a stock relative to the S&P, you are projecting the stock into a 1-dimensional latent space. If you calculate the stock's betas relative to 10 factors, you are projecting it into a 10-dimensional latent space. If you take any two stocks, their projections into these latent spaces concisely describe aspects of how their behavior relates to one another. Factor models are linear projections, but for deep learning applications we use nonlinear projections using neural networks. Some have applied deep embeddings successfully to financial markets, see e.g. this paper by [Cong et al.](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3554486).
- Another example of the use of embeddings and latent spaces is image *style transfer*:
  - Input a photograph
  - Input an Impressionist painting
  - Train an algorithm to apply the style of the painting to the photograph
    - We project the macro features of the photograph into a latent space representing the fact that it shows a haystack in a green field under a blue sky.
    - We project the micro features of the painting into a latent space representing micro features like palette and brush strokes. 
    - Then we tell the algorithm to find an image that is close to the photograph when projected into the 'macro feature' latent space, while also being closest to the painting when projected into the 'micro feature' latent space. Finding the image that minimizes the sum of those two distances yields an impressionist painting-like image of a haystack under a blue sky.
  - Similarly, when we tell a generative text-to-image program like Stable Diffusion to generate "Haystack in a green field under a blue sky in the style of Monet", we project the sentence into a latent space and find the image that would have the closest projection in the same latent space.

### A Short History Of Deep Learning

- A lot of theory and empirical work on NLP was suddenly rendered moot by the far better-performing 'statistical' ML models. This is a recurrent theme in machine learning research,  the [bitter lesson](http://www.incompleteideas.net/IncIdeas/BitterLesson.html https://twitter.com/hausman_k/status/1612509549889744899). Humans trying to make thoughtful models of how to perform complex tasks often have been crushed by conceptually simple models backed by massive computation. Otherwise stated, "The algorithm knows best" and "GPU go brrrr." 
- Neural networks had been around since the 40s and 50s. But research was pretty dead by the mid-80s. They were considered historical artifacts and no one could get any funding to research them in the US. But in 1986 Geoffrey Hinton, after moving to Canada, popularized the backprop algorithm to train them. Then computers got more powerful and datasets got bigger, and in 2012 Hinton's student Alex Krizhevsky achieved state-of-the-art (SOTA) image recognition with [AlexNet](https://en.wikipedia.org/wiki/AlexNet). Since then, deep learning using neural networks has gone supernova, with better GPUs, better software architectures, scaled up to train on big data, leading to better results, driving more investment in hardware, software, and research, perpetuating the cycle.
- [Transformers](https://towardsdatascience.com/beautifully-illustrated-nlp-models-from-rnn-to-transformer-80d69faf2109) are the current SOTA architecture for dealing with sequence to sequence tasks, like translation and question answering. They were invented by [Google researchers](http://nlp.seas.harvard.edu/annotated-transformer/) in 2017 (None still work at Google, one works at OpenAI). [Related blog post](https://dzone.com/articles/a-deep-dive-into-the-transformer-architecture-the). Key concepts:
	- A transformer layer takes a sequence of vectors and outputs a sequence of vectors. So it can be trained, with an appropriate loss function, to take the word embedding of a prompt and output the word embeddings of a response.
	- A transformer layer doesn't process tokens sequentially, it looks at e.g. a whole sentence or text fragment at a time.
	- An *attention* mechanism is a neural network architecture we can train to predict the importance of each token in generating an output.
	- *Multi-headed* attention learns multiple attention models to determine the importance of each token in different tasks. For instance, a local context vs. importance in the whole sentence, importance on multiple dimensions of the output.
	- *Self-attention* learns how much each token in the input relates to every other token in the input.
	- Transformers can be *stacked*. The first seq2seq transformer layer outputs a sequence that is an intermediate representation that goes to another layer, and so on. Chat-GPT3 has around 400 layers per [Stephen Wolfram](https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/)
	- See OpenAI's [Lilian Weng](https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/) for more on the transformer family and variations.

### A Short History of LLMs

- [ELIZA](https://en.wikipedia.org/wiki/ELIZA) is sometimes cited as a precursor of LLM chatbots, but it has little in common. It was more of a Mad Lib template engine that would parse key words and try to put them into some sensible-looking response. It didn't perform a task we would recognize as machine learning in the sense of trying to learn or minimize a loss function. 
- Evolution of LLMs
	- GPT-1 (June 11, 2018) : 117m parameters (OpenAI)
	- BERT (October 2018): 340m parameters (Google)
	- GPT-2 (February 14, 2019): 1.5b parameters
	- [GPT-3](https://arxiv.org/pdf/2005.14165.pdf) (May 28, 2020): 175b parameters (4 bytes per param = 700GB just to store params).
	- LaMDa (March 2022) - 137b parameters (Google)
	- GPT-3.5 (November 2022) Parameters: ??? A modest update to GPT-3, went to context size of 8k and 32k tokens. This was launched as [ChatGPT](https://chat.openai.com/) and set off all the hullabaloo.
	- [GPT-4 (2023)](https://cdn.openai.com/papers/gpt-4.pdf): Powers [Bing AI](https://www.bing.com/new). Parameters: ??? OpenAI is now ClosedAI, no longer provides algo details. Asking Bing AI itself, guesses range from 300 billion to 1 trillion. Based on the long lag between GPT-3 and GPT-4, and the existence of other 1-trillion-parameter models, I would initially guess at the higher end. OpenAI doesn't mess around, maybe more likely a 10x increase than a 2x increase. On the other hand, an Nvidia A100 delivers 312 teraFLOPs, so would generate about 1 token per second on a 312-billion-parameter model. Empirically I see 200-word answers in ~20 seconds from Bing AI (GPT-4), 300-word answers from ChatGPT (GPT-3.5) in ~ 1 minute. I'm not sure if you can or would want to use a lot of A100s per end user (list price $8K plus 300 watts of power, although Microsoft buys both pretty cheap). So 1 A100 and 2s per token might put an upper bound around 624b parameters. Since they may be focusing more on go-to-market than major model improvements, plus ChatGPT-3 seemed big enough to do a good job, that would argue for closer to 2x GPT-3. 
	- Facebook LLaMA (2023): 65b (biggest model)  
	- Alpaca (2023): 7B parameters (similar to LLaMA small model) in an open source model you can train on your desktop

# ChatGPT-4 Overview
- Training
	- Uses large web scrapes (these numbers are from [GPT-3 paper](https://arxiv.org/pdf/2005.14165.pdf)):
    
		 | Data set  | Size | %    |
         | --------- | ---  | ---- |
		 | [ Common Crawl](https://en.wikipedia.org/wiki/Common_Crawl "Common Crawl")  | 410 billion pages | 60% |
		 | WebText2 | 19 billion |  22% |
		 | Books1 | 12 billion | 8% |
		 | Books2 | 55 billion | 8% |
		 | Wikipedia | 3 billion | 3% |
         
	 - I think it has GitHub and other code repos, tech paper just says 'data licensed from third-party providers'
	     - [many languages](https://commoncrawl.github.io/cc-crawl-statistics/plots/languages)
		 - [top websites](https://commoncrawl.github.io/cc-crawl-statistics/plots/domains)
        
	- Training follows a bootstrapping process - Reinforcement Learning from Human Feedback
		- Image from OpenAI: ![](/assets/2023/gpt/The-ChatGPT-training-process-The-figure-is-from-OpenAI-2022a.png)
        - Speculating but the algorithmic scoring is the key. First train an algo that can rate answers using very basic criteria, uses real words, grammatical sentences, uses words from the question. Train GPT-1 using this minimal algorithm to output minimally coherent answers.
        - Have humans rate GPT answers - the algo improves based on human feedback via 'active learning' or 'human-in-the-loop' learning.
        - Take the human ratings and further train algorithmic scoring to rate GPT answers ... now the algo can improve itself via 'self-supervised learning'. You can apply the same architecture that understands questions and generates good responses and train it to rate answers. So the algorithm is essentially training itself.
        - Keep the humans in the loop and further train both the GPT algorithm and the scoring algorithm. By the time you get to GPT-2 and eventually GPT-4 you already have a very large training corpus of good and bad responses and a good automated scoring algorithm.
        - Also you have good baselines, so you can try a new architecture and hyperparameters and train for 1/10,000 of the full training time and generate a solid prediction of whether the new version will be an improvement. And of course you can automate this process to try many variations of architectures and hyperparameters.
		- [LambdaLabs](https://lambdalabs.com/blog/demystifying-gpt-3) estimated $4.6 million US dollars to train, and 355 GPU-years.

- Inference (running the model)
	- Take the input, map it via lookup table to OpenAI-native embedding scheme.
    - Build a distribution of likely responses with probabilities.
	- Rerank them for factuality, safety, and other criteria.
	- Sample the distribution.
    - Map the embeddings back to characters and return them to user.
	
- Performance
	- On one level, ChatGPT is just predicting a statistically likely sentence to appear in a context, like a response to a question. But in order to do this, it has to implicitly learn a great deal of knowledge:
		- Rules of grammar of English and Python and other languages, 
		- Idiomatic expressions. People say 'my big fat Greek wedding' but not 'my Greek fat big wedding' for reasons that aren't always clear even to me as a native English speaker. Sometimes there may be rules for idiomatic language, sometimes you just have to learn n-grams a/k/a sequences that usually go together
		- Elements of style, punctuation.
		- Facts like 
			- Sun Tzu wrote The Art of War. I don't know but I am curious if something about the training process allows it to build a parameter structure that encodes something recognizable as a fact table connecting the relevant embeddings to each other and to an author relationship. AFAIK this knowledge just get encoded in some emergent implicit way that is not yet understood, like in a human brain. It seems like one could train small neural networks on different fact patterns and observe how this sort of information is encoded. It also seems conceivable that one could train with explicit fact tables and train a network to do database lookups in some layer. Since OpenAI stopped releasing details about anything, and they have 180 IQs and I don't, they may have a few new secret-sauce breakthroughs up their sleeve. ![](/assets/2023/gpt/SunTzu.png)
			- Ask it to write a limerick, it knows that a limerick has 5 verses, an aabba rhyming scheme, verses 1,2, 5 have 3 anapests, verses 3 and 4 have 2 anapests![](/assets/2023/gpt/Limerick.png)
		- Patterns of building chains of solid reasoning.
		- Understanding questions and what makes a response relevant.
	- ChatGPT-4 outperformed many / most humans on a variety of standardized tests. ChatGPT-4 was not trained specifically for these tests, although some questions were seen in training. It performs well in many languages including uncommon languages like Latvian, Welsh, Swahili.
		- bar exams
		- SATs - 700 math, 710 verbal
		- GRE
		- USA Biology Olympiad
		- Scored 5 on AP Art History, AP Biology, AP Environmental Science, AP Macroeconomics, AP Microeconomics, AP Psychology, AP Statistics, AP US Government and AP US History
		- US medical licensing exams
		- Leetcode assessments
		- Sommelier written test
		- failed the ethics portion of CFA level 3, [per the FT](https://www.ft.com/content/16342e5a-550e-46ae-a3d6-5244c140cb9b)
		- got a 2 in AP English and 54th percentile in GRE Verbal and did not improve from GPT-3. Also not great on American Math Contest 10th grade math.
		  ![](/assets/2023/gpt/TestScores.png)
- ChatGPT-4 vs. ChatGPT-3
	- Larger maximum token limit: 32K v 4K
	- Multi-modal, can take image inputs. For instance, you can ask it to describe an image (quite poorly in my experience)
	- You can ask it to summarize a web page, it will go out and fetch it and summarize, or ask it 'summarize today's news'
	- Improved answers to many queries. OpenAI says users prefered 70% of answers vs. ChatGPT-3.5.
	- Plugin architecture, integration so it can go out to WolframAlpha, get domain-specific info for math.
	- Cites sources.
	- Better blocking of harmful queries.
- Prompt engineering, or how to ask a question
	- Zero-shot - just ask a question
	- Few-shot - ask a question and provide a few examples
	- Choice of prompt format, training examples, and the order of the examples can lead to dramatically different performance, from near random guess to near SOTA.
	- It gets stuff wrong, so for research, never ask a question where you can't validate the answer (no sweat if you're asking it to write a limerick or something unimportant)
	- Set the context
		- What type of info to use to answer a question
		- How you want it to solve the problem
		- What audience the answer is for
	- Be as specific as possible
	- Give examples with solutions
		- Tell it what to do, avoid telling it what not to do
		- You can describe output format, math format, tables, plots, links
	- Tell it to comment every line of code or explain every reasoning step. "Let's think step by step" magically helps.
	- Check everything and follow up with questions
	- Ask multiple times because there is randomness in responses.
	- Use more than one LLM.
	- Use common prompts and prompt assistants
		https://prompts.chat/
		https://learnprompting.org/docs/intro
		https://promptperfect.jina.ai/

- A Universal AI?
	- It's trained on EVERYTHING. GitHub all the programming languages, Web all the human languages. Well, a lot, anyway.
	- Any task can be described as an English question. Become good at answering questions = become good at all learning tasks = universal intelligence?
	- Sentiment analysis - ask it to complete a review with 'it was ....'![](/assets/2023/gpt/Sentiment.png)
	- Translate anything (Bing AI just says it's calling Bing Translate)![](/assets/2023/gpt/Translate.png)
	- Summarize anything
	  ![](/assets/2023/gpt/Summarize.png)
	- Write code: Copilot - programming tasks - doing pretty good ... but probably only solves complex problems where the code or something close already exists in GitHub. It can fix problems with code and answer Stack Overflow type questions.
	- [Make all the collateral for a business website in a few minutes](https://oneusefulthing.substack.com/p/superhuman-what-can-ai-do-in-30-minutes)
	- [Copilot for MS Office](https://blogs.microsoft.com/blog/2023/03/16/introducing-microsoft-365-copilot-your-copilot-for-work/) coming soon - what are the trends in this data, please make a PowerPoint of the highlights.
	- [Security Copilot](https://www.youtube.com/watch?v=g1HoXNoP3V)
	- Language teaching / tutoring
	- Listen in to customer support calls and pop up hints and suggestions and personalized script. If a customer is calling Chewy and their pet is named Darcy, sprinkle in Pride and Prejudice references.
	- Answer complicated medical questions - Google doctor is now Bing doctor or [veterinarian](https://twitter.com/peakcooper/status/1639716822680236032?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1639716822680236032%7Ctwgr%5Ed2aacacdf8ac80645bb3286f741fc589d9323937%7Ctwcon%5Es1_&ref_url=https%3A%2F%2Fnypost.com%2F2023%2F03%2F27%2Fchatgpt-saved-my-dogs-life-after-vet-couldnt-diagnosis-it%2F).
	- Personalized marketing copy, websites, and emails / spam.
	- Can easily build chatbots to answer plain English questions about any body of text or data like a company wiki.

# Future Directions and Impact

- ChatGPT-5? Speculations:
	- Avatar like Siri on your phone, maybe photorealistic animation and voice of your favorite celebrity or man/woman of your dreams.
	- Will learn from experience, including your interests and communication likes and dislikes like TikTok, deliver personalized banter. Basically the movie [Her](https://www.imdb.com/title/tt1798709/?ref_=fn_al_tt_1). 
	- Physical robots with natural language interactions, probably first on factory floors and commercial establishments.
	- Incorporate conversational responses in Google and Bing and other products without going to a specific bot product.
	- Better Copilot for Office, write/complete many emails automatically.
	- Plain language customized help, tooltips as you work, your software will talk back.
	- More integrations - Expedia, FiscalNote, Instacart, Kayak, Klarna, Milo, OpenTable, Shopify, Slack, Speak, Wolfram, and Zapier.
	- Every body of documentation will be queryable with plain English
	- Receive a deluge of personalized, human-like conversational comms on email and social and text message platforms. I might say, you have to Turing test / Voight-Kampff test everyone you talk to online or on phone, but what would even be the point?

- Impact and issues: There is a dichotomy - helps humans but there are always undesirable side effects and risks.
	- *Economic impact*. Will make a lot of creative tasks easier and democratize them. anyone can code, do graphic design, video editing, it removes language gaps, etc. but
		- Even more content gets generated, now slicker and more personalized, what will be even more scarce is attention.
		- Inboxes will get filled with highly personalized spam, social media will get flooded with even worse, more human trolling, a lot of human-created content will get driven out of the public sphere. ["John Anderson, you could use a Guinness right about now!](https://www.youtube.com/watch?v=7bXJ_obaiYQ")
		- A lot of people will have [a lot of their work automated, lose jobs](https://old.reddit.com/r/blender/comments/121lhfq/i_lost_everything_that_made_me_love_my_job/) 
        - Historically, tech advances didn't hurt aggregate demand for labor. When cars emerged, buggy drivers became truck drivers. A few whip manufacturers and a lot of horses lost their jobs, but overall productivity and labor demand increased. However, if humans and robots are perfectly interchangeable in the labor force, economic theory doesn't rule out human wages declining to what it costs to rent a robot, cheap robots building more cheap robots, and  humans going the way of horses. Historically the elasticity of substitution between labor and capital has remained near 1 because humans always found higher-value stuff that machines couldn't do. But that is an empirical observation, and there is no iron theoretical guarantee that humans can always adapt.
        
	- *Hallucinations.* if there are no high-probability answers, [it will make up something plausible but fictitious](https://twitter.com/ylecun/status/1640122342570336267).- The longer you talk to it [the weirder things get](https://www.nytimes.com/2023/02/16/technology/bing-chatbot-transcript.html?unlocked_article_code=5L9j4UTGa2Sq9R0FulDKfmcq3aesNcqqwZkPBwsIW7EvqkbanIdsZU0hoKktwWijvrw8qobVzmdbyZfxtRjhZQWMN1Qi39t035TVJEllXwc9_zmJYNu_Y9gQ-8ROXYZ8ReuKsdz9WCv7bu_V75hZNZXipGPrv_h5g6phWp8GUX2DTCGQMjV-CTjJybDonBvQaR_kPJbX2zzNM8s4PROKIK32tEVmqGFG0Z1mfzUetZbXqKtz-3b7jFe4M4ylQHHbCnqtSyX4zN6HT4SLsCliddmSW4Aq5V5MZXuwzlSaLgLra8kE5mc1MQuP7cMU9jA6JiP7lhfRpgwcKeHKrGOCRiL21Bw&smid=url-share).
	- *Reasoning errors*. See the math problem below. Often it doesn't know what it's talking about in the sense of a world model. If it's seen something close in training you'll get a good answer, otherwise caveat promptor?![](/assets/2023/gpt/BadMath.png)
	first leg: 20 miles, 60 mph, time 20 minutes
	second leg: 15 miles, 45mph, time 20 minutes
	third leg: 10 miles, 30mph, time 20 minutes
	total: 45 miles, 45mph, time 1 hour (I don't know how it comes up with 0.833 hours)
	- *Overconfidence.* Does not noticeably seem aware of how certain it is of what it says, always sounds confident even when wrong. Bing AI gives source links but it doesn't amount to explainable AI that can give a chain of evidence.
	- Does not learn from experience, not an online learning system. 
	- *Bias* - Sentences in Turkish don't have gender for 'doctor' or 'nurse', when you translate to gendered language doctors are male and nurses are female. ![](/assets/2023/gpt/Bias.png)
	  [Search tools have been known to get worse](https://www.wired.com/story/google-autocomplete-vile-suggestions/) when you ask questions about women, Black people, different nationalities. There is a lot of toxic stuff in the training set. There is an attempt to filter toxic output but it's imperfect and can be [jailbroken using adversarial prompts like DAN](https://medium.com/@neonforge/meet-dan-the-jailbreak-version-of-chatgpt-and-how-to-use-it-ai-unchained-and-unfiltered-f91bfa679024). Sometimes it goes too far, filtering out simple questions that might generate controversial answers.
	  ![](/assets/2023/gpt/Bias2.png)
	- *Harmful questions*, how do I build a bomb, synthesize sarin, commit the perfect crime, beat my spouse and leave no marks. A red team was used to identify questions that should not be answered. There is a post-inference layer that short-circuits bad questions with "I cannot provide information on X".  Or disclaimers, like if you ask how to obtain cheap cigarettes, it will say smoking is bad but here are some options. But again, people can jailbreak with adversarial prompts.
	  ![](/assets/2023/gpt/Harmful.png)
There are many tricky and dual-use questions to block:
![](/assets/2023/gpt/Tricky2.png)
    - *Intellectual property* - It generates text based on online sources, some of which may be copyrighted. Unclear if paying for stock photos is still a thing if you can make AI create any photo (more a Stable Diffusion / Midjourney issue). Since these generative AI creations are based on material created by humans, this raises questions about derived works, fair use, copyright infringement. Which of these concepts apply to a robot? Does the 1st Amendment apply? Some will argue that the 2nd amendment applies and they can have armed robots patrolling their property.
    - *Monoculture* - only huge companies may be able to create these AI models due to the brainpower, dev/training expense, infrastructure.  Jf everyone is asking the same questions of the same models based on the same data, everyone will be learning and doing the same thing. Crowds go to the same places due to Instagram, similarly people will herd in financial markets, sciences, fashions, fads, moral panics, conspiracy theories.
    - *Overreliance* - if people don't verify answers and spread them, rumors, urban myths may spread. People will train these models based on chan boards and create conspiracy- and hate-driven chatbots. Adherents will say, the algo said it, it must be true.
    - *Privacy*
	  - Personal info in training data, like people might have leaked info they don't want public, ChatGPT might make it even easier to find and harder to get rid of.
      - People putting personal info or sensitive corporate questions into ChatGPT, leakage. There was a serious bug showing other people's queries, someone could have asked about themselves and then e.g. an STD or something
	  - Is it smart enough to guess someone's email based on public info, like their initials and where they work, can you figure out the real identity of some social media handle?
    - *Security*
	  - Has potential to generate great personalized phishing emails, carry on long personalized conversations, generative AI can generate deepfakes of your boss calling you and asking you to wire money to a casino in the Philippines. TBH companies should probably have a 'word of the day' that a bot wouldn't know to challenge anyone. Just don't put it anywhere ChatGPT can see it, even on a post-it that's in a picture on the Web. I searched my email for a password and Google returned an email with an image where it was visible, so now it's in Google.
	  - Code it generates might have security vulnerabilities and now everyone is cutting and pasting it.
	  - Might be able to do 'google dorks' to find security vulnerabilities
	  - Changes what people will put online probably, if you don't want it to become part of the Borg of universal knowledge, don't put it on the public web. If ChatGPT can answer all the programming questions no one will go to StackOverflow with their question. No one will put photos in Shutterstock. Over the last 10 years there have been more walled gardens, convos taking place in Slacks, Substacks, private spaces. The golden age of 'information wants to be free' may be over.
    - *Misinformation.* What is real anymore - easy to create super personalized content, with deep fake photos, audios, videos. See [Synthetic Steve Jobs](https://twitter.com/BEASTMODE/status/1637613704312242176)  and the [Pope's puffer jacket](https://inews.co.uk/news/technology/ai-image-pope-francis-puffer-jacket-coat-fooled-internet-experts-fear-theres-worse-come-2234247). 
    - *Acceleration.* The race between e.g. Google, OpenAI, other participants to win the AI race, especially smaller companies and possibly international entrants, and of course bad actors who are geopolitical rivals of the current power, or just want to burn everything down, may result in cutting corners and putting tech out which poses big risks.
    - *Is it really intelligence?* I don't think ChatGPT has a world model, I have tried examples [like this](https://twitter.com/d_feldman/status/1636955260680847361). Seems like a [Clever Hans](https://en.wikipedia.org/wiki/Clever_Hans) or a much smarter Eliza that can sort of BS on a huge amount of processed text. Raises the [Chinese Room](https://en.wikipedia.org/wiki/Chinese_room) issue. If we do come up with algos that act human, are they sentient? Where does the sentience reside, if for instance we simulated the algo (slowly) on pencil and paper?
    - *Winners and losers* - haven't thought about too much
	  - winner - msft, nvidia?
      - loser - shutterstock, stack overflow, adobe maybe, google maybe
      - Microsoft is looking like a big winner, they have an exclusive with OpenAI which is a nimble startup running circles around everyone with ChatGPT. Initially I would have said it's kind of BS, Google has all the AI papers and market power, and Chat-GPT3 is not the most state of the art ... but long experience says first movers are first movers for a reason and keep winning a lot of the time, plus they have Microsoft market power in their corner. [Google is in their flop era similar to Ballmer's at Microsoft.](https://medium.com/@pravse/the-maze-is-in-the-mouse-980c57cfd61a). The Google+ pivot was a disaster, it's hard to innovate at scale and saddled with career risk. It's easier to just try to find ways to goose ad revenue.


