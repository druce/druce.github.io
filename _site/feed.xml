<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator><link href="http://0.0.0.0:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://0.0.0.0:4000/" rel="alternate" type="text/html" /><updated>2021-01-14T12:46:42-05:00</updated><id>http://0.0.0.0:4000/feed.xml</id><title type="html">Druce.ai</title><subtitle>Druce's Blog on Machine Learning, Tech, Markets and Economics</subtitle><author><name>Druce Vertes</name></author><entry><title type="html">What I would have written if I were Jack Dorsey</title><link href="http://0.0.0.0:4000/2021/01/what-i-would-have-written-if-i-were-jack-dorsey" rel="alternate" type="text/html" title="What I would have written if I were Jack Dorsey" /><published>2021-01-14T06:28:57-05:00</published><updated>2021-01-14T06:28:57-05:00</updated><id>http://0.0.0.0:4000/2021/01/what-i-would-have-written-if-i-were-jack-dorsey</id><content type="html" xml:base="http://0.0.0.0:4000/2021/01/what-i-would-have-written-if-i-were-jack-dorsey">&lt;blockquote&gt;
  &lt;p&gt;&lt;em&gt;Our decision to permanently suspend Donald Trump from the Twitter platform, may be a major inflection point in Twitter’s history. As CEO, I owe our users and employees a clear statement of why we took this action and how this decision evolved, i.e. not just some pablum about what a hard decision and potentially dangerous decision it was.&lt;/em&gt;
&lt;!--more--&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In the early days of Twitter, we took a position as &lt;a href=&quot;https://www.eff.org/deeplinks/2014/05/twitter-steps-down-free-speech-party&quot;&gt;“The free speech wing of the free speech party.”&lt;/a&gt;. Unfortunately, over time that position has proved untenable.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;We have to follow the law. Even in the USA, a bastion of free speech fundamentalism, we need robust protections against copyright infringement and child pornography, as just two examples. In Germany, laws forbid glorifying Hitler and Nazis. In some countries, certain religious symbols are protected.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;We had hoped and expected that users would penalize false or repugnant speech by users unfollowing and blocking bad actors. Unfortunately, this has not been the case. Controversial speech demands and receives attention. It is more likely to be widely shared. Algorithms and even people have a hard time telling if something is shared in agreement or disagreement. (Our algorithmic engineers and even human safety teams are working on this but have a very long way to go.) Engagement-maximizing algorithms tend to drive people to controversial content.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;As a result, instead of wide distribution helping to moderate speech, the opposite turns out to be the case. People feel free to harass from behide the safety of a keyboard. Bad actors like bots and cynical politicians and exploit the algorithm. As the algorithm drives people to controversial content, people increase the quantity and extremism of their content. This results in an extremism singularity. This is due to the vulnerability built into the social media ecosystem and exploitation of the vulnerability by bad actors.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Bad content drives out good. Once you have a critical mass of followers, every post attracts criticism which spills into harassment. As a result, social media becomes inhospitable for some opinions and especially members of marginalized groups, women, people of color. Good people leave the platform, resulting in a preponderance of extreme content. The platform enters a vicious circle and doom loop.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Frankly I have to thank the people who continue to invest their time and energy into attempts at civilized debate in the context of this garbage fire despite what must seem like our constant attempts to drive them away.&lt;/p&gt;

&lt;p&gt;The bottom line is that moderation has turned out to be a necessary and fundamental of any social platform. That means adjusting the algorithm to favor quality, or at least promote extremism less.&lt;/p&gt;

&lt;p&gt;In the worst cases, that includes deleting content and suspending repeat offenders. Online communities have formed a consensus on drawing a hard line around online speech that causes offline harm. Doxxing, swatting, organizing a lynch mob crosses the line where activity goes beyond merely speech.&lt;/p&gt;

&lt;p&gt;In the offline world, once speech has a nexus to criminal activity, it’s called ‘conspiracy’, ‘fraud’, ‘incitement’. Freedom of speech does not let you yell ‘fire’ in a crowded theater. In this case, speech on Twitter was being used find and reach people prepared to engage in illegal activity, motivate them to do it, and organize the transportation and logistics for political violence.&lt;/p&gt;

&lt;p&gt;We can delete content that crosses this line. However suspension must also be part of the arsenal. Language is infinitely malleable. Euphemisms and informal code switching can have the same effect without explicitly crossing bright lines. Context is meaning. When someone demonstrates the intent to act in bad faith and repeatedly does so, after warnings and temporary suspension, permanent suspension becomes the only alternative.&lt;/p&gt;

&lt;p&gt;This is the context in which we suspended Donald Trump. Far from being motivated by politics, we bend over backwards when speech is political, or by a political leader like the president of the USA. In this case a political leader clearly crossed the line into using our platform to reach people disposed to violence, motivating them and organizing the means for them to do so.&lt;/p&gt;

&lt;p&gt;As an American, it pains me to be forced to take these necessary steps against our president. However, this was not a close or difficult call. If I could go back and change anything, knowing the president’s abandonment of facts and legal means and embrace of antidemocratic, illegal means, I would have taken this action sooner.&lt;/p&gt;

&lt;p&gt;We always seek to give people as much freedom as possible, without adversely affecting the platform and the marketplace of ideas. This is maximizes the value of the platform, an obligation we have to our shareholders. And also maximizes Twitter’s value in the marketplace of ideas. Progress comes from debate. Progress may encounter resistance, as it has in the case of civil rights, women’s rights, and if you go back far enough, democracy itself. However, when sharp debate becomes harassment and calls for violence, when Twitter is used to oppose democracy and support real-world lawbreaking and violence, we have to draw a sharp line.&lt;/p&gt;

&lt;p&gt;I  look forward to engaging in civilized debate on the Twitter platform, and I hope we will not have to take such drastic steps again.&lt;/p&gt;

&lt;p&gt;/not Jack Dorsey/&lt;/p&gt;</content><author><name>Druce Vertes</name></author><category term="politics" /><category term="politics" /><summary type="html">Our decision to permanently suspend Donald Trump from the Twitter platform, may be a major inflection point in Twitter’s history. As CEO, I owe our users and employees a clear statement of why we took this action and how this decision evolved, i.e. not just some pablum about what a hard decision and potentially dangerous decision it was.</summary></entry><entry><title type="html">Demystifying Portfolio Optimization with Python and CVXOPT</title><link href="http://0.0.0.0:4000/2020/12/portfolio-opimization" rel="alternate" type="text/html" title="Demystifying Portfolio Optimization with Python and CVXOPT" /><published>2020-12-06T17:28:57-05:00</published><updated>2020-12-06T17:28:57-05:00</updated><id>http://0.0.0.0:4000/2020/12/portfolio-optimization</id><content type="html" xml:base="http://0.0.0.0:4000/2020/12/portfolio-opimization">&lt;p&gt;&lt;img src=&quot;/assets/2020/efficientfrontier1.png&quot; alt=&quot;Efficient frontier&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;em&gt;Do you want to do fast and easy portfolio optimization with Python? Then CVXOPT, and this post, are for you! Here’s a gentle intro to portfolio theory and some &lt;a href=&quot;https://github.com/druce/portfolio_optimization/blob/master/Portfolio%20optimization.ipynb&quot;&gt;code&lt;/a&gt; to get you started.&lt;/em&gt;
&lt;!--more--&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;a href=&quot;https://stockcharts.com/h-sc/ui?s=gld&amp;amp;id=p33407302522&amp;amp;def=Y&amp;amp;listNum=1#&quot;&gt;Gold&lt;/a&gt; has been performing well in the Covid-19 market, rising close to $2000/oz. before trading recently around $1700. Should long-term investors have gold in their portfolio, and how much?&lt;/p&gt;

&lt;p&gt;One way to answer this question is to look at how portfolios with and without gold performed. Using a portfolio optimization framework, we can find the highest-return portfolio for any feasible level of risk: the classic &lt;em&gt;efficient frontier&lt;/em&gt; above.&lt;/p&gt;

&lt;p&gt;For each asset, the marker represents &lt;em&gt;the historical mean real return&lt;/em&gt; (y-axis) vs. &lt;em&gt;the historical standard deviation of real returns&lt;/em&gt; (x-axis). The line represents the return and risk for optimal combinations of these 5 assets that maximize the return for that level of risk (or vice versa, minimize the level of risk for a specified threshold return). The efficient frontier line slopes upward, because the more risk you take, the more return you should get. More to the point, if it sloped down, that would mean you accepted more risk for less return, which by our definition would not be an optimal combination.&lt;/p&gt;

&lt;p&gt;By looking at whether optimal portfolios contain gold, and over which time periods and risk levels, we can get a sense of whether we should own gold, and how much. We can plot a transition map of the composition of the portfolio as you move along the efficient frontier:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2020/transitionmap1.png&quot; alt=&quot;Transition map&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The transition map is a stacked area chart of the composition of the portfolio at each point on the frontier. The leftmost point shows the minimum risk portfolio, with the risk on the top x-axis and the return on the bottom x-axis, and the colors representing each asset in the portfolio. The rightmost point shows the riskiest optimal portfolio, which is 100% stocks. In between, we see the composition of the optimal portfolio as we vary our risk and move along the efficient frontier.&lt;/p&gt;

&lt;p&gt;We observe that a small allocation of gold is present in most portfolios, except for the riskiest optimal portfolio. The minimum-volatility portfolio is T-bills plus a little bit of S&amp;amp;P plus a little bit of gold. The highest-volatility portfolio is of course 100% stocks, but if you lower the volatility constraint, the first asset you add is gold.&lt;/p&gt;

&lt;p&gt;Next, we’ll take a step back and discuss portfolio theory basics, how we compute the risk, and how we optimize with CVXOPT.&lt;/p&gt;

&lt;h3 id=&quot;why-do-we-use-the-standard-deviation-of-returns-as-a-proxy-for-risk&quot;&gt;Why do we use the standard deviation of returns as a proxy for risk?&lt;/h3&gt;

&lt;p&gt;Using &lt;a href=&quot;https://fs.blog/2013/11/mr-market/&quot;&gt;Warren Buffet’s analogy&lt;/a&gt;, we anthropomorphize the market as a moody partner who co-invests in our stocks, and whose valuation varies with his mood swings. On a good day, Mr. Market values a security according to the best-case scenario: at the high end of the range of potential valuations. On a bad day, he values it according to the worst-case scenario, at the low end of the range.&lt;/p&gt;

&lt;p&gt;On a &lt;em&gt;typical&lt;/em&gt; day, Mr. Market moves a typical distance between the two extremes of fear and greed. &lt;em&gt;‘Volatility’ is how much a stock moves on a typical day based on Mr. Market’s mood swings.&lt;/em&gt; This distance is a proxy for how wide Mr. Market’s valuation range is, hence &lt;em&gt;how risky the market thinks the security is&lt;/em&gt;. See footnote&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; for a more mathematical explanation.&lt;/p&gt;

&lt;p&gt;As an investor, should you care how the market perceives risk? That depends. First, do you believe the market’s implicit risk perception is worth knowing to inform your own decisions? And even if not, are you going to trade with the market?&lt;/p&gt;

&lt;p&gt;If the answer to both questions is ‘no’, then maybe you don’t care about volatility. If you are an omniscient Warren Buffett, with your own deeply-considered valuation of a company, and you want to hold forever, then the daily market price and volatility don’t affect your actions in any way. Risk just means that many futures &lt;em&gt;can&lt;/em&gt; unfold but only one future &lt;em&gt;will&lt;/em&gt; unfold, even when you know as much as possible. The meaningful measure of risk is the margin of error around your own valuation, due to factors outside your control. You should spend your time thinking about those factors and the margin of error, not the market volatility.&lt;/p&gt;

&lt;p&gt;So, if you have confidence in your own risk estimation, the &lt;em&gt;market&lt;/em&gt; volatility may be irrelevant to your risk estimate. But if you have to &lt;em&gt;trade&lt;/em&gt; with the market, you may still wish to take volatility into account.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Fundamental&lt;/em&gt; valuation metrics like price/earnings, price/book, enterprise value/EBITDA are a bit like looking at your poker hand and determining if you have a pair of aces or a pair of unsuited rags. &lt;em&gt;Technical&lt;/em&gt; metrics like volatility are a bit like looking at how your opponents have been playing and what their patterns and tells are. Any good poker player will tell you it’s a good idea to pay attention to all of that. (To be honest, in most cases if I had to pick between knowing the value of my hand and knowing how the players will react to my betting, I would pick &lt;a href=&quot;https://www.sportingnews.com/us/other-sports/news/annette-obrestad-on-legendary-blind-poker-tournament-win/nsw780tdkfmx187t3rx67bgeh&quot;&gt;the latter&lt;/a&gt;.)&lt;/p&gt;

&lt;p&gt;If you are Warren Buffett, you don’t &lt;em&gt;have&lt;/em&gt; to trade, ever. That is a big part of his edge. Market volatility is Buffett’s opportunity. But if you’re planning to retire or might need to sell in the foreseeable future, you should have a feel for volatility. Otherwise, you end up trading at an inopportune time, and worst of all, getting thrown off your game by unforeseen volatility.&lt;/p&gt;

&lt;p&gt;Just like historical P/Es, historical volatilities are a tool for investing judgment, not a substitute for it. But most of us aren’t Warren Buffett and are students of the markets. Understanding volatility and risk are part of our journey. Maybe one day I’ll stop caring about portfolio volatility, but today is not that day.&lt;/p&gt;

&lt;h3 id=&quot;how-do-you-compute-portfolio-volatility-from-individual-asset-volatilities&quot;&gt;How do you compute portfolio volatility from individual asset volatilities?&lt;/h3&gt;

&lt;p&gt;Given the historical volatility of each of our assets, how do we compute the volatility of our portfolio?&lt;/p&gt;

&lt;p&gt;Of course, we can compute all the historical asset-weighted returns of the portfolio and then compute the standard deviation. But if we want to do optimization we need an expression to optimize, we need to understand how portfolio volatility is determined.&lt;/p&gt;

&lt;p&gt;Suppose you own 1 share of asset \(a_1\) and 1 share of asset \(a_2\). And you know the standard deviation of \(a_1\)’s absolute returns \(\sigma_{1}=\) $3 and the standard deviation of \(a_2\)’s absolute returns \(\sigma_{2} =\) $4. What is the volatility of a portfolio with one share of each?&lt;/p&gt;

&lt;p&gt;To compute the portfolio return SD, the &lt;em&gt;correlation&lt;/em&gt; \(\rho\) (&lt;em&gt;rho&lt;/em&gt;) between \(a_1\) and \(a_2\) comes into play, because you need to know if the assets generally move in the same direction or in opposite directions. The correlation is how many SDs we expect \(a_1\)’s return to move from its mean when \(a_2\)’s return moves 1 SD from its mean, and vice versa.&lt;/p&gt;

&lt;p&gt;The expression for the risk of the combined portfolio of \(a_1\) and \(a_2\) is:&lt;/p&gt;

\[\sigma_{port} = \sqrt{\sigma_{1}^2 + \sigma_{2}^2 + 2 \cdot \rho \cdot \sigma_{1} \cdot \sigma_{2} }\]

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Return vol \(\sigma_{1}\)&lt;/th&gt;
      &lt;th&gt;Return vol \(\sigma_{2}\)&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;Return correlation \(\rho_{12}\)&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;Portfolio volatility&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;$3&lt;/td&gt;
      &lt;td&gt;$4&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;1.0&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;\(\sqrt{3^2 + 4^2 + 2 \cdot 3 \cdot 4 \cdot 1} =  3 + 4 = $7\)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$3&lt;/td&gt;
      &lt;td&gt;$4&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;-1.0&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;\(\sqrt{3^2 + 4^2 + 2 \cdot 3 \cdot 4 \cdot (-1)} = 4 - 3 = $1\)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$3&lt;/td&gt;
      &lt;td&gt;$4&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.5&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;\(\sqrt{3^2 + 4^2 + 2 \cdot 3 \cdot 4 \cdot 0.5}  = \sqrt{37} = $6.08\)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$3&lt;/td&gt;
      &lt;td&gt;$4&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;-0.5&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;\(\sqrt{3^2 + 4^2 + 2 \cdot 3 \cdot 4 \cdot (-0.5)} = \sqrt{13} = $3.61\)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$3&lt;/td&gt;
      &lt;td&gt;$4&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;\(\sqrt{3^2 + 4^2} = $5\)&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt; &amp;nbsp; &lt;/p&gt;

&lt;p&gt;This should remind you of Pythagoras and square triangles and the cosine rule in trigonometry.&lt;/p&gt;

&lt;p&gt;Cosine rule:&lt;/p&gt;

\[a = \sqrt{b^2 + c^2 - 2 \cdot cos{\alpha} \cdot b \cdot c }\]

&lt;p&gt;&lt;img src=&quot;/assets/2020/cosinerule.png&quot; height=&quot;348&quot; width=&quot;432&quot; alt=&quot;cosine rule&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Correlation math is like the cosine rule, but we &lt;em&gt;change the sign&lt;/em&gt; of the last term. This gives us a handy way to visualize how correlation and risk interact.&lt;/p&gt;

&lt;h3 id=&quot;visualizing-risk-math-geometrically-with-triangles&quot;&gt;Visualizing risk math geometrically with triangles&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Consider our 2 assets
    &lt;ul&gt;
      &lt;li&gt;Asset \(a_1\) has SD of returns \(\sigma_{1}\)&lt;/li&gt;
      &lt;li&gt;Asset \(a_2\) has SD of returns \(\sigma_{2}\)&lt;/li&gt;
      &lt;li&gt;\(\rho\) is the correlation between the returns of \(a_1\) and \(a_2\)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Take the inverse cosine of the correlation. Find the angle \(\alpha' = 180^{\circ} - cos^{-1}\rho\)
    &lt;ul&gt;
      &lt;li&gt;If \(\rho=1, cos^{-1}1 = 0^{\circ}, \alpha'=180^{\circ}\)&lt;/li&gt;
      &lt;li&gt;If \(\rho=0, cos^{-1}0 = 90^{\circ}, \alpha' = 180^{\circ} - 90^{\circ} = 90^{\circ}\)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Make a triangle with 2 sides of length \(\sigma_{1}\) and \(\sigma_{2}\) and the angle \(\alpha'\) between those 2 sides&lt;/li&gt;
  &lt;li&gt;The SD of the portfolio returns is given by the length of the third side.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;When we plug in angle \(\alpha'\) = 180° - cos&lt;sup&gt;-1&lt;/sup&gt;\(\rho\) in place of \(\alpha\), we reverse the sign of the last term&lt;sup id=&quot;fnref:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;, and the cosine rule becomes:&lt;/p&gt;

\[a = \sqrt{b^2 + c^2 + 2 \cdot cos \alpha' \cdot b \cdot c }\]

&lt;p&gt;This matches the risk expression when we use \(\alpha' = 180^\circ - \alpha\).&lt;/p&gt;

&lt;h3 id=&quot;examples&quot;&gt;Examples:&lt;/h3&gt;

&lt;p style=&quot;font-size: 120%; text-align: center;&quot;&gt;Correlation \(\rho = 0, \alpha' = 90^{\circ}\)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2020/corr0.png&quot; height=&quot;268&quot; width=&quot;434&quot; alt=&quot;0 correlation&quot; /&gt;&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;p style=&quot;font-size: 120%; text-align: center;&quot;&gt;Correlation \(\rho = 0.5, \alpha' = 60^{\circ}\)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2020/corr0.5.png&quot; height=&quot;222&quot; width=&quot;440&quot; alt=&quot;0.5 correlation&quot; /&gt;&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;p style=&quot;font-size: 120%; text-align: center;&quot;&gt;Correlation \(\rho = 1.0, \alpha' = 0^{\circ}\)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2020/corr1.png&quot; height=&quot;130&quot; width=&quot;463&quot; alt=&quot;1 correlation&quot; /&gt;&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;p style=&quot;font-size: 120%; text-align: center;&quot;&gt;Correlation \(\rho = -1.0, \alpha' = 180^{\circ}\)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2020/corr-1.png&quot; height=&quot;117&quot; width=&quot;396&quot; alt=&quot;1 correlation&quot; /&gt;&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;p style=&quot;font-size: 120%; text-align: center;&quot;&gt;Correlation \(\rho = -0.5, \alpha' = 120^{\circ}\)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2020/corr-0.5.png&quot; height=&quot;236&quot; width=&quot;454&quot; alt=&quot;1 correlation&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;the-covariance-matrix-and-generalizing-to-n-assets&quot;&gt;The covariance matrix, and generalizing to &lt;em&gt;n&lt;/em&gt; assets&lt;/h3&gt;

&lt;p&gt;\(\sigma_{1}\sigma_{2}\rho_{12}\) is the &lt;em&gt;covariance&lt;/em&gt; \(\sigma_{12}\). So we can rewrite:&lt;/p&gt;

\[\sigma_{port} = \sqrt{\sigma_{1}^2 + \sigma_{2}^2 + 2\sigma_{12}}\]

&lt;p&gt;The covariance is the expected value of the product of A’s deviation from its mean and B’s deviation from its mean. The correlation is the covariance scaled by (divided by) the product of A’s standard deviation and B’s standard deviation (the maximum possible covariance if \(\rho=1\)).&lt;/p&gt;

&lt;p&gt;The triangles above are drawn assuming 1 share of each asset and absolute dollar returns. To use portfolio weights and percentage returns, we can write the return as:&lt;/p&gt;

\[r_{p} = w_{1}r_{1} + w_{2}r_{2}\]

&lt;p&gt;and the volatility as:&lt;/p&gt;

\[\sigma_{p} = \sqrt{ w_{1}^2\sigma_{1}^2 + w_{2}^2\sigma_{2}^2 + 2w_{1}w_{2}\sigma_{12}}\]

&lt;p&gt;\(\sigma_{p}\) is now the volatility in return space instead of absolute space. Generalizing to \(n\) assets and using \(\sigma_{ii}\) to denote the variance of asset \(i\):&lt;/p&gt;

\[\sigma_{p} = \sqrt{\sum_{i=1}^{n}\sum_{j=1}^{n}w_{i}w_{j}\sigma_{ij}}\]

&lt;p&gt;When you do a nested summation over a square array, you should probably consider a matrix. Define the &lt;em&gt;covariance matrix&lt;/em&gt; \(\Sigma\) (using \(\rho=0.5\)) as:&lt;/p&gt;

\[\Sigma = \begin{vmatrix}
\sigma_{1}^2 &amp;amp; \sigma_{12} \\
\sigma_{21} &amp;amp; \sigma_{2}^2
\end{vmatrix} = \begin{vmatrix}
9 &amp;amp; 6 \\
6 &amp;amp; 16
\end{vmatrix}\]

&lt;p&gt;with variances \(\sigma^2\) on the diagonal and covariances \(\sigma_{ij}\) off the diagonal.&lt;/p&gt;

&lt;p&gt;Then we get an elegant matrix notation for the double summation above:&lt;/p&gt;

\[\sigma_{p} = \sqrt{\boldsymbol{w^T} \Sigma \boldsymbol{w}}\]

&lt;h3 id=&quot;drawing-the-efficient-frontier&quot;&gt;Drawing the efficient frontier&lt;/h3&gt;

&lt;p&gt;What is the efficient frontier? The line describing the set of feasible portfolios a rational investor might choose.&lt;/p&gt;

&lt;p&gt;What is the leftmost point on the efficient frontier? It’s the minimum volatility portfolio.&lt;/p&gt;

&lt;p&gt;We can find the weights for the minimum volatility portfolio through a &lt;a href=&quot;https://en.wikipedia.org/wiki/Convex_optimization&quot;&gt;convex optimization&lt;/a&gt;. When the objective function is convex, there is a global minimum which we can find efficiently by &lt;a href=&quot;http://homepages.laas.fr/henrion/courses/lmi06/lmiIII1.pdf&quot;&gt;various methods&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;CVXPY is a Python modeling framework for convex optimization (&lt;a href=&quot;https://arxiv.org/abs/1603.00943&quot;&gt;paper&lt;/a&gt;), by Steven Diamond and Stephen Boyd of Stanford (who wrote a &lt;a href=&quot;https://web.stanford.edu/~boyd/cvxbook/&quot;&gt;textbook&lt;/a&gt; on convex optimization). In the way Pandas is a Python extension for dataframes, CVXPY is a Python extension for describing convex optimization problems.&lt;/p&gt;

&lt;p&gt;CVXPY is a little more user-friendly and more performant than &lt;a href=&quot;https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html&quot;&gt;scipy.optimize&lt;/a&gt;, and CVXPY supports &lt;a href=&quot;https://www.cvxpy.org/tutorial/advanced/index.html#solve-method-options&quot;&gt;many solvers&lt;/a&gt; on the back end, open-source and commercial.&lt;/p&gt;

&lt;p&gt;In particular, CVXPY’s parameter abstraction allows solvers to efficiently re-use previous calculations when tracing out an efficient frontier.&lt;/p&gt;

&lt;p&gt;The first step is to load some data from &lt;a href=&quot;http://people.stern.nyu.edu/adamodar&quot;&gt;Professor Aswath Damodaran&lt;/a&gt;’s website into a Pandas dataframe:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;data_xls&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'http://www.stern.nyu.edu/~adamodar/pc/datasets/histretSP.xls'&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;data_sheet&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Returns by year&quot;&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# these will change as rows get added on Damodaran website
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;skiprows&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;17&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;skipfooter&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;download_df&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read_excel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'http://www.stern.nyu.edu/~adamodar/pc/datasets/histretSP.xls'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
                         &lt;span class=&quot;n&quot;&gt;sheet_name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data_sheet&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
                         &lt;span class=&quot;n&quot;&gt;skiprows&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;skiprows&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                         &lt;span class=&quot;n&quot;&gt;skipfooter&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;skipfooter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;download_df&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;download_df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_index&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Year'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Similarly we can load data from the FRED economic indicator database:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import pandas_datareader as pdr
# load gold data from FRED API &amp;amp; save copy locally to CSV file
series = ['GOLDAMGBD228NLBM']
gold_download = pdr.data.DataReader(series, 
                                    'fred', 
                                    start='1968-12-31')
# convert daily to annual
gold_download = gold_download.resample('A').last().reset_index()
gold_download.set_index(pd.DatetimeIndex(gold_download['DATE']).year, inplace=True)
gold_download['return'] = gold_download['GOLDAMGBD228NLBM'].pct_change()
gold_download.to_csv('gold_fred.csv')
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;After some additional data-wrangling we have a dataframe &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;df&lt;/code&gt; of real returns 1928-1999 for T-bills, T-notes, Baa corporate bonds, S&amp;amp;P, and gold (see the &lt;a href=&quot;https://github.com/druce/portfolio_optimization/blob/master/Portfolio%20optimization.ipynb&quot;&gt;notebook&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;To find the minimum-volatility portfolio:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;cvxpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cp&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# compute covariance matrix (df being the dataframe of historical returns)
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Sigma&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cov&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transpose&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# number of assets
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Sigma&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# average returns
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mu&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;values&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# asset SDs
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;asset_vols&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Sigma&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;diagonal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# variable to optimize over - portfolio weights
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Variable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# objectives to optimize
# portfolio return
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ret&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt; 
&lt;span class=&quot;c1&quot;&gt;# volatility
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vol&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;quad_form&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Sigma&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;prob&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Problem&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Minimize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# minimize volatility
&lt;/span&gt;                  &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;   &lt;span class=&quot;c1&quot;&gt;# sum of weights = 1
&lt;/span&gt;                   &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;           &lt;span class=&quot;c1&quot;&gt;# weights &amp;gt; 0 (long-only)
&lt;/span&gt;                 &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;prob&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;solve&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;wts&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'%0.4f'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;v&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;v&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;minvol&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;value&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now we have the leftmost point on the efficient frontier. The rightmost point is the highest possible return we could have achieved, which is 100% in the highest-performing asset. We know it’s the S&amp;amp;P but let’s let CVXPY figure it out for us:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# Solve max return portfolio (corner solution)
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;prob&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Problem&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Maximize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ret&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# maximize return
&lt;/span&gt;                  &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
                   &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
                 &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;prob&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;solve&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;wts&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'%0.4f'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;v&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;v&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;maxretvol&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;value&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Finally, we trace out the rest of the frontier. We create an array of 200 volatilities between minvol and maxretvol inclusive. For each volatility, we solve the optimization for the highest return portfolio subject to volatility &amp;lt;= vol:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;vol_limit = cp.Parameter(nonneg=True)

prob = cp.Problem(cp.Maximize(ret),
                  [cp.sum(w) == 1, 
                   w &amp;gt;= 0,
                   vol &amp;lt;= vol_limit   # new constraint: vol &amp;lt;= vol_limit parameter
                  ]
                 )

# define helper function 
def solve_vl(vl_val):
    vol_limit.value = vl_val
    result = prob.solve()
    return (ret.value, np.sqrt(vol.value), w.value)

# number of points on the frontier
NPOINTS = 200
vl_vals = np.linspace(minvol, maxretvol, NPOINTS)

# iterate over vl_vals
results_dict = {}
for vl_val in vl_vals:
    results_dict[vl_val] = solve_vl(vl_val)

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Then we draw the frontier with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;matplotlib&lt;/code&gt; (same chart as at the top of this post):
&lt;img src=&quot;/assets/2020/efficientfrontier1.png&quot; alt=&quot;Efficient frontier&quot; /&gt;&lt;/p&gt;

&lt;p&gt;And the transition map:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2020/transitionmap1.png&quot; alt=&quot;Transition map&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This covers a very long timespan. Let’s look at 1972-2019, i.e. after the US abandoned the gold standard:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2020/efficientfrontier2.png&quot; alt=&quot;Efficient frontier&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2020/transitionmap2.png&quot; alt=&quot;Transition map&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In this more inflationary period, somewhat more gold would have been optimal.&lt;/p&gt;

&lt;p&gt;Finally, let’s look at 1983-2018, i.e. the era of disinflation (compare the gold marker vs. the previous chart):&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2020/efficientfrontier3.png&quot; alt=&quot;Efficient frontier&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2020/transitionmap3.png&quot; alt=&quot;Transition map&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Gold added value in the most inflationary period, but is present even in the last period. I would argue that the question is not whether you should have some inflation hedge in your portfolio but if you should have gold or something else, like TIPS, Bitcoin, leveraged real estate.&lt;/p&gt;

&lt;h3 id=&quot;long-short-optimization&quot;&gt;Long-short optimization&lt;/h3&gt;

&lt;p&gt;To illustrate CVXOPT for a long-short portfolio, we create a synthetic asset that returns -5% per year and has 0.9 correlation with the S&amp;amp;P, which we called ‘stonks’. We remove the constraint of weights being positive but add a constraint that the gross exposure must be less than 150%:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;vol_limit = cp.Parameter(nonneg=True)

prob = cp.Problem(cp.Maximize(ret), 
                  [cp.norm1(w) &amp;lt;= 1.5,  # 1-norm &amp;lt;= 1.5, i.e. gross exposure &amp;lt; 150%
                   cp.sum(w) == 1,
                   vol &amp;lt;= vol_limit]
                 )

# define function so we can solve many in parallel
def solve_vl(vl_val):
    vol_limit.value = vl_val
    result = prob.solve()
    return (ret.value, np.sqrt(vol.value), w.value)

# number of points on the frontier
NPOINTS = 200
vl_vals = np.linspace(minvol, maxretvol, NPOINTS)

# iterate in-process
results_dict = {}
for vl_val in vl_vals:
    results_dict[vl_val] = solve_vl(vl_val)
  
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2020/efficientfrontier4.png&quot; alt=&quot;Efficient frontier&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2020/transitionmap4.png&quot; alt=&quot;Transition map&quot; /&gt;&lt;/p&gt;

&lt;p&gt;On the efficient frontier, for the same risk as stocks, you get a much higher return, or you get an equal return to stocks at about half the risk. Blinding glimpse of the obvious: if you can find good shorts and use leverage, you can supercharge returns. Note that the transition map visualizes gross exposures, but the stonks and occasionally some of the other assets go short.&lt;/p&gt;

&lt;h3 id=&quot;factor-model&quot;&gt;Factor model&lt;/h3&gt;

&lt;p&gt;As a final CVXOPT example, we can create a set of random factors and stocks with random exposures to these factors and random returns, and solve for the minimum-volatility portfolio and an efficient frontier:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# number of stocks
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1000&lt;/span&gt; 
&lt;span class=&quot;c1&quot;&gt;# random historical mean returns for each stock
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mu&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;normal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# number of factors
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# factor covariance matrix - random symmetrical matrix
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SigmaFactor&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;SigmaFactor&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SigmaFactor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SigmaFactor&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# factor loadings, determine volatility and covariances between stocks
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;F&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# idiosyncratic risk of each stock
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;D&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;diag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;uniform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;ret&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;             &lt;span class=&quot;c1&quot;&gt;# solve for weights that maximize portfolio return
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;F&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;                &lt;span class=&quot;c1&quot;&gt;# portfolio factor loading
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Lmax&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Parameter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;      &lt;span class=&quot;c1&quot;&gt;# leverage constraint
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Lmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;value&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# portfolio volatility: factor risk + idiosyncratic risk
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;risk&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;quad_form&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SigmaFactor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;quad_form&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;prob&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Problem&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Minimize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;risk&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
                  &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
                   &lt;span class=&quot;n&quot;&gt;cp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;norm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Lmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;prob&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;solve&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;solver&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;OSQP&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;minvol&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;risk&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;value&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;minvolret&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ret&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;value&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Min vol portfolio (return=%.4f, risk=%.4f)&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;minvolret&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;minvol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;concluding-remarks&quot;&gt;Concluding remarks&lt;/h3&gt;

&lt;p&gt;Mean-variance optimization is a useful framework. I think of markets as boundedly efficient, and I believe efficient-market maximalists have done investors a disservice by overselling modern portfolio theory. Beyond the forms of the &lt;a href=&quot;https://www.investopedia.com/ask/answers/032615/what-are-differences-between-weak-strong-and-semistrong-versions-efficient-market-hypothesis.asp#:~:text=The%20strong%20form%20version%20of,an%20advantage%20on%20the%20market.&quot;&gt;efficient market hypothesis&lt;/a&gt;, even the weakest of which is only approximately true, here are 3 important fallacies (or at least oversimplifications) that sometimes get taught:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;The risk-free rate and the capital market line.&lt;/em&gt; Suppose there is a risk-free rate with an SD of 0 and some positive return. If you draw a line on the efficient frontier chart, starting from the risk-free rate and tangent to the efficient frontier, it represents the highest feasible Sharpe ratio. You can maximize the Sharpe ratio by holding the market portfolio at the tangent point, and the risk-free asset in some combination, choosing your desired level of risk and return. Similarly, if you can borrow at some rate you can lever up the max-Sharpe portfolio to achieve the highest possible Sharpe at higher levels of risk. It follows that everyone should just hold something close to the market portfolio with varying degrees of leverage based on their risk tolerance. But in practice, &lt;strong&gt;there ain’t no such thing as a risk-free rate&lt;/strong&gt;. If you are allocating for a timeframe greater than a few years, you should be focusing on real inflation-adjusted, after-tax returns, not nominal returns. In that context, there is no risk-free rate. And there can be no risk-free positive real rate in a real world subject to disasters and policy discontinuities. To expect markets or governments to ensure investors are guaranteed a positive real after-tax return, in a world of wars and pandemics, where participants in goods and labor markets receive no such guarantee, is a chimerical notion.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;Betas (or factors) are all that matters.&lt;/em&gt; If you assume that factors capture all the correlations between stocks, then the unsystematic risk of one stock is always uncorrelated with the risk of other stocks, and furthermore, with a sufficient number of stocks, the unsystematic risk diversifies out. All that’s left is beta, the risk captured by the factors. In practice, this is a bad assumption, particularly at times of market stress. No factor model can never capture all the underlying real-world correlations and potential correlations, many of which are never realized. &lt;strong&gt;No model captures everything.&lt;/strong&gt; The common factors that influence stock returns consistently over the long run, like GDP growth, don’t explain all the correlations, and the factors that would explain all the correlations would be numerous and change over time. For instance, if California tumbles into the sea, a lot of stocks that previously had low correlations will be highly correlated. Because, first of all, California exposure was not previously a factor but now suddenly is. And secondly because in a big market discontinuity correlations tend to go to one, and seemingly diversified portfolios become riskier than predicted. The discontinuity overwhelms everything else, traders trade what there is a market for and not what is specifically impacted by news flows, and betas tend not to sway them. The modeled risk of a portfolio should be viewed as a lower bound in a normal market where the present is like the past.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;Everyone should hold the market portfolio because it is game-theory optimal.&lt;/em&gt; Granted, if you hold the market portfolio, and the market portfolio never changes, and you never need to trade, you are guaranteed the market return. But everyone &lt;em&gt;has&lt;/em&gt; to trade &lt;em&gt;sometime&lt;/em&gt;. Once you receive dividend or interest income to reinvest, or need to rebalance, or have any cash flows into or out of the portfolio, your returns depend on your trading efficiency and alignment with market timing. No one can be 100% passive. And when you have to trade, the sharks are waiting to exploit you. The smart money, like market-makers or Warren Buffett, can demand an explicit or implicit bid-ask by only giving the other side of the trade when it’s worth their while. Furthermore, when the index changes, you are forced to trade to match it, and people will front-run you. If Tesla goes into the S&amp;amp;P at $700/share as a top-10 market cap, and you are able to buy it at $700/share, you will match the index, but it doesn’t mean you didn’t get fleeced. The dominance of indexing assumes cost-free trading, and &lt;strong&gt;trading always costs money, sometimes a lot&lt;/strong&gt;. In practice, the more investors index, the more inefficient the market gets, making it more costly to trade, and increasing the opportunity set for non-indexers.  If you don’t have an edge, indexing and implicitly trying to tag along with the average investor is extremely sensible. But it is not guaranteed to be a free lunch. You have to trade as little as possible, and avoid buying at the top and selling at the bottom. Most retail investors don’t match the market &lt;a href=&quot;https://faculty.haas.berkeley.edu/odean/papers%20current%20versions/behavior%20of%20individual%20investors.pdf&quot;&gt;for many reasons&lt;/a&gt;, some of which are neutralized by indexing. But even retail index investors underperform because they tend to buy and sell at the worst times.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;I’m reminded of Donald Knuth, “Beware of bugs in the above code; I have only proved it correct, not tried it.” This maxim may apply to modern portfolio theory as well as to what I just wrote. Economics is the only discipline in which two academics can receive a Nobel Prize for research where they directly contradict each other (&lt;a href=&quot;https://www.nytimes.com/2013/10/15/business/3-american-professors-awarded-nobel-in-economic-sciences.html&quot;&gt;Fama and Shiller&lt;/a&gt;). Portfolio theory is a brilliant and useful map of reality, &lt;a href=&quot;https://www.museumtv.art/artnews/articles/rene-magritte-ceci-nest-pas-une-pipe/&quot;&gt;not reality itself&lt;/a&gt;. Navigating with &lt;a href=&quot;https://www.travelandleisure.com/travel-news/baunei-sardinia-italy-bans-google-maps-after-tourists-drive-wrong-directions&quot;&gt;excessive confidence&lt;/a&gt; on the basis of imperfect models can be taken to extremes where models lose some of their usefulness.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/druce/portfolio_optimization&quot;&gt;Code here&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;A desk is a dangerous place from which to view the world. – John Le Carré&lt;/em&gt;&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;For a more mathematical explanation, suppose market returns, expressed as sequential changes in log, follow a normal distribution, and market prices therefore follow a lognormal distribution. The daily volatility maps directly to long-run volatility, because long-run log diffs are the sum of daily log diffs. So short-run volatility tells you long-run volatility over your investing time frame, which is risk. Risk means that over your investment horizon, say 10 years, the investment could do very well or very poorly; that distribution of returns defines your risk. If you know the volatility of daily returns, you can infer the volatility of 10-year returns and vice versa. For normally distributed returns, you can annualize volatility following a square-root-of-time law. Modulo serial correlation, the annualized daily volatility will match the annualized 10-year volatility. (It seems reasonable that quant or trend traders would largely arb out any systematic serial correlation.) Note that stock returns &lt;a href=&quot;https://towardsdatascience.com/are-stock-returns-normally-distributed-e0388d71267e&quot;&gt;do not exactly follow a normal distribution&lt;/a&gt;, they have fatter tails, especially on the left. Entropy means it’s easier to have a negative shock than a positive one: you can instantaneously implode a skyscraper but you can’t instantaneously build one. Similarly you can have a market meltdown but typically not a melt-up.  Maybe an appropriate distribution is a &lt;a href=&quot;https://www.cambridge.org/core/journals/journal-of-financial-and-quantitative-analysis/article/abs/simplified-jump-process-for-common-stock-returns/3175D09CA2BF3EBA9219E54ED64980A3&quot;&gt;combination of a normal distribution and a Poisson distribution&lt;/a&gt; for rare shocks. But that’s getting into the weeds, the point is that short-run volatility is a useful proxy for longer-term risk. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;We reverse the sign because of the inconvenient convention of how triangle angles are measured. Suppose you start walking in the direction of segment \(a_1\) for the first asset. If for segment \(a_2\) you keep going in the same direction, that corresponds to \(\alpha'\) = 0°, correlation = 1.0. If you do a 180, that corresponds to \(\alpha'\) = 180°, correlation = -1.0. If you make a right-angle turn in either direction, that corresponds to \(\alpha'\) = +/- 90°, correlation = 0. The way we usually measure triangle angles, an angle of 0 corresponds to doing a 180, which is a slightly backwards way of looking at it. So we have to reverse the sign on the classic cosine rule to make cosine math correspond to correlation math. But the spatial relationship is the same. &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Druce Vertes</name></author><category term="datascience" /><category term="datascience" /><category term="markets" /><category term="investing" /><summary type="html">Do you want to do fast and easy portfolio optimization with Python? Then CVXOPT, and this post, are for you! Here’s a gentle intro to portfolio theory and some code to get you started.</summary></entry><entry><title type="html">Beyond Grid Search: Using Hyperopt, Optuna, and Ray Tune to hypercharge hyperparameter tuning for XGBoost and LightGBM</title><link href="http://0.0.0.0:4000/2020/10/hyperparameter-tuning-with-xgboost-ray-tune-hyperopt-and-optuna" rel="alternate" type="text/html" title="Beyond Grid Search: Using Hyperopt, Optuna, and Ray Tune to hypercharge hyperparameter tuning for XGBoost and LightGBM" /><published>2020-10-12T18:28:57-04:00</published><updated>2020-10-12T18:28:57-04:00</updated><id>http://0.0.0.0:4000/2020/10/hyperparameter-tuning-with-xgboost-ray-tune-hyperopt-and-optuna</id><content type="html" xml:base="http://0.0.0.0:4000/2020/10/hyperparameter-tuning-with-xgboost-ray-tune-hyperopt-and-optuna">&lt;p&gt;&lt;img src=&quot;/assets/2020/fig1.png&quot; alt=&quot;RandomizedSearch HPO vs. Bayesian HPO&quot; /&gt;&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;&lt;em&gt;Bayesian optimization of machine learning model hyperparameters works faster and better than grid search. Here’s how we can speed up hyperparameter tuning using 1) Bayesian optimization with Hyperopt and Optuna, running on… 2) the &lt;a href=&quot;https://ray.io/&quot;&gt;Ray&lt;/a&gt; distributed machine learning framework, with a &lt;a href=&quot;https://medium.com/riselab/cutting-edge-hyperparameter-tuning-with-ray-tune-be6c0447afdf&quot;&gt;unified API to many hyperparameter search algos&lt;/a&gt; and early stopping schedulers, and… 3) a distributed cluster of cloud instances for even faster tuning.&lt;/em&gt;
&lt;!--more--&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;outline&quot;&gt;Outline:&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;#1-results&quot;&gt;Results&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#2-hyperparameter-tuning-overview&quot;&gt;Hyperparameter tuning overview&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#3-bayesian-optimization&quot;&gt;Bayesian optimization&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#4-early-stopping&quot;&gt;Early stopping&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#5-implementation-details&quot;&gt;Implementation details&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#6-baseline-linear-regression&quot;&gt;Baseline linear regression&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#7-elasticnetcv&quot;&gt;ElasticNetCV (Linear regression with L1 and L2 regularization)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#8-gridsearchcv&quot;&gt;ElasticNet with GridSearchCV &lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#9-xgboost-with-sequential-grid-search&quot;&gt;XGBoost: sequential grid search over hyperparameter subsets with early stopping &lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#10-xgboost-with-hyperopt-optuna-and-ray&quot;&gt;XGBoost: Hyperopt and Optuna search algorithms&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#11-lightgbm-with-hyperopt-and-optuna&quot;&gt;LightGBM: Hyperopt and Optuna search algorithms&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#12-xgboost-on-a-ray-cluster&quot;&gt;XGBoost on a Ray cluster&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#13-lightgbm-on-a-ray-cluster&quot;&gt;LightGBM on a Ray cluster&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#14-concluding-remarks&quot;&gt;Concluding remarks&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;1-results&quot;&gt;1. Results&lt;/h2&gt;

&lt;p&gt;Bottom line up front: Here are results on the Ames housing data set, predicting Iowa home prices:&lt;/p&gt;

&lt;h3 id=&quot;xgb-and-lightgbm-using-various-hyperparameter-optimization-methodologies&quot;&gt;XGB and LightGBM using various hyperparameter optimization methodologies&lt;/h3&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;ML Algo&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Search algo&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;CV Error&lt;br /&gt; (RMSE in $)&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Time&lt;br /&gt; h:mm::ss&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;XGB&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Sequential Grid Search&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;18302&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1:27:14&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;XGB&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Hyperopt (1024 samples)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;18309&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0:53:41&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;XGB&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Optuna (1024 samples)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;18325&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0:48:02&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;XGB&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Hyperopt (2048 samples) - 32x cluster&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;18030&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1:30:58&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;XGB&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Optuna (2048 samples) - 32x cluster&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;18028&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1:29:57&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;LightGBM&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Hyperopt (1024 samples)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;18615&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1:13:40&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;LightGBM&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Optuna (1024 samples)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;18614&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1:08:40&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;LightGBM&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Hyperopt (2048 samples) - 32x cluster&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;18459&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1:05:19&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;LightGBM&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Optuna (2048 samples) - 32x cluster&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;18458&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0:48:16&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt; &lt;/p&gt;

&lt;h3 id=&quot;baseline-linear-models&quot;&gt;Baseline linear models&lt;/h3&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;ML Algo&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Search algo&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;CV Error (RMSE in $)&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Time mm::ss&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Linear Regression&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;–&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;18192&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0:01&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;ElasticNet&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;ElasticNetCV (Grid Search)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;17964&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0:02&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;ElasticNet&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;GridSearchCV&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;17964&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0:05&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;em&gt;Times for single-instance are on a local desktop with 12 threads, comparable to EC2 4xlarge. Times for cluster are on m5.large x 32 (1 head node + 31 workers).&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;We saw a big speedup when using Hyperopt and Optuna locally, compared to grid search. The sequential search performed about 261 trials, so the XGB/Optuna search performed about 3x as many trials in half the time and got a similar RMSE.&lt;/li&gt;
  &lt;li&gt;The cluster of 32 instances (64 threads) gave a modest RMSE improvement vs. the local desktop with 12 threads. I attempted to set this up so we would get some improvement in RMSE vs. local Hyperopt/Optuna (which we did with 2048 trials), and some speedup in training time (which we did not get with 64 threads). It ran twice the number of trials in slightly less than twice the time. The comparison is imperfect, local desktop vs. AWS, running Ray 1.0 on local and 1.1 on the cluster, different number of trials (better hyperparameter configs don’t get early-stopped and take longer to train). But the point was to see what kind of improvement one might obtain in practice, leveraging a cluster vs. a local desktop or laptop. Bottom line, modest benefit here from a 32-node cluster.&lt;/li&gt;
  &lt;li&gt;RMSEs are similar across the board. XGB with 2048 trials is best by a small margin among the boosting models.&lt;/li&gt;
  &lt;li&gt;LightGBM doesn’t offer improvement over XGBoost here in RMSE or run time. In my experience LightGBM is often faster so  you can train and tune more in a given time. But we don’t see that here. Possibly XGB interacts better with ASHA early stopping.&lt;/li&gt;
  &lt;li&gt;Similar RMSE between Hyperopt and Optuna. Optuna is consistently faster (up to 35% with LGBM/cluster).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Our simple ElasticNet baseline yields slightly better results than boosting, in seconds. This may be because our feature engineering was intensive and designed to fit the linear model. Not shown, SVR and KernelRidge outperform ElasticNet, and an ensemble improves over all individual algos.&lt;/p&gt;

&lt;p&gt;Full notebooks are on &lt;a href=&quot;https://github.com/druce/iowa/blob/master/hyperparameter_optimization.ipynb&quot;&gt;GitHub&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;2-hyperparameter-tuning-overview&quot;&gt;2. Hyperparameter Tuning Overview&lt;/h2&gt;

&lt;p&gt;(If you are not a data scientist ninja, here is some context. If you are, you can safely skip to &lt;a href=&quot;#3-bayesian-optimization&quot;&gt;Bayesian optimization&lt;/a&gt;
and implementations below.)&lt;/p&gt;

&lt;p&gt;Any sufficiently advanced machine learning model is indistinguishable from magic, and any sufficiently advanced machine learning model needs good tuning.&lt;/p&gt;

&lt;p&gt;Suppose you have a neural network to predict whether a stock will go up or down next week (binary classification). Suppose the neural network is described by  3 discrete hyperparameters: how many layers, how many units in each layer, and an activation function (relu or logistic). The hyperparameter optimization problem is to find the parameter vector that yields the best results.&lt;/p&gt;

&lt;p&gt;One dumb way is an exhaustive grid search over all possible values. Another way is a random search, drawing hyperparameter values from independent uniform distributions. A smarter Bayesian search starts off sampling from independent uniform distributions but tries to learn the best region and distribution to sample from. So it works faster and better. That’s pretty much it.&lt;/p&gt;

&lt;p&gt;To dive in a little, here is a typical modeling workflow:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Exploratory data analysis: understand your data.&lt;/li&gt;
  &lt;li&gt;Feature engineering and feature selection: clean, transform and engineer the best possible features&lt;/li&gt;
  &lt;li&gt;Modeling: model selection and hyperparameter tuning to identify the best model architecture, and ensembling to combine multiple models&lt;/li&gt;
  &lt;li&gt;Evaluation: Describe the out-of-sample error and its expected distribution.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To minimize the out-of-sample error, you minimize the error from &lt;em&gt;bias&lt;/em&gt;, meaning the model is too simple or insufficiently sensitive to the signal in the data, and &lt;em&gt;variance&lt;/em&gt;, meaning the model is too complex or too sensitive to noise in the training data in ways that don’t generalize out-of-sample. Modeling is 90% data prep, the other half is all finding the &lt;a href=&quot;http://scott.fortmann-roe.com/docs/BiasVariance.html&quot;&gt;optimal bias-variance tradeoff&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Hyperparameters help you tune the bias-variance tradeoff. For a simple logistic regression predicting survival on the Titanic, a regularization parameter lets you control overfitting by penalizing sensitivity to any individual feature. For a massive neural network doing machine translation, the number and types of layers, units, activation function, in addition to regularization, are hyperparameters. We select the best hyperparameters using &lt;em&gt;&lt;a href=&quot;https://machinelearningmastery.com/k-fold-cross-validation/&quot;&gt;k-fold cross-validation&lt;/a&gt;&lt;/em&gt;; this is what we call hyperparameter tuning.&lt;/p&gt;

&lt;p&gt;The regression algorithms we use in this post are XGBoost and LightGBM, which are variations on  &lt;em&gt;gradient boosting&lt;/em&gt;. Gradient boosting is an ensembling method that usually involves decision trees. A decision tree constructs rules like, if the passenger is in first class and female, they probably survived the sinking of the Titanic. Trees are powerful, but a single deep decision tree with all your features will tend to overfit the training data. A &lt;em&gt;random forest&lt;/em&gt; algorithm builds many decision trees based on random subsets of observations and features which then vote (&lt;em&gt;bagging&lt;/em&gt;). The outcome of a vote by &lt;em&gt;weak learners&lt;/em&gt; is less overfitted than training on all the data rows and all the feature columns to generate a single strong learner, and performs better out-of-sample. Random forest hyperparameters include the number of trees, tree depth, and how many features and observations each tree should use.&lt;/p&gt;

&lt;p&gt;Instead of aggregating many independent learners working in parallel, &lt;em&gt;i.e.&lt;/em&gt; bagging, &lt;em&gt;boosting&lt;/em&gt; uses many learners in series:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Start with a simple estimate like the median or base rate.&lt;/li&gt;
  &lt;li&gt;Fit a tree to the &lt;em&gt;error&lt;/em&gt; in this prediction.&lt;/li&gt;
  &lt;li&gt;If you can &lt;em&gt;predict&lt;/em&gt; the error, you can &lt;em&gt;adjust&lt;/em&gt; for it and improve the prediction. Adjust the prediction not all the way to the tree prediction, but part of the way based on a &lt;em&gt;learning rate&lt;/em&gt; (a hyperparameter).&lt;/li&gt;
  &lt;li&gt;Fit another tree to the error in the updated prediction and adjust the prediction further based on the learning rate.&lt;/li&gt;
  &lt;li&gt;Iteratively continue reducing the error for a specified number of boosting rounds (another hyperparameter).&lt;/li&gt;
  &lt;li&gt;The final estimate is the initial prediction plus the sum of all the predicted necessary adjustments (weighted by the learning rate).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The learning rate performs a similar function to voting in random forest, in the sense that no single decision tree determines too much of the final estimate. This ‘wisdom of crowds’ approach helps prevent overfitting.&lt;/p&gt;

&lt;p&gt;Gradient boosting is the current state of the art for regression and classification on traditional structured tabular data (in contrast to less structured data like image/video/natural language processing, where deep learning, &lt;em&gt;i.e.&lt;/em&gt; deep neural nets are state of the art).&lt;/p&gt;

&lt;p&gt;Gradient boosting algorithms like  &lt;a href=&quot;https://xgboost.readthedocs.io/en/latest/parameter.html&quot;&gt;XGBoost&lt;/a&gt;, &lt;a href=&quot;https://lightgbm.readthedocs.io/en/latest/Parameters.html&quot;&gt;LightGBM&lt;/a&gt;, and &lt;a href=&quot;https://catboost.ai/docs/concepts/python-reference_parameters-list.html&quot;&gt;CatBoost&lt;/a&gt; have a very large number of hyperparameters, and tuning is an important part of using them.&lt;/p&gt;

&lt;p&gt;These are &lt;a href=&quot;https://en.wikipedia.org/wiki/Hyperparameter_optimization&quot;&gt;the principal approaches to hyperparameter tuning&lt;/a&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;Grid search&lt;/em&gt;: Given a finite set of discrete values for each hyperparameter, exhaustively cross-validate all combinations.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;Random search&lt;/em&gt;: Given a discrete or continuous distribution for each hyperparameter, randomly sample from the joint distribution. Generally &lt;a href=&quot;https://dl.acm.org/doi/10.5555/2188385.2188395&quot;&gt;more efficient than exhaustive grid search.&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;Bayesian optimization&lt;/em&gt;: Sample like random search, but update the search space you sample from as you go, based on outcomes of prior searches.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;Gradient-based optimization&lt;/em&gt;: Attempt to estimate the gradient of the cross-validation metric with respect to the hyperparameters and ascend/descend the gradient.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;Evolutionary optimization&lt;/em&gt;: Sample the search space, discard combinations with poor metrics, and genetically evolve new combinations based on the successful combinations.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;Population-based training&lt;/em&gt;: A method of performing hyperparameter optimization at the same time as training.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In this post, we focus on Bayesian optimization with Hyperopt and Optuna.&lt;/p&gt;

&lt;h2 id=&quot;3-bayesian-optimization&quot;&gt;3. Bayesian Optimization&lt;/h2&gt;

&lt;p&gt;What is Bayesian optimization? When we perform a grid search, the search space is a &lt;a href=&quot;https://en.wikipedia.org/wiki/Prior_probability&quot;&gt;prior&lt;/a&gt;: we believe that the best hyperparameter vector is in this search space. And &lt;em&gt;a priori&lt;/em&gt; each hyperparameter combination has equal probability of being the best combination (a uniform distribution). So we try them all and pick the best one.&lt;/p&gt;

&lt;p&gt;Perhaps we might do two passes of grid search. After an initial search on a broad, coarsely spaced grid, we do a deeper dive in a smaller area around the best metric from the first pass, with a more finely-spaced grid. In Bayesian terminology, we &lt;em&gt;updated our prior&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Bayesian optimization starts by sampling randomly, e.g. 30 combinations, and computes the cross-validation metric for each of the 30 randomly sampled combinations using &lt;em&gt;&lt;a href=&quot;https://machinelearningmastery.com/k-fold-cross-validation/&quot;&gt;k-fold cross-validation&lt;/a&gt;&lt;/em&gt;. Then the algorithm updates the distribution it samples from, so that it is more likely to sample combinations similar to the good metrics, and less likely to sample combinations similar to the poor metrics. As it continues to sample, it continues to update the search distribution it samples from, based on the metrics it finds.&lt;/p&gt;

&lt;iframe frameborder=&quot;no&quot; border=&quot;0&quot; marginwidth=&quot;0&quot; marginheight=&quot;0&quot; width=&quot;720&quot; height=&quot;360&quot; src=&quot;/assets/2020/plotly.html&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;(The size of the markers corresponds to the size of RMSE. On the left, markers are the same color, but if two markers overlap the alpha will show darker overlap. On the right marker color corresponds ot the search order, so we can see later trials are close to the final value. Hover the mouse over markers for more info.)&lt;/p&gt;

&lt;p&gt;If good metrics are not uniformly distributed, but found close to one another in a Gaussian distribution or any distribution which we can model, then Bayesian optimization can exploit the underlying pattern, and is likely to be more efficient than grid search or naive random search.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://hyperopt.github.io/hyperopt/&quot;&gt;HyperOpt&lt;/a&gt; is a Bayesian optimization algorithm by &lt;a href=&quot;https://conference.scipy.org/proceedings/scipy2013/pdfs/bergstra_hyperopt.pdf&quot;&gt;James Bergstra et al.&lt;/a&gt;, see this &lt;a href=&quot;https://blog.dominodatalab.com/hyperopt-bayesian-hyperparameter-optimization/&quot;&gt;excellent blog post by Subir Mansukhani&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://optuna.org/&quot;&gt;Optuna&lt;/a&gt; is a Bayesian optimization algorithm by &lt;a href=&quot;https://arxiv.org/abs/1907.10902&quot;&gt;Takuya Akiba et al.&lt;/a&gt;, see this &lt;a href=&quot;https://medium.com/optuna/using-optuna-to-optimize-xgboost-hyperparameters-63bfcdfd3407&quot;&gt;excellent blog post by Crissman Loomis&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;4-early-stopping&quot;&gt;4. Early Stopping&lt;/h2&gt;

&lt;p&gt;If, while evaluating a hyperparameter combination, the evaluation metric is not improving in training, or not improving fast enough to beat our best to date, we can discard a combination before fully training on it. &lt;em&gt;Early stopping&lt;/em&gt; of unsuccessful training runs increases the speed and effectiveness of our search.&lt;/p&gt;

&lt;p&gt;XGBoost and LightGBM helpfully provide early stopping callbacks to check on training progress and stop a training trial early (&lt;a href=&quot;https://xgboost.readthedocs.io/en/latest/python/callbacks.html&quot;&gt;XGBoost&lt;/a&gt; ; &lt;a href=&quot;https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.early_stopping.html#lightgbm.early_stopping&quot;&gt;LightGBM&lt;/a&gt;). Hyperopt, Optuna, and Ray use these callbacks to stop bad trials quickly and accelerate performance.&lt;/p&gt;

&lt;p&gt;In this post, we will use the &lt;a href=&quot;https://arxiv.org/abs/1810.05934&quot;&gt;Asynchronous Successive Halving Algorithm (ASHA)&lt;/a&gt; for early stopping, described in this &lt;a href=&quot;https://blog.ml.cmu.edu/2018/12/12/massively-parallel-hyperparameter-optimization/&quot;&gt;blog post&lt;/a&gt;.&lt;/p&gt;

&lt;h4 id=&quot;further-reading&quot;&gt;Further reading:&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2003.05689&quot;&gt;Hyper-Parameter Optimization: A Review of Algorithms and Applications&lt;/a&gt; Tong Yu, Hong Zhu (2020)&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1502.02127v2&quot;&gt;Hyperparameter Search in Machine Learning&lt;/a&gt;, Marc Claesen, Bart De Moor (2015)&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://link.springer.com/chapter/10.1007/978-3-030-05318-5_1&quot;&gt;Hyperparameter Optimization&lt;/a&gt;, Matthias Feurer, Frank Hutter (2019)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;5-implementation-details&quot;&gt;5. Implementation Details&lt;/h2&gt;

&lt;p&gt;We use data from the &lt;a href=&quot;https://www.kaggle.com/c/house-prices-advanced-regression-techniques&quot;&gt;Ames Housing Dataset&lt;/a&gt;. The original data set has 79 raw features. The data we will use has 100 features with a fair amount of feature engineering from &lt;a href=&quot;https://github.com/druce/iowa&quot;&gt;my own attempt at modeling&lt;/a&gt;, which was in the top 5% or so when I submitted it to Kaggle. We model the log of the sale price, and use RMSE as our metric for model selection. We convert the RMSE back to raw dollar units for easier interpretability.&lt;/p&gt;

&lt;p&gt;We use 4 regression algorithms:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;LinearRegression&lt;/em&gt;: baseline with no hyperparameters&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;ElasticNet&lt;/em&gt;: Linear regression with L1 and L2 regularization (2 hyperparameters).&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;XGBoost&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;LightGBM&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We use 5 approaches:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Native CV&lt;/em&gt;: In sklearn if an algorithm &lt;em&gt;xxx&lt;/em&gt; has hyperparameters it will often have an &lt;em&gt;xxxCV&lt;/em&gt; version, like ElasticNetCV, which performs automated grid search over hyperparameter iterators with specified kfolds.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;GridSearchCV&lt;/em&gt;: Abstract grid search that can wrap around any sklearn algorithm, running multithreaded trials over specified kfolds.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Manual sequential grid search&lt;/em&gt;: How we typically implement grid search with XGBoost, which doesn’t play very well with GridSearchCV and has too many hyperparameters to tune in one pass.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Ray on local desktop&lt;/em&gt;: Hyperopt and Optuna with ASHA early stopping.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Ray on AWS cluster&lt;/em&gt;: Additionally scale out to run a single hyperparameter optimization task over many instances in a cluster.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;6-baseline-linear-regression&quot;&gt;6. Baseline linear regression&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Use the same kfolds for each run so the variation in the RMSE metric is not due to variation in kfolds.&lt;/li&gt;
  &lt;li&gt;We fit on the log response, so we convert error back to dollar units, for interpretability.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sklearn.model_selection.cross_val_score&lt;/code&gt; for evaluation&lt;/li&gt;
  &lt;li&gt;Jupyter &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;%%time&lt;/code&gt; magic for wall time&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;n_jobs=-1&lt;/code&gt; to run folds in parallel using all CPU cores available.&lt;/li&gt;
  &lt;li&gt;Note the wall time &amp;lt; 1 second and RMSE of 18192.&lt;/li&gt;
  &lt;li&gt;Full notebooks are on &lt;a href=&quot;https://github.com/druce/iowa/blob/master/hyperparameter_optimization.ipynb&quot;&gt;GitHub&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;o&quot;&gt;%%&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# always use same RANDOM_STATE k-folds for comparability between tests, reproducibility
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;RANDOMSTATE&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;42&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;RANDOMSTATE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;kfolds&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;KFold&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_splits&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shuffle&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random_state&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;RANDOMSTATE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;MEAN_RESPONSE&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;response&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;cv_to_raw&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cv_val&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mean_response&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;MEAN_RESPONSE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;convert log1p rmse to underlying SalePrice error&quot;&quot;&quot;&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# MEAN_RESPONSE assumes folds have same mean response, which is true in expectation but not in each fold
&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;# we can also pass the mean response for each fold
&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;# but we're really just looking to consistently convert the log value to a more meaningful unit
&lt;/span&gt;    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;expm1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean_response&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cv_val&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;expm1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean_response&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	
&lt;span class=&quot;n&quot;&gt;lr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;LinearRegression&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# compute CV metric for each fold
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cross_val_score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predictors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;response&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
                          &lt;span class=&quot;n&quot;&gt;scoring&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;neg_root_mean_squared_error&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                          &lt;span class=&quot;n&quot;&gt;cv&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kfolds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                          &lt;span class=&quot;n&quot;&gt;n_jobs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;raw_scores&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cv_to_raw&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Raw CV RMSE %.0f (STD %.0f)&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;raw_scores&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;raw_scores&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Raw CV RMSE 18192 (STD 1839)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Wall time: 65.4 ms&lt;/code&gt;&lt;/p&gt;

&lt;h2 id=&quot;7-elasticnetcv&quot;&gt;7. ElasticNetCV&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;ElasticNet is linear regression with L1 and L2 &lt;a href=&quot;https://en.wikipedia.org/wiki/Regularization_(mathematics)&quot;&gt;regularization&lt;/a&gt; (2 hyperparameters).&lt;/li&gt;
  &lt;li&gt;When we use regularization, we need to scale our data so that the coefficient penalty has a similar impact across features. We use a pipeline with RobustScaler for scaling.&lt;/li&gt;
  &lt;li&gt;Fit a model and extract hyperparameters from the fitted model.&lt;/li&gt;
  &lt;li&gt;Then we do &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cross_val_score&lt;/code&gt; with reported hyperparams (There doesn’t appear to be a way to extract the score from the fitted model without refitting)&lt;/li&gt;
  &lt;li&gt;Verbose output reports 130 tasks, for full grid search on 10 folds we would expect 13x9x10=1170. Apparently a clever optimization.&lt;/li&gt;
  &lt;li&gt;Note the modest reduction in RMSE vs. linear regression without regularization.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;elasticnetcv&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;make_pipeline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;RobustScaler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt;
                             &lt;span class=&quot;n&quot;&gt;ElasticNetCV&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;max_iter&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
                                          &lt;span class=&quot;n&quot;&gt;l1_ratio&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.01&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.05&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.95&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.99&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
                                          &lt;span class=&quot;n&quot;&gt;alphas&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;logspace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
                                          &lt;span class=&quot;n&quot;&gt;cv&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kfolds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                          &lt;span class=&quot;n&quot;&gt;n_jobs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                          &lt;span class=&quot;n&quot;&gt;verbose&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                         &lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;#train and get hyperparams
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;elasticnetcv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predictors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;response&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;l1_ratio&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;elasticnetcv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_final_estimator&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l1_ratio_&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;elasticnetcv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_final_estimator&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alpha_&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'l1_ratio'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l1_ratio&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'alpha'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# evaluate using kfolds on full dataset
# I don't see API to get CV error from elasticnetcv, so we use cross_val_score
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;elasticnet&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ElasticNet&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                        &lt;span class=&quot;n&quot;&gt;l1_ratio&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l1_ratio&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                        &lt;span class=&quot;n&quot;&gt;max_iter&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cross_val_score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;elasticnet&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predictors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;response&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
                          &lt;span class=&quot;n&quot;&gt;scoring&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;neg_root_mean_squared_error&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                          &lt;span class=&quot;n&quot;&gt;cv&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kfolds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                          &lt;span class=&quot;n&quot;&gt;n_jobs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;raw_scores&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cv_to_raw&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Log1p CV RMSE %.04f (STD %.04f)&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Raw CV RMSE %.0f (STD %.0f)&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;raw_scores&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;raw_scores&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;l1_ratio 0.01
alpha 0.0031622776601683794

Log1p CV RMSE 0.1030 (STD 0.0109)
Raw CV RMSE 18061 (STD 2008)
CPU times: user 5.93 s, sys: 3.67 s, total: 9.6 s
Wall time: 1.61 s

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;h2 id=&quot;8-gridsearchcv&quot;&gt;8. GridSearchCV&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Identical result, runs a little slower.&lt;/li&gt;
  &lt;li&gt;GridSearchCV verbose output shows 1170 jobs, which is the expected number 13x9x10.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;gs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;make_pipeline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;RobustScaler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt;
                   &lt;span class=&quot;n&quot;&gt;GridSearchCV&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ElasticNet&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;max_iter&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
                                &lt;span class=&quot;n&quot;&gt;param_grid&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'l1_ratio'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.01&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.05&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.95&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.99&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
                                            &lt;span class=&quot;s&quot;&gt;'alpha'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;logspace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
                                           &lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;
                                &lt;span class=&quot;n&quot;&gt;scoring&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'neg_root_mean_squared_error'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                &lt;span class=&quot;n&quot;&gt;refit&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                &lt;span class=&quot;n&quot;&gt;cv&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kfolds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                &lt;span class=&quot;n&quot;&gt;n_jobs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                &lt;span class=&quot;n&quot;&gt;verbose&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
                               &lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# do cv using kfolds on full dataset
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predictors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;response&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'best params'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_final_estimator&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;best_params_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'best score'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_final_estimator&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;best_score_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;l1_ratio&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_final_estimator&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;best_params_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'l1_ratio'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_final_estimator&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;best_params_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'alpha'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# eval similarly to before
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;elasticnet&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ElasticNet&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                        &lt;span class=&quot;n&quot;&gt;l1_ratio&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l1_ratio&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                        &lt;span class=&quot;n&quot;&gt;max_iter&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;elasticnet&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cross_val_score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;elasticnet&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predictors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;response&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
                          &lt;span class=&quot;n&quot;&gt;scoring&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;neg_root_mean_squared_error&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                          &lt;span class=&quot;n&quot;&gt;cv&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kfolds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                          &lt;span class=&quot;n&quot;&gt;n_jobs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;raw_scores&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cv_to_raw&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Log1p CV RMSE %.06f (STD %.04f)&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Raw CV RMSE %.0f (STD %.0f)&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;raw_scores&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;raw_scores&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;best params {'alpha': 0.0031622776601683794, 'l1_ratio': 0.01}
best score 0.10247177583755482
ElasticNet(alpha=0.0031622776601683794, l1_ratio=0.01, max_iter=100000)

Log1p CV RMSE 0.103003 (STD 0.0109)
Raw CV RMSE 18061 (STD 2008)

Wall time: 5 s
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;9-xgboost-with-sequential-grid-search&quot;&gt;9. XGBoost with sequential grid search&lt;/h2&gt;

&lt;p&gt;It &lt;em&gt;should&lt;/em&gt; be possible to use GridSearchCV with XGBoost. But when we also try to use early stopping, XGBoost wants an eval set. OK, we can give it a static eval set held out from GridSearchCV. Now, GridSearchCV does k-fold cross-validation in the training set but XGBoost uses a separate dedicated eval set for early stopping. It seems like a bit of a Frankenstein methodology. See the &lt;a href=&quot;https://github.com/druce/iowa/blob/master/hyperparameter_optimization.ipynb&quot;&gt;notebook&lt;/a&gt; for the attempt at GridSearchCV with XGBoost and early stopping if you’re really interested.&lt;/p&gt;

&lt;p&gt;Instead we write our own grid search that gives XGBoost the correct hold-out set for each CV fold:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;EARLY_STOPPING_ROUNDS&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# stop if no improvement after 100 rounds
&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;my_cv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;predictors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;response&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kfolds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;regressor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;verbose&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;Roll our own CV 
    train each kfold with early stopping
    return average metric, sd over kfolds, average best round&quot;&quot;&quot;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;metrics&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;best_iterations&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_fold&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv_fold&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kfolds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; 
        &lt;span class=&quot;n&quot;&gt;fold_X_train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predictors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_fold&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;fold_y_train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;response&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_fold&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;fold_X_test&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predictors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cv_fold&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;fold_y_test&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;response&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cv_fold&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;regressor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fold_X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fold_y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                      &lt;span class=&quot;n&quot;&gt;early_stopping_rounds&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;EARLY_STOPPING_ROUNDS&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                      &lt;span class=&quot;n&quot;&gt;eval_set&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fold_X_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fold_y_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)],&lt;/span&gt;
                      &lt;span class=&quot;n&quot;&gt;eval_metric&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'rmse'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                      &lt;span class=&quot;n&quot;&gt;verbose&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;verbose&lt;/span&gt;
                     &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;y_pred_test&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;regressor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fold_X_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;metrics&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean_squared_error&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fold_y_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_pred_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;best_iterations&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;regressor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;best_iteration&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;average&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;metrics&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;metrics&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;average&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;best_iterations&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;XGBoost has many tuning parameters so an exhaustive grid search has an unreasonable number of combinations. Instead, we tune reduced sets sequentially using grid search and use early stopping.&lt;/p&gt;

&lt;p&gt;This is the typical grid search methodology to tune XGBoost:&lt;/p&gt;

&lt;h4 id=&quot;xgboost-tuning-methodology&quot;&gt;XGBoost tuning methodology&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;Set an initial set of starting parameters.&lt;/li&gt;
  &lt;li&gt;Tune sequentially on groups of hyperparameters that don’t interact too much between groups, to reduce the number of combinations tested.
    &lt;ul&gt;
      &lt;li&gt;First, tune &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;max_depth&lt;/code&gt;.&lt;/li&gt;
      &lt;li&gt;Then tune &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;subsample&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;colsample_bytree&lt;/code&gt;, and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;colsample_bylevel&lt;/code&gt;.&lt;/li&gt;
      &lt;li&gt;Finally, tune &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;learning rate&lt;/code&gt;: a lower learning rate will need more boosting rounds (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;n_estimators&lt;/code&gt;).&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Do 10-fold cross-validation on each hyperparameter combination. Pick hyperparameters to minimize average RMSE over kfolds.&lt;/li&gt;
  &lt;li&gt;Use XGboost early stopping to halt training in each fold if no improvement after 100 rounds.&lt;/li&gt;
  &lt;li&gt;After tuning and selecting the best hyperparameters, retrain and evaluate on the full dataset without early stopping, using the average boosting rounds across xval kfolds.&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
  &lt;li&gt;As discussed, we use the XGBoost sklearn API and roll our own grid search which understands early stopping with k-folds, instead of GridSearchCV. (An alternative would be to use native xgboost .cv which understands early stopping but doesn’t use sklearn API (uses DMatrix, not numpy array or dataframe))&lt;/li&gt;
  &lt;li&gt;We write a helper function &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cv_over_param_dict&lt;/code&gt; which takes a list of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;param_dict&lt;/code&gt; dictionaries, runs trials over all dictionaries, and returns the best &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;param_dict&lt;/code&gt; dictionary plus a dataframe of results.&lt;/li&gt;
  &lt;li&gt;We run &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cv_over_param_dict&lt;/code&gt; 3 times to do 3 grid searches over our 3 tuning rounds.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;BOOST_ROUNDS&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;50000&lt;/span&gt;   &lt;span class=&quot;c1&quot;&gt;# we use early stopping so make this arbitrarily high
&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;cv_over_param_dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;param_dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;predictors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;response&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kfolds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;verbose&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;given a list of dictionaries of xgb params
    run my_cv on params, store result in array
    return updated param_dict, results dataframe
    &quot;&quot;&quot;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;start_time&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;datetime&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;now&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;%-20s %s&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Start Time&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;start_time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;results&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;enumerate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;param_dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;xgb&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;XGBRegressor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;objective&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'reg:squarederror'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;n_estimators&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;BOOST_ROUNDS&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;random_state&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;RANDOMSTATE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;    
            &lt;span class=&quot;n&quot;&gt;verbosity&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;n_jobs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;booster&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'gbtree'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;   
            &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;    

        &lt;span class=&quot;n&quot;&gt;metric_rmse&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;metric_std&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;best_iteration&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;my_cv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;predictors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;response&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kfolds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xgb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;verbose&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;    
        &lt;span class=&quot;n&quot;&gt;results&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;metric_rmse&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;metric_std&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;best_iteration&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
    
        &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;%s %3d result mean: %.6f std: %.6f, iter: %.2f&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;datetime&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;strftime&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;datetime&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;now&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;%T&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;metric_rmse&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;metric_std&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;best_iteration&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
        
    &lt;span class=&quot;n&quot;&gt;end_time&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;datetime&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;now&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;%-20s %s&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Start Time&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;start_time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;%-20s %s&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;End Time&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;end_time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;timedelta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seconds&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;end_time&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;start_time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seconds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
    
    &lt;span class=&quot;n&quot;&gt;results_df&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DataFrame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;results&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;columns&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'rmse'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'std'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'best_iter'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'param_dict'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sort_values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'rmse'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;display&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;results_df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;head&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
    
    &lt;span class=&quot;n&quot;&gt;best_params&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;results_df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iloc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'param_dict'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;best_params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;results_df&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# initial hyperparams
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;current_params&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;'max_depth'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;'colsample_bytree'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;'colsample_bylevel'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;'subsample'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;'learning_rate'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.01&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;##################################################
# round 1: tune depth
##################################################
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;max_depths&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;grid_search_dicts&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'max_depth'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;md&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;md&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;max_depths&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# merge into full param dicts
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;full_search_dicts&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[{&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;current_params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grid_search_dicts&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# cv and get best params
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;current_params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;results_df&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv_over_param_dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;full_search_dicts&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;predictors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;response&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kfolds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;##################################################
# round 2: tune subsample, colsample_bytree, colsample_bylevel
##################################################
# subsamples = np.linspace(0.01, 1.0, 10)
# colsample_bytrees = np.linspace(0.1, 1.0, 10)
# colsample_bylevel = np.linspace(0.1, 1.0, 10)
# narrower search
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;subsamples&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linspace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.25&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.75&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;11&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;colsample_bytrees&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linspace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;colsample_bylevel&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linspace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# subsamples = np.linspace(0.4, 0.9, 11)
# colsample_bytrees = np.linspace(0.05, 0.25, 5)
&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;grid_search_dicts&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;zip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'subsample'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'colsample_bytree'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'colsample_bylevel'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]))&lt;/span&gt; 
                     &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;product&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;subsamples&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;colsample_bytrees&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;colsample_bylevel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# merge into full param dicts
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;full_search_dicts&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[{&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;current_params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grid_search_dicts&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# cv and get best params
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;current_params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;results_df&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv_over_param_dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;full_search_dicts&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;predictors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;response&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kfolds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# round 3: learning rate
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;learning_rates&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;logspace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;grid_search_dicts&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'learning_rate'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lr&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;learning_rates&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# merge into full param dicts
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;full_search_dicts&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[{&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;current_params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grid_search_dicts&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# cv and get best params
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;current_params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;results_df&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv_over_param_dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;full_search_dicts&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;predictors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;response&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kfolds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;verbose&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The total training duration (the sum of times over the 3 iterations) is 1:24:22. This time may be an underestimate, since this search space is based on prior experience.&lt;/p&gt;

&lt;p&gt;Finally, we refit using the best hyperparameters and evaluate:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;xgb&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;XGBRegressor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;objective&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'reg:squarederror'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;n_estimators&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3438&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;random_state&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;RANDOMSTATE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;    
    &lt;span class=&quot;n&quot;&gt;verbosity&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;n_jobs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;booster&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'gbtree'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;   
    &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;current_params&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;    

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xgb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cross_val_score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xgb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predictors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;response&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
                          &lt;span class=&quot;n&quot;&gt;scoring&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;neg_root_mean_squared_error&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                          &lt;span class=&quot;n&quot;&gt;cv&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kfolds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                          &lt;span class=&quot;n&quot;&gt;n_jobs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;raw_scores&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cv_to_raw&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Log1p CV RMSE %.06f (STD %.04f)&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Raw CV RMSE %.0f (STD %.0f)&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;raw_scores&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;raw_scores&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The result essentially matches linear regression but is not as good as ElasticNet.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Raw CV RMSE 18193 (STD 2461)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;10-xgboost-with-hyperopt-optuna-and-ray&quot;&gt;10. XGBoost with Hyperopt, Optuna, and Ray&lt;/h2&gt;

&lt;p&gt;The steps to run a Ray tuning job with Hyperopt are:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Set up a Ray search space as a config dict.&lt;/li&gt;
  &lt;li&gt;Refactor the training loop into a function which takes the config dict as an argument and calls &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tune.report(rmse=rmse)&lt;/code&gt; to optimize a metric like RMSE.&lt;/li&gt;
  &lt;li&gt;Call &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ray.tune&lt;/code&gt; with the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;config&lt;/code&gt; and a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;num_samples&lt;/code&gt; argument which specifies how many times to sample.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Set up the Ray search space:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;xgb_tune_kwargs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;n_estimators&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tune&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loguniform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;max_depth&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tune&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randint&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;subsample&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tune&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;quniform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.25&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.75&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.01&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;colsample_bytree&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tune&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;quniform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.05&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.01&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;colsample_bylevel&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tune&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;quniform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.05&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.01&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;    
    &lt;span class=&quot;s&quot;&gt;&quot;learning_rate&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tune&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;quniform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;3.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# powers of 10
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;xgb_tune_params&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;k&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xgb_tune_kwargs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;keys&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;k&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;!=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'wandb'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;xgb_tune_params&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Set up the training function. Note that some search algos expect all hyperparameters to be floats and some search intervals to start at 0. So we convert params as necessary.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;my_xgb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# fix these configs to match calling convention
&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;# search wants to pass in floats but xgb wants ints
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'n_estimators'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'n_estimators'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;   &lt;span class=&quot;c1&quot;&gt;# pass float eg loguniform distribution, use int
&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;# hyperopt needs left to start at 0 but we want to start at 2    
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'max_depth'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'max_depth'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'learning_rate'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'learning_rate'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    
    &lt;span class=&quot;n&quot;&gt;xgb&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;XGBRegressor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;objective&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'reg:squarederror'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;n_jobs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;random_state&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;RANDOMSTATE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;booster&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'gbtree'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;   
        &lt;span class=&quot;n&quot;&gt;scale_pos_weight&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
        &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cross_val_score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xgb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predictors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;response&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
                                      &lt;span class=&quot;n&quot;&gt;scoring&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;neg_root_mean_squared_error&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                      &lt;span class=&quot;n&quot;&gt;cv&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kfolds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;rmse&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;tune&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;report&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rmse&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rmse&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;rmse&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rmse&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Run Ray Tune:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;algo&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;HyperOptSearch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random_state_seed&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;RANDOMSTATE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# ASHA
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scheduler&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;AsyncHyperBandScheduler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;analysis&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tune&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;my_xgb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                    &lt;span class=&quot;n&quot;&gt;num_samples&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;NUM_SAMPLES&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                    &lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xgb_tune_kwargs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;                    
                    &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;hyperopt_xgb&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                    &lt;span class=&quot;n&quot;&gt;metric&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;rmse&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                    &lt;span class=&quot;n&quot;&gt;mode&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;min&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                    &lt;span class=&quot;n&quot;&gt;search_alg&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;algo&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                    &lt;span class=&quot;n&quot;&gt;scheduler&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scheduler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                    &lt;span class=&quot;n&quot;&gt;verbose&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                   &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Extract the best hyperparameters, and evaluate a model using them:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# results dataframe sorted by best metric
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;param_cols&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'config.'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;k&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;k&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xgb_tune_params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;analysis_results_df&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;analysis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;results_df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'rmse'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'date'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'time_this_iter_s'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;param_cols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sort_values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'rmse'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# extract top row
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;best_config&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;analysis_results_df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iloc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'config.'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xgb_tune_params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;xgb&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;XGBRegressor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;objective&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'reg:squarederror'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;random_state&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;RANDOMSTATE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;    
    &lt;span class=&quot;n&quot;&gt;verbosity&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;n_jobs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;best_config&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xgb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cross_val_score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xgb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predictors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;response&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
                          &lt;span class=&quot;n&quot;&gt;scoring&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;neg_root_mean_squared_error&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                          &lt;span class=&quot;n&quot;&gt;cv&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kfolds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;raw_scores&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cv_to_raw&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Log1p CV RMSE %.06f (STD %.04f)&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Raw CV RMSE %.0f (STD %.0f)&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;raw_scores&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;raw_scores&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;With NUM_SAMPLES=1024 we obtain:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;Raw&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;CV&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;RMSE&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;18309&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;STD&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2428&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We can swap out Hyperopt for Optuna as simply as:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;algo&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;OptunaSearch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;With NUM_SAMPLES=1024 we obtain:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;Raw&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;CV&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;RMSE&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;18325&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;STD&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2473&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;11-lightgbm-with-hyperopt-and-optuna&quot;&gt;11. LightGBM with Hyperopt and Optuna&lt;/h2&gt;

&lt;p&gt;We can also easily swap out XGBoost for LightGBM.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Update the search space using &lt;a href=&quot;https://sites.google.com/view/lauraepp/parameters&quot;&gt;LightGBM equivalents&lt;/a&gt;.&lt;/p&gt;

    &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;lgbm_tune_kwargs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;n_estimators&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tune&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loguniform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;max_depth&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tune&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randint&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;'num_leaves'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tune&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;quniform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;               &lt;span class=&quot;c1&quot;&gt;# xgb max_leaves
&lt;/span&gt;    &lt;span class=&quot;s&quot;&gt;&quot;bagging_fraction&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tune&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;quniform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.01&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;# xgb subsample
&lt;/span&gt;    &lt;span class=&quot;s&quot;&gt;&quot;feature_fraction&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tune&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;quniform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.05&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.01&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;   &lt;span class=&quot;c1&quot;&gt;# xgb colsample_bytree
&lt;/span&gt;    &lt;span class=&quot;s&quot;&gt;&quot;learning_rate&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tune&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;quniform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;3.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Update training function:&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;my_lgbm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    
    &lt;span class=&quot;c1&quot;&gt;# fix these configs 
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'n_estimators'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'n_estimators'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;   &lt;span class=&quot;c1&quot;&gt;# pass float eg loguniform distribution, use int
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'num_leaves'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'num_leaves'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'learning_rate'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'learning_rate'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    
    &lt;span class=&quot;n&quot;&gt;lgbm&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;LGBMRegressor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;objective&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'regression'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                         &lt;span class=&quot;n&quot;&gt;max_bin&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;200&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                         &lt;span class=&quot;n&quot;&gt;feature_fraction_seed&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                         &lt;span class=&quot;n&quot;&gt;min_data_in_leaf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                         &lt;span class=&quot;n&quot;&gt;verbose&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                         &lt;span class=&quot;n&quot;&gt;n_jobs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                         &lt;span class=&quot;c1&quot;&gt;# these are specified to suppress warnings
&lt;/span&gt;                         &lt;span class=&quot;n&quot;&gt;colsample_bytree&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                         &lt;span class=&quot;n&quot;&gt;min_child_samples&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                         &lt;span class=&quot;n&quot;&gt;subsample&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                         &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                         &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    
    &lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cross_val_score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lgbm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predictors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;response&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
                              &lt;span class=&quot;n&quot;&gt;scoring&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;neg_root_mean_squared_error&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                              &lt;span class=&quot;n&quot;&gt;cv&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kfolds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;rmse&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  
    &lt;span class=&quot;n&quot;&gt;tune&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;report&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rmse&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rmse&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'rmse'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)}&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;and run as before, swapping my_lgbm in place of my_xgb. Results for LGBM: (NUM_SAMPLES=1024):&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;Raw&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;CV&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;RMSE&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;18615&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;STD&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2356&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Swapping out Hyperopt for Optuna:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;Raw&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;CV&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;RMSE&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;18614&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;STD&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2423&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;12-xgboost-on-a-ray-cluster&quot;&gt;12. XGBoost on a Ray cluster&lt;/h2&gt;

&lt;p&gt;Ray is a distributed framework. We can run a Ray Tune job over many instances using a cluster with a &lt;em&gt;head node&lt;/em&gt; and many &lt;em&gt;worker nodes&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Launching Ray is straightforward. On the head node we run &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ray start&lt;/code&gt;. On each worker node we run &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ray start --address x.x.x.x&lt;/code&gt;  with the address of the head node. Then in python we call &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ray.init()&lt;/code&gt; to connect to the head node. Everything else proceeds as before, and the head node runs trials using all instances in the cluster and stores results in Redis.&lt;/p&gt;

&lt;p&gt;Where it gets more complicated is specifying all the AWS details, instance types, regions, subnets, etc.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Clusters are defined in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ray1.1.yaml&lt;/code&gt;. (So far in this notebook we have been using the current production ray 1.0, but I had difficulty getting a cluster to run with ray 1.0 so I switched to the dev nightly. YMMV.)&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;boto3&lt;/code&gt; and AWS CLI configured credentials are used to spawn instances, so &lt;a href=&quot;https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-install.html&quot;&gt;install and configure AWS CLI&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Edit &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ray1.1.yaml&lt;/code&gt; file with, at a minimum, your AWS region and availability zone. Imageid may vary across regions, search for the current Deep Learning AMI (Ubuntu 18.04). You may not need to specify subnet, I had an issue with an inaccessible subnet when I let Ray default the subnet, possibly bad defaults somewhere.
    &lt;ul&gt;
      &lt;li&gt;To obtain those variables, launch the latest Deep Learning AMI (Ubuntu 18.04) currently Version 35.0 into a small instance in your favorite region/zone&lt;/li&gt;
      &lt;li&gt;Test that it works&lt;/li&gt;
      &lt;li&gt;Note the 4 variables: region, availability zone, subnet, AMI imageid&lt;/li&gt;
      &lt;li&gt;Terminate the instance and edit &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ray1.1.yaml&lt;/code&gt; with your region, availability zone, AMI imageid, optionally subnet&lt;/li&gt;
      &lt;li&gt;It may be advisable create your own image with all updates and requirements pre-installed and specify its AMI imageid, instead of using the generic image and installing everything at launch.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;To run the cluster: 
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ray up ray1.1.yaml&lt;/code&gt;
    &lt;ul&gt;
      &lt;li&gt;Creates head instance using AMI specified.&lt;/li&gt;
      &lt;li&gt;Installs Ray and related requirements including XGBoost from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;requirements.txt&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;Clones the druce/iowa repo from GitHub&lt;/li&gt;
      &lt;li&gt;Launches worker nodes per auto-scaling parameters (currently we fix the number of nodes because we’re not benchmarking the time the cluster will take to auto-scale)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;After the cluster starts you can check the AWS console and note that several instances were launched.&lt;/li&gt;
  &lt;li&gt;Check &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ray monitor ray1.1.yaml&lt;/code&gt; for any error messages&lt;/li&gt;
  &lt;li&gt;Run Jupyter on the cluster with port forwarding
 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ray exec ray1.1.yaml --port-forward=8899 'jupyter notebook --port=8899'&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Open the notebook on the generated URL which is printed on the console at startup e.g. http://localhost:8899/?token=5f46d4355ae7174524ba71f30ef3f0633a20b19a204b93b4&lt;/li&gt;
  &lt;li&gt;You can run a terminal on the head node of the cluster with
 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ray attach /Users/drucev/projects/iowa/ray1.1.yaml&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;You can ssh explicitly with the IP address and the generated private key
 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ssh -o IdentitiesOnly=yes -i ~/.ssh/ray-autoscaler_1_us-east-1.pem ubuntu@54.161.200.54&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Run port forwarding to the Ray dashboard with &lt;br /&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ray dashboard ray1.1.yaml&lt;/code&gt;
and then open
 http://localhost:8265/&lt;/li&gt;
  &lt;li&gt;Make sure to choose the default kernel in Jupyter to run in the correct conda environment with all installs&lt;/li&gt;
  &lt;li&gt;Make sure to use the ray.init() command given in the startup messages.
 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ray.init(address='localhost:6379', _redis_password='5241590000000000')&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;The cluster will incur AWS charges so &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ray down ray1.1.yaml&lt;/code&gt; when complete&lt;/li&gt;
  &lt;li&gt;See &lt;a href=&quot;https://github.com/druce/iowa/blob/master/hyperparameter_optimization_cluster.ipynb&quot;&gt;hyperparameter_optimization_cluster.ipynb&lt;/a&gt;, separated out so each notebook can be run end-to-end with/without cluster setup&lt;/li&gt;
  &lt;li&gt;See &lt;a href=&quot;https://docs.ray.io/en/latest/cluster/launcher.html&quot;&gt;Ray docs&lt;/a&gt; for additional info on Ray clusters.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Besides connecting to the cluster instead of running Ray Tune locally, no other change to code is needed to run on the cluster&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;analysis&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tune&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;my_xgb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                    &lt;span class=&quot;n&quot;&gt;num_samples&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;NUM_SAMPLES&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                    &lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xgb_tune_kwargs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;                    
                    &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;hyperopt_xgb&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                    &lt;span class=&quot;n&quot;&gt;metric&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;rmse&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                    &lt;span class=&quot;n&quot;&gt;mode&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;min&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                    &lt;span class=&quot;n&quot;&gt;search_alg&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;algo&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                    &lt;span class=&quot;n&quot;&gt;scheduler&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scheduler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                    &lt;span class=&quot;c1&quot;&gt;# add this because distributed jobs occasionally error out
&lt;/span&gt;                    &lt;span class=&quot;n&quot;&gt;raise_on_failed_trial&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# otherwise no reults df returned if any trial error           
&lt;/span&gt;                    &lt;span class=&quot;n&quot;&gt;verbose&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                   &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Results for XGBM on cluster (2048 samples, cluster is 32 m5.large instances):&lt;/p&gt;

&lt;p&gt;Hyperopt (time 1:30:58)&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Raw CV RMSE 18030 (STD 2356)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Optuna (time 1:29:57)&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Raw CV RMSE 18028 (STD 2353)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;13-lightgbm-on-a-ray-cluster&quot;&gt;13. LightGBM on a Ray cluster&lt;/h2&gt;

&lt;p&gt;Similarly for LightGBM:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;analysis&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tune&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;my_lgbm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                    &lt;span class=&quot;n&quot;&gt;num_samples&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;NUM_SAMPLES&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                    &lt;span class=&quot;n&quot;&gt;config&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lgbm_tune_kwargs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                    &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;hyperopt_lgbm&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                    &lt;span class=&quot;n&quot;&gt;metric&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;rmse&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                    &lt;span class=&quot;n&quot;&gt;mode&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;min&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                    &lt;span class=&quot;n&quot;&gt;search_alg&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;algo&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                    &lt;span class=&quot;n&quot;&gt;scheduler&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scheduler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                    &lt;span class=&quot;n&quot;&gt;raise_on_failed_trial&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# otherwise no reults df returned if any trial error                                                            
&lt;/span&gt;                    &lt;span class=&quot;n&quot;&gt;verbose&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                   &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Results for LightGBM on cluster (2048 samples, cluster is 32 m5.large instances):&lt;/p&gt;

&lt;p&gt;Hyperopt (time: 1:05:19) :&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Raw CV RMSE 18459 (STD 2511)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Optuna (time 0:48:16):&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Raw CV RMSE 18458 (STD 2511)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;14-concluding-remarks&quot;&gt;14. Concluding remarks&lt;/h2&gt;

&lt;p&gt;In every case I’ve applied them, Hyperopt and Optuna have obtained at least a small improvement in the best metrics I found using grid search methods. Bayesian optimization tunes faster with a less manual process vs. sequential tuning.  It’s fire-and-forget.&lt;/p&gt;

&lt;p&gt;Is Ray Tune the way to go for hyperparameter tuning? Provisionally, yes. Ray provides integration between the underlying ML (e.g. XGBoost), the Bayesian search (e.g. Hyperopt), and early stopping (ASHA). It allows us to easily swap search algorithms.&lt;/p&gt;

&lt;p&gt;There are other alternative search algorithms in the &lt;a href=&quot;https://docs.ray.io/en/master/tune/api_docs/suggestion.html&quot;&gt;Ray docs&lt;/a&gt; but these seem to be the most popular, and I haven’t got the others to run yet. If after a while I find I am always using e.g. Hyperopt and never use clusters, I might use the native Hyperopt/XGBoost integration without Ray, to access any native Hyperopt features and because it’s one less technology in the stack.&lt;/p&gt;

&lt;p&gt;Clusters? Most of the time I don’t have a need, costs add up, did not see as large a speedup as expected. I only see  ~2x speedup on the 32-instance cluster.  Setting up the test I expected a bit less than 4x speedup accounting for slightly less-than-linear scaling. The longest run I have tried, with 4096 samples, ran overnight on desktop. My MacBook Pro w/16 threads and desktop with 12 threads and GPU are plenty powerful for this data set. Still, it’s useful to have the clustering option in the back pocket. In production, it may be more standard and maintainable to deploy with e.g. Terraform, Kubernetes than the Ray native YAML cluster config file. If you want to train big data at scale you need to really understand and streamline your pipeline.&lt;/p&gt;

&lt;p&gt;It continues to surprise me that ElasticNet, i.e. regularized linear regression, performs slightly better than boosting on this dataset. I heavily engineered features so that linear methods work well. Predictors were chosen using Lasso/ElasticNet and I used log and Box-Cox transforms to force predictors to follow assumptions of least-squares. But still, boosting is supposed to be the gold standard for tabular data.&lt;/p&gt;

&lt;p&gt;This may tend to validate one of the &lt;a href=&quot;https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3624052&quot;&gt;critiques of machine learning&lt;/a&gt;, that the most powerful machine learning methods don’t necessarily always converge all the way to the best solution. If you have a ground truth that is linear plus noise, a complex XGBoost or neural network algorithm should get arbitrarily close to the closed-form optimal solution, but will probably never match the optimal solution exactly. XGBoost regression is piecewise constant and the complex neural network is subject to the vagaries of stochastic gradient descent. I thought arbitrarily close meant almost indistinguishable. But clearly this is not always the case.&lt;/p&gt;

&lt;p&gt;ElasticNet with L1 + L2 regularization plus gradient descent and hyperparameter optimization is still machine learning. It’s simply a form of ML better matched to this problem. In the real world where data sets don’t match assumptions of OLS, gradient boosting generally performs extremely well. And even on this dataset, engineered for success with the linear models, SVR and KernelRidge performed better than ElasticNet (not shown) and ensembling ElasticNet with XGBoost, LightGBM, SVR, neural networks worked best of all.&lt;/p&gt;

&lt;p&gt;To paraphrase Casey Stengel, clever feature engineering will always outperform clever model algorithms and vice-versa&lt;sup id=&quot;fnref:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;. But improving your hyperparameters will always improve your results. Bayesian optimization can be considered a best practice.&lt;/p&gt;

&lt;p&gt;Again, the full code is on &lt;a href=&quot;https://github.com/druce/iowa&quot;&gt;GitHub&lt;/a&gt;&lt;/p&gt;

&lt;script&gt;
    var headings = document.querySelectorAll(&quot;h2[id]&quot;);

    for (var i = 0; i &lt; headings.length; i++) {
        headings[i].innerHTML =
            '&lt;a name=&quot;' + headings[i].id + '&quot;&gt;' +
                headings[i].innerText +
            '&lt;/a&gt;';
    }
&lt;/script&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;It would be more sound to separately tune the stopping rounds. Just averaging the best stopping time across kfolds is questionable. In a real world scenario, we should keep a holdout test set. We should retrain on the full training dataset (not kfolds) with early stopping to get the best number of boosting rounds. Then we should measure RMSE in the test set using all the cross-validated parameters including number of boosting rounds for the expected OOS RMSE. For the purpose of comparing tuning algorithms, comparing the CV error is OK. We are evaluating how we would make model decisions using CV and not too concerned about the generalization error. One could even argue it adds a little more noise to the comparison of hyperparameter selection. But a test set would be the correct methodology in most real-world scenarios. It wouldn’t change conclusions directionally and I’m not going to rerun everything, but if I were to start over I would do it that way. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;This is not intended to make sense. &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Druce Vertes</name></author><category term="datascience" /><category term="datascience" /><summary type="html">Bayesian optimization of machine learning model hyperparameters works faster and better than grid search. Here’s how we can speed up hyperparameter tuning using 1) Bayesian optimization with Hyperopt and Optuna, running on… 2) the Ray distributed machine learning framework, with a unified API to many hyperparameter search algos and early stopping schedulers, and… 3) a distributed cluster of cloud instances for even faster tuning.</summary></entry><entry><title type="html">Deploy a Microservice to AWS Elastic Container Service: The Harder Way and the Easier Way</title><link href="http://0.0.0.0:4000/2020/08/deploy-a-container-to-ecs" rel="alternate" type="text/html" title="Deploy a Microservice to AWS Elastic Container Service: The Harder Way and the Easier Way" /><published>2020-08-27T18:28:57-04:00</published><updated>2020-08-27T18:28:57-04:00</updated><id>http://0.0.0.0:4000/2020/08/deploy-a-container-to-ecs</id><content type="html" xml:base="http://0.0.0.0:4000/2020/08/deploy-a-container-to-ecs">&lt;blockquote&gt;
  &lt;p&gt;A while back I made this &lt;a href=&quot;http://www.streeteye.com/static/Pizza/#&quot;&gt;Pizza service &lt;/a&gt; weekend project and I thought I could just press a button in AWS and deploy it in the cloud. It turned out to be… more complicated. With the latest version of Docker it’s getting easier. Here’s the harder (old) way and the easier (new) way. After some configuration, you can just say &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;docker compose up&lt;/code&gt; and your container is deployed.
&lt;!--more--&gt;&lt;/p&gt;
  &lt;h2 id=&quot;the-harder-way&quot;&gt;The harder way&lt;/h2&gt;
&lt;/blockquote&gt;

&lt;p&gt;Based on this AWS document: &lt;a href=&quot;https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-cli-tutorial-fargate.html&quot;&gt;Tutorial: Creating a Cluster with a Fargate Task Using the Amazon ECS CLI&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;1) &lt;a href=&quot;https://docs.aws.amazon.com/cli/latest/userguide/cli-environment.html&quot;&gt;Install and configure AWS CLI&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;2) &lt;a href=&quot;https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ECS_CLI_installation.html&quot;&gt;Install ECS CLI&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;3) Assume IAM task execution role if needed:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Create the file &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;task-execution-assume-role.json&lt;/code&gt; (see &lt;a href=&quot;task-execution-assume-role.json&quot;&gt;example&lt;/a&gt;. Update region as necessary.)&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
aws iam &lt;span class=&quot;nt&quot;&gt;--region&lt;/span&gt; us-east-1 create-role &lt;span class=&quot;nt&quot;&gt;--role-name&lt;/span&gt; ecsTaskExecutionRole &lt;span class=&quot;nt&quot;&gt;--assume-role-policy-document&lt;/span&gt; file://task-execution-assume-role.json

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;Attach role policy:&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
aws iam &lt;span class=&quot;nt&quot;&gt;--region&lt;/span&gt; us-east-1 attach-role-policy &lt;span class=&quot;nt&quot;&gt;--role-name&lt;/span&gt; ecsTaskExecutionRole &lt;span class=&quot;nt&quot;&gt;--policy-arn&lt;/span&gt; arn:aws:iam::aws:policy/service-role/AmazonECSTaskExecutionRolePolicy

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;4) Configure credentials and default cluster:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ecs-cli configure profile &lt;span class=&quot;nt&quot;&gt;--access-key&lt;/span&gt; &amp;lt;access-key&amp;gt; &lt;span class=&quot;nt&quot;&gt;--secret-key&lt;/span&gt; &amp;lt;secret-key&amp;gt; &lt;span class=&quot;nt&quot;&gt;--profile-name&lt;/span&gt; pizza-profile
INFO[0000] Saved ECS CLI profile configuration default.

ecs-cli configure &lt;span class=&quot;nt&quot;&gt;--cluster&lt;/span&gt; pizza &lt;span class=&quot;nt&quot;&gt;--default-launch-type&lt;/span&gt; FARGATE &lt;span class=&quot;nt&quot;&gt;--config-name&lt;/span&gt; pizza &lt;span class=&quot;nt&quot;&gt;--region&lt;/span&gt; us-east-1
INFO[0000] Saved ECS CLI cluster configuration pizza.

ecs-cli up &lt;span class=&quot;nt&quot;&gt;--cluster-config&lt;/span&gt; pizza &lt;span class=&quot;nt&quot;&gt;--ecs-profile&lt;/span&gt; pizza-profile
INFO[0000] Created cluster                               &lt;span class=&quot;nv&quot;&gt;cluster&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;pizza &lt;span class=&quot;nv&quot;&gt;region&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;us-east-1
INFO[0061] Waiting &lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;your cluster resources to be created...
INFO[0062] Cloudformation stack status                   &lt;span class=&quot;nv&quot;&gt;stackStatus&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;CREATE_IN_PROGRESS
INFO[0122] Cloudformation stack status                   &lt;span class=&quot;nv&quot;&gt;stackStatus&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;CREATE_IN_PROGRESS
VPC created: vpc-123456789012e1234
Subnet created: subnet-1234567890abcdef
Subnet created: subnet-fedcba0987654321
Cluster creation succeeded.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Note that this doesn’t create your own personal cluster but takes advantage of the Fargate container service.&lt;/p&gt;

&lt;p&gt;Take note of info on cluster VPC and subnets reported.&lt;/p&gt;

&lt;p&gt;Get security group&lt;/p&gt;
&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
aws ec2 describe-security-groups &lt;span class=&quot;nt&quot;&gt;--filters&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;Name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;vpc-id,Values&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;vpc-123456789012e1234 &lt;span class=&quot;nt&quot;&gt;--region&lt;/span&gt; us-east-1

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Take note of e.g. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&quot;GroupId&quot;: &quot;sg-1111222233334444&quot;&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;5) Configure security group to allow inbound on the desired port, using the security group-id reported (and correct port and region):&lt;/p&gt;
&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;aws ec2 authorize-security-group-ingress &lt;span class=&quot;nt&quot;&gt;--group-id&lt;/span&gt; sg-1111222233334444 &lt;span class=&quot;nt&quot;&gt;--protocol&lt;/span&gt; tcp &lt;span class=&quot;nt&quot;&gt;--port&lt;/span&gt; 8181 &lt;span class=&quot;nt&quot;&gt;--cidr&lt;/span&gt; 0.0.0.0/0 &lt;span class=&quot;nt&quot;&gt;--region&lt;/span&gt; us-east-1

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;6) Create container repository&lt;/p&gt;
&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;aws ecr create-repository &lt;span class=&quot;nt&quot;&gt;--repository-name&lt;/span&gt; pizza
aws ecr describe-repositories

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Take note of repo URI.&lt;/p&gt;

&lt;p&gt;7) Push Docker image to ECR (replace with your repo URI, region):&lt;/p&gt;
&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;aws ecr get-login-password &lt;span class=&quot;nt&quot;&gt;--region&lt;/span&gt; us-east-1 | docker login &lt;span class=&quot;nt&quot;&gt;-u&lt;/span&gt; AWS &lt;span class=&quot;nt&quot;&gt;--password-stdin&lt;/span&gt; 123412341234.dkr.ecr.us-east-1.amazonaws.com
docker build &lt;span class=&quot;nb&quot;&gt;.&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-t&lt;/span&gt; pizza
docker tag pizza:latest 123412341234.dkr.ecr.us-east-1.amazonaws.com/pizza:latest
docker push 123412341234.dkr.ecr.us-east-1.amazonaws.com/pizza:latest

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;8) Start the container:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Make docker-compose.yml file (see &lt;a href=&quot;ecs-docker-compose.yml&quot;&gt;example&lt;/a&gt;. Specify correct ECR image and region)&lt;/li&gt;
  &lt;li&gt;Make ecs-params.yml file (see &lt;a href=&quot;ecs-params.yml&quot;&gt;example&lt;/a&gt;. Specify correct subnets and security group)&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
ecs-cli compose &lt;span class=&quot;nt&quot;&gt;--project-name&lt;/span&gt; pizza service up &lt;span class=&quot;nt&quot;&gt;--create-log-groups&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--cluster-config&lt;/span&gt; pizza &lt;span class=&quot;nt&quot;&gt;--ecs-profile&lt;/span&gt; pizza-profile

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This will take a minute and show some log messages.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
INFO[0000] Using ECS task definition                     &lt;span class=&quot;nv&quot;&gt;TaskDefinition&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;pizza:1&quot;&lt;/span&gt;
INFO[0000] Auto-enabling ECS Managed Tags
INFO[0016] &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;service pizza&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; has started 1 tasks: &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;task cc0dce4d34054f63b3c734b7d9071189&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;.&lt;/span&gt;  &lt;span class=&quot;nv&quot;&gt;timestamp&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;2020-08-23 19:11:14 +0000 UTC&quot;&lt;/span&gt;
INFO[0066] Service status                                &lt;span class=&quot;nv&quot;&gt;desiredCount&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;1 &lt;span class=&quot;nv&quot;&gt;runningCount&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;1 &lt;span class=&quot;nv&quot;&gt;serviceName&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;pizza
INFO[0066] ECS Service has reached a stable state        &lt;span class=&quot;nv&quot;&gt;desiredCount&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;1 &lt;span class=&quot;nv&quot;&gt;runningCount&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;1 &lt;span class=&quot;nv&quot;&gt;serviceName&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;pizza
INFO[0066] Created an ECS service                        &lt;span class=&quot;nv&quot;&gt;service&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;pizza &lt;span class=&quot;nv&quot;&gt;taskDefinition&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;pizza:1&quot;&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Go to AWS Elastic Container Service console and you should see your cluster running.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2020/ECS.png&quot; alt=&quot;Image of ECS console&quot; /&gt;&lt;/p&gt;

&lt;p&gt;9) Get info on your container:&lt;/p&gt;
&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ecs-cli compose &lt;span class=&quot;nt&quot;&gt;--project-name&lt;/span&gt; pizza service ps &lt;span class=&quot;nt&quot;&gt;--cluster-config&lt;/span&gt; pizza &lt;span class=&quot;nt&quot;&gt;--ecs-profile&lt;/span&gt; pizza-profile
Name                                              State    Ports                        Task          Definition
Healthpizza/0fec210e48734bf1bfca123a88e3a2f1/web  RUNNING  3.237.198.63:8181-&amp;gt;8181/tcp  pizza:1       UNKNOWN

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Note the IP address and port. You should now be able to access the service on IP:port. For pizza try
http://3.237.198.63:8181/query?location=brooklynheights&amp;amp;keyword=pizza&lt;/p&gt;

&lt;p&gt;Note the task id and view logs&lt;/p&gt;
&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
ecs-cli logs &lt;span class=&quot;nt&quot;&gt;--task-id&lt;/span&gt; 0fec210e48734bf1bfca123a88e3a2f1 &lt;span class=&quot;nt&quot;&gt;--follow&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--cluster-config&lt;/span&gt; pizza &lt;span class=&quot;nt&quot;&gt;--ecs-profile&lt;/span&gt; pizza-profile

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;10) Update the docker image&lt;/p&gt;
&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
docker login &lt;span class=&quot;nt&quot;&gt;-u&lt;/span&gt; AWS &lt;span class=&quot;nt&quot;&gt;-p&lt;/span&gt; &lt;span class=&quot;si&quot;&gt;$(&lt;/span&gt;aws ecr get-login-password &lt;span class=&quot;nt&quot;&gt;--region&lt;/span&gt; us-east-1&lt;span class=&quot;si&quot;&gt;)&lt;/span&gt; 123412341234.dkr.ecr.us-east-1.amazonaws.com
docker build &lt;span class=&quot;nb&quot;&gt;.&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-t&lt;/span&gt; pizza
docker tag pizza:latest 123412341234.dkr.ecr.us-east-1.amazonaws.com/pizza:latest
docker push 123412341234.dkr.ecr.us-east-1.amazonaws.com/pizza:latest

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;Go to ECS console https://console.aws.amazon.com/ecs/home?region=us-east-1&lt;/li&gt;
  &lt;li&gt;Click on cluster e.g. pizza&lt;/li&gt;
  &lt;li&gt;Click on service checkbox for Pizza&lt;/li&gt;
  &lt;li&gt;Choose ‘Update’ from dropdown&lt;/li&gt;
  &lt;li&gt;On next page click ‘Force new deployment’, and leave other settings the same , and Update Service&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2020/cluster_update.png&quot; alt=&quot;Image of ECS console&quot; /&gt;
&lt;img src=&quot;/assets/2020/force.png&quot; alt=&quot;Image of ECS console&quot; /&gt;&lt;/p&gt;

&lt;p&gt;11) Shut it down:&lt;/p&gt;
&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
ecs-cli compose &lt;span class=&quot;nt&quot;&gt;--project-name&lt;/span&gt; pizza service down &lt;span class=&quot;nt&quot;&gt;--cluster-config&lt;/span&gt; pizza &lt;span class=&quot;nt&quot;&gt;--ecs-profile&lt;/span&gt; pizza-profile

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;the-easier-way&quot;&gt;The easier way&lt;/h2&gt;

&lt;p&gt;Docker and AWS have been working to integrate and streamline this process. In theory, modulo configuring region and credentials, you should be able to say &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;docker compose up&lt;/code&gt; and run all the above steps automatically.&lt;/p&gt;

&lt;h4 id=&quot;prerequisites&quot;&gt;Prerequisites&lt;/h4&gt;

&lt;p&gt;1) As of this writing (9/2020) you need the Edge Docker Release (&lt;a href=&quot;https://docs.docker.com/docker-for-mac/edge-release-notes/&quot;&gt;Mac&lt;/a&gt; or &lt;a href=&quot;https://docs.docker.com/docker-for-windows/edge-release-notes/&quot;&gt;Windows&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;2) You need a default VPC in your EC2 region. If you have a newer AWS account this should be set up by default. If you have an ancient AWS account you may need to &lt;a href=&quot;https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/vpc-migrate.html#convert-ec2-classic-account&quot;&gt;convert from EC2-Classic to VPC&lt;/a&gt;, or use a new region with no old-style resources.&lt;/p&gt;

&lt;p&gt;3) Enable new ARN format: &lt;a href=&quot;https://us-east-1.console.aws.amazon.com/ecs/home?region=us-east12#/settings&quot;&gt;see here&lt;/a&gt;.&lt;/p&gt;

&lt;h4 id=&quot;steps&quot;&gt;Steps&lt;/h4&gt;

&lt;p&gt;1) Create a new AWS CLI profile for ECS (access key, secret, region, output format e.g. JSON)&lt;/p&gt;
&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
aws configure &lt;span class=&quot;nt&quot;&gt;--profile&lt;/span&gt; ecs

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;2) Create a Docker context for ecs&lt;/p&gt;
&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
docker context create ecs ecs

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Choose your AWS named profile created above i.e. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ecs&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Type your region e.g. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;us-east-1&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Hit enter a couple of times.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;base&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; drucev@MacBook-Pro ~/projects/dockertest/docker-ecs-compose &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;main&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker context create ecs ecs
? Select AWS Profile ecs
? Region us-east-1
? Enter credentials No
Successfully created ecs context &lt;span class=&quot;s2&quot;&gt;&quot;ecs&quot;&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now you can switch between docker profiles with&lt;/p&gt;
&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;		   
docker context use ecs
docker context use default

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;See https://docs.docker.com/engine/reference/commandline/context/ .&lt;/p&gt;

&lt;p&gt;3) Build the Docker image similar to above:&lt;/p&gt;
&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;   
docker context use default
docker build &lt;span class=&quot;nb&quot;&gt;.&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-t&lt;/span&gt; pizza
docker tag pizza:latest 123412341234.dkr.ecr.us-east-1.amazonaws.com/pizza:latest

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;4) Push to Elastic Container Repository similar to above&lt;/p&gt;
&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
aws ecr get-login-password &lt;span class=&quot;nt&quot;&gt;--region&lt;/span&gt; us-east-1 | docker login &lt;span class=&quot;nt&quot;&gt;-u&lt;/span&gt; AWS &lt;span class=&quot;nt&quot;&gt;--password-stdin&lt;/span&gt; 123412341234.dkr.ecr.us-east-1.amazonaws.com
aws ecr create-repository &lt;span class=&quot;nt&quot;&gt;--profile&lt;/span&gt; ecs &lt;span class=&quot;nt&quot;&gt;--repository-name&lt;/span&gt; pizza
docker push 123412341234.dkr.ecr.us-east-1.amazonaws.com/pizza:latest

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;5) Create &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;docker-compose.yml&lt;/code&gt;. See &lt;a href=&quot;new-docker-compose.yml&quot;&gt;new-docker-compose.yml&lt;/a&gt;, which specifies image: $(FRONTEND_IMG). (or you can hard-code it.) Rename or copy to docker-compose.yml.&lt;/p&gt;
&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
version: &lt;span class=&quot;s2&quot;&gt;&quot;3.8&quot;&lt;/span&gt;
services:
  pizza:
    image: &lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;FRONTEND_IMG&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;
    ports:
      - &lt;span class=&quot;s2&quot;&gt;&quot;8181:8181&quot;&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;If you previously created the ‘pizza’ app, go to the ECS console and delete the existing cluster and task. Run the image with:&lt;/p&gt;
&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
docker context use ecs
&lt;span class=&quot;nv&quot;&gt;FRONTEND_IMG&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;123412341234.dkr.ecr.us-east-1.amazonaws.com/pizza:latest docker compose up

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This will take a couple of minutes but should run all the steps specified above.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2020/new_docker_compose_up.png&quot; alt=&quot;Image of docker compose up&quot; /&gt;&lt;/p&gt;

&lt;p&gt;You can then do&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
docker compose ps
ID                                NAME                REPLICAS            PORTS
pizza-PizzaService-9UKSYVKGPMX8   pizza               1/1                 PizzaLoadBalancer-c4f8d3de53d346be.elb.us-east-2.amazonaws.com:8181-&amp;gt;8181/tcp

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;You should be able to connect to your service on the specified port. It may take a minute for DNS to propagate.&lt;/p&gt;

&lt;p&gt;You can also do:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
docker compose logs
&lt;span class=&quot;nv&quot;&gt;FRONTEND_IMG&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;123412341234.dkr.ecr.us-east-1.amazonaws.com/drucev/pizza:latest docker compose convert
docker compose down

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;docker compose convert&lt;/code&gt; outputs the CloudFormation JSON to run the container in the ECS service, with all the network setup, security group and rules etc.&lt;/p&gt;

&lt;p&gt;That’s it! Deploying a public container still seems a little rough around the edges but it’s getting better.&lt;/p&gt;</content><author><name>Druce Vertes</name></author><category term="datascience" /><category term="datascience" /><summary type="html">A while back I made this Pizza service weekend project and I thought I could just press a button in AWS and deploy it in the cloud. It turned out to be… more complicated. With the latest version of Docker it’s getting easier. Here’s the harder (old) way and the easier (new) way. After some configuration, you can just say docker compose up and your container is deployed.</summary></entry><entry><title type="html">The Biggest Bluff: She Stoops To Conquer</title><link href="http://0.0.0.0:4000/2020/07/the-biggest-bluff-she-stoops-to-conquer/" rel="alternate" type="text/html" title="The Biggest Bluff: She Stoops To Conquer" /><published>2020-07-09T18:28:57-04:00</published><updated>2020-07-09T18:28:57-04:00</updated><id>http://0.0.0.0:4000/2020/07/the-biggest-bluff-she-stoops-to-conquer</id><content type="html" xml:base="http://0.0.0.0:4000/2020/07/the-biggest-bluff-she-stoops-to-conquer/">&lt;p&gt;&lt;img src=&quot;/assets/2020/biggestbluff.jpg&quot; alt=&quot;The Biggest Bluff, by Maria Konnikova (cover art)&quot; /&gt;&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;&lt;em&gt;I loved &lt;a href=&quot;https://www.amazon.com/Biggest-Bluff-Learned-Attention-Master/dp/052552262X/&quot;&gt;The Biggest Bluff: How I Learned to Pay Attention, Master Myself, and Win, by Maria Konnikova&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;!--more--&gt;

&lt;p&gt;Like Maria Konnikova, I think poker is a great laboratory for decision-making under uncertainty. Personal story: I had a job as a market analyst, but I was a terrible trader. One issue I had is I couldn’t pull the trigger. For example, in 2002 after Apple introduced the iPod and the stock market was crashing I wrote something about how with their brand they could own this consumer media space. I had it on a list to buy as the market was crashing but the volatility spooked me. Then when the market stabilized the stock was up 50% and I was like, naw, too much. This was at a price of about $12 at the time but split-adjusted about $1, so about 400x ago.&lt;/p&gt;

&lt;p&gt;I was a casual poker player, would play a few times a year for pennies with friends, it was more of an excuse to shoot the shit, not a serious game. Then the big poker boom came along and I thought I should try to beat these small games. Started playing $5 tourneys on PartyPoker and built a bankroll of a couple of thousand dollars, enough to buy some Apple devices and enter a couple of WSOP satellites, which I failed to cash.&lt;/p&gt;

&lt;p&gt;It made me a much better investor because I no longer had a problem pulling the trigger. If I had an idea, I was able to get comfortable with a quick upside/downside +EV calculation and bet a small enough 1/n part of my bankroll that intuitively I knew if I did that n times, on average I would do well and never go bust. I internalized &lt;a href=&quot;https://medium.com/@nickyoder/the-kelly-criterion-cd986d037d87&quot;&gt;Kelly&lt;/a&gt; and what loss I was willing to take and the discipline to take and cut the loss. Poker developed confidence. Now, getting a good investment idea is still rare but I’ve had a few and have done OK overall.&lt;/p&gt;

&lt;p&gt;So anyway, I like poker, am casually serious about understanding the game, I have a bookshelf with about 3 feet of poker books, and have &lt;a href=&quot;/2013/08/risk-arbitrage-investing-and-poker/&quot;&gt;blogged about the lessons poker teaches about investing&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The Biggest Bluff fits in with &lt;a href=&quot;https://www.amazon.com/Positively-Fifth-Street-Murderers-Cheetahs/dp/0312422520/&quot;&gt;Positively 4th Street&lt;/a&gt;, &lt;a href=&quot;https://www.amazon.com/Noble-Hustle-Poker-Jerky-Death/dp/0345804333/&quot;&gt;The Noble Hustle&lt;/a&gt;, &lt;a href=&quot;https://www.amazon.com/Professor-Banker-Suicide-King-Richest/dp/0446694975&quot;&gt;The Professor, The Banker and the Suicide King&lt;/a&gt;, &lt;a href=&quot;https://www.amazon.com/Biggest-Game-Town-Al-Alvarez/dp/0312428421/&quot;&gt;The Biggest Game In Town&lt;/a&gt;, as a very readable memoir of a poker journey.&lt;/p&gt;

&lt;p&gt;It stands apart because Maria really took it seriously, she is a scientist and invested 100% effort to learn the game. She enlists all-time great Erik Seidel (made famous by the WSOP highlight in Rounders, with Matt Damon) to coach her on her journey. She talks to many great players, poker psychologists, uses training/simulation software and really goes all out to win. And she wins a live tournament for an $80,000 payout.&lt;/p&gt;

&lt;p&gt;She puts me to shame because she read (really?) Von Neumann and Morgenstern’s &lt;a href=&quot;https://www.amazon.com/Theory-Games-Economic-Behavior-Commemorative/dp/0691130612/&quot;&gt;Theory of Games&lt;/a&gt;, which is a fairly impenetrable tome. It would literally take me a year. It’s like reading the Feynman lectures on physics to master hacky-sack. All I ever did was take a &lt;a href=&quot;https://www.coursera.org/learn/game-theory-1&quot;&gt;Coursera&lt;/a&gt; on game theory (recommended), work through my own hand-written sims in Java, and work through most of Chen’s &lt;a href=&quot;https://www.amazon.com/Mathematics-Poker-Bill-Chen/dp/1886070253&quot;&gt;Mathematics of Poker&lt;/a&gt;. In my defense when I got into poker some of the modern tools were not available, Pio Solver, PokerTracker, FlopZilla, SnapShove.&lt;/p&gt;

&lt;p&gt;The book succeeds as, what you have to sacrifice to become good at something. She’s very honest. It’s a tough game, the struggle is real, and you feel it. There’s a huge element of mastering yourself. I personally do this by not playing stakes above $1/2 level, or tournaments for much over $100. In other words, being a coward. She really challenged herself to play some of the biggest, toughest tournaments. I’m in it mostly for the cheap thrills, the social element, the mental and self-mastery challenge, and proving I can be good at something.&lt;/p&gt;

&lt;p&gt;Why did she do it, really? Was it worth it? I have questions.&lt;/p&gt;

&lt;p&gt;I have a great deal of sympathy for Maria’s grandmother, who is dismayed that, as a made woman of the intelligentsia, Columbia PhD, multiple books and New Yorker bylines, she would stoop to become a gambler. Like Baba Anya, I think poker is a fun but somewhat pointless exercise. Like Shaw said about chess, a foolish expedient for making idle people think they are doing something clever when they are only wasting their time.&lt;/p&gt;

&lt;p&gt;After all that studying, why does she bet 2x the big blind in early position with AA?&lt;/p&gt;

&lt;p&gt;And did Von Neumann, the guy who invented game theory, really think he could beat roulette? Unlikely.&lt;/p&gt;

&lt;p&gt;In her early ‘failure’, doesn’t she get all in with the best of it and lose to a flush suck-out? And in her big success, doesn’t she benefit from a miracle suck-out? As a player, I wish she would walk through the math and give examples of breakthroughs that made her play better. It’s not a book for a hard-core poker audience. And even though avoiding the skill/luck fallacy is a theme of the book, I think that in order to make the book into a readable narrative, she still falls victim to it.&lt;/p&gt;

&lt;p&gt;When I occasionally read about some poker player of the year in &lt;a href=&quot;https://www.cardplayer.com/poker-players/player-of-the-year/&quot;&gt;Card Player&lt;/a&gt; magazine, I’m reminded a little of tongue-in-cheek &lt;a href=&quot;https://twitter.com/davdchristmas/status/1239196410857340933&quot;&gt;marble racing videos&lt;/a&gt;, a &lt;a href=&quot;https://time.com/5803841/marble-racing-videos/&quot;&gt;thing&lt;/a&gt; apparently. Or &lt;a href=&quot;https://twitter.com/joonlee/status/808415700905787392?lang=en&quot;&gt;Janken Queen&lt;/a&gt;. (Or &lt;a href=&quot;https://www.youtube.com/watch?v=kB5XXn0eKow&quot;&gt;World Series of Dice&lt;/a&gt;, but that’s a bit different.) I would love to see a ‘Best In Show’ style spoof of the poker world. The winner of the WSOP is usually a very good player who went on a lucky heater, or at least dodged an unlucky streak.&lt;/p&gt;

&lt;p&gt;One thing poker teaches you is, variance is really going to vary. While I was a consistently winning micro-stakes online player, I once had a big drawdown over a month when it seemed like &lt;em&gt;every time&lt;/em&gt; I went in with the best of it, I lost. I was astounded. People attribute far too much to skill. Taleb may be an ass, but ‘Fooled by Randomness’ is a classic. Nevertheless what you learn about luck from playing poker, you cannot learn from a book.&lt;/p&gt;

&lt;p&gt;Ultimately poker is a fascinating, but slightly silly game. In tennis, golf, chess, a pro cannot lose to an amateur at the turn of an unfriendly card. You can’t have a Jamie Gold going on a huge heater, getting hit by the deck, and winning the WSOP, despite being an apparently mediocre player. All the hard work you can muster creates a couple of big blinds an hour of edge in a game where you have to risk your whole stack to cash in that edge.&lt;/p&gt;

&lt;p&gt;Maybe that’s one reason why it’s a metaphor for life? An endlessly fascinating but perhaps ultimately pointless exercise, which we have a capacity for making great or terrible by investing with manufactured meaning?&lt;/p&gt;

&lt;p&gt;Sometimes poker is how life works. You have to work hard every day to get a little better, exercise great judgment to stay in the game. And be all-in where you have the best of it. And if you play your cards right you’ll do well. But a couple of bad breaks, and you’re screwed. Graduate at the wrong time in the teeth of recession, pick a company or industry that goes belly-up and people no longer value skills that took years to master, get cancer. Hell, we’re lucky to be born in the right place at the right time and be at the table. All you can do is make good decisions and master yourself, which is the true object of the game. And let the cards fall how they may.&lt;/p&gt;

&lt;p&gt;It was interesting to see &lt;a href=&quot;https://www.youtube.com/watch?v=srQnnJtjJ2k&quot;&gt;Doug Polk&lt;/a&gt; quit the game. Burnout, depression, a more mature perspective on life? I wonder what Maria thinks about that?&lt;/p&gt;

&lt;p&gt;The book alludes to the moral degeneracy of the poker world, but maybe pulls a few punches. If you view poker with detachment as an exercise in understanding risk, probability, yourself, and making good decisions at small stakes, it’s fun and healthy. When you make gambling your profession, taking others’ money by your wits, there can be something fundamentally problematic about the zero-sum aspect of it, about staking your worth on the outcome of the turn of a card, about the dark side of angle shooters, of substance abusers.&lt;/p&gt;

&lt;p&gt;A used-car salesman can say he is providing a service, helping people get into the right car. Being a poker pro is a bit hard to rationalize. You’re providing an entertainment service, letting people challenge their wits against the best?&lt;/p&gt;

&lt;p&gt;The jailbird Ed Norton character in Rounders illustrates the darker side of the game. Not a great movie but &lt;a href=&quot;https://www.youtube.com/watch?v=i4A4TzHHdko&quot;&gt;High Roller: The Stu Ungar Story&lt;/a&gt; illustrates it. There’s something weirdly intoxicating and addictive to gambling, to be good at poker you have to be obsessive, and these guys are often prone to addictive behaviors and not very healthy mentally or physically. The household names who become stars on ESPN2 go bust and beg strangers to stake them. I’ve seen ridiculous tantrums and chair-throwing (which does get you banned LOL). I’ve seen college students fresh off the bus tilt and lose a semester of savings in 30 minutes.&lt;/p&gt;

&lt;p&gt;Poker is a great training ground for investing, but investing is a positive-sum game in the long run. Ultimately there is a social purpose, to &lt;a href=&quot;https://www.brainyquote.com/quotes/john_maynard_keynes_152044#:~:text=John%20Maynard%20Keynes%20Quotes&amp;amp;text=The%20social%20object%20of%20skilled%20investment%20should%20be%20to%20defeat,ignorance%20which%20envelope%20our%20future.&quot;&gt;defeat the dark forces of time and ignorance that envelop our future.&lt;/a&gt; The market is not always as fun and you can’t bluff the market, and it has its share of con artists and angle-shooters, but I think it’s a more proper object of talent for people who understand risk, expected value, human nature… and luck.&lt;/p&gt;</content><author><name>Druce Vertes, CFA</name></author><category term="poker" /><category term="poker" /><category term="books" /><summary type="html">I loved The Biggest Bluff: How I Learned to Pay Attention, Master Myself, and Win, by Maria Konnikova.</summary></entry><entry><title type="html">Deep Reinforcement Learning For Trading Applications (Alpha Architect)</title><link href="http://0.0.0.0:4000/2020/02/deep-reinforcement-learning-for-trading-applications" rel="alternate" type="text/html" title="Deep Reinforcement Learning For Trading Applications (Alpha Architect)" /><published>2020-02-26T17:28:57-05:00</published><updated>2020-02-26T17:28:57-05:00</updated><id>http://0.0.0.0:4000/2020/02/deep-reinforcement-learning-for-trading-applications</id><content type="html" xml:base="http://0.0.0.0:4000/2020/02/deep-reinforcement-learning-for-trading-applications">&lt;p&gt;&lt;img src=&quot;/assets/2020/rl.png&quot; alt=&quot;Observing one reinforcement learning episode of stock trading&quot; /&gt;&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;Reinforcement learning is a machine learning paradigm that can learn behavior to achieve maximum reward in complex dynamic environments, as simple as Tic-Tac-Toe, or as complex as Go, and options trading. In this post, we will try to explain what reinforcement learning is, share code to apply it, and references to learn more about it. First, we’ll learn a simple algorithm to play Tic-Tac-Toe, then learn to trade a non-random price series. Finally, we’ll talk about how reinforcement learning can master complex financial concepts like option pricing and optimal diversification.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;!--more--&gt;

&lt;p&gt;Originally Published at &lt;a href=&quot;alphaarchitect.com&quot;&gt;Alpha Architect&lt;/a&gt;:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://alphaarchitect.com/2020/02/26/reinforcement-learning-for-trading/&quot;&gt;Deep Reinforcement Learning For Trading Applications&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;By &lt;a href=&quot;https://alphaarchitect.com/user/druce.vertes/&quot;&gt;Druce Vertes&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;February 26, 2020&lt;/p&gt;

&lt;p&gt;Tags: Research Insights, Machine Learning&lt;/p&gt;</content><author><name>Druce Vertes</name></author><category term="datascience" /><category term="investing" /><category term="datascience" /><category term="investing" /><summary type="html">Reinforcement learning is a machine learning paradigm that can learn behavior to achieve maximum reward in complex dynamic environments, as simple as Tic-Tac-Toe, or as complex as Go, and options trading. In this post, we will try to explain what reinforcement learning is, share code to apply it, and references to learn more about it. First, we’ll learn a simple algorithm to play Tic-Tac-Toe, then learn to trade a non-random price series. Finally, we’ll talk about how reinforcement learning can master complex financial concepts like option pricing and optimal diversification.</summary></entry><entry><title type="html">Forecasting US Equity Market Returns with Machine Learning (Alpha Architect)</title><link href="http://0.0.0.0:4000/2020/01/forecasting-us-equity-market-returns-with-machine-learning" rel="alternate" type="text/html" title="Forecasting US Equity Market Returns with Machine Learning (Alpha Architect)" /><published>2020-01-07T17:28:57-05:00</published><updated>2020-01-07T17:28:57-05:00</updated><id>http://0.0.0.0:4000/2020/01/forecasting-us-equity-market-returns-with-machine-learning</id><content type="html" xml:base="http://0.0.0.0:4000/2020/01/forecasting-us-equity-market-returns-with-machine-learning">&lt;p&gt;&lt;img src=&quot;/assets/2020/cape.png&quot; alt=&quot;Image source: https://www.valuewalk.com/wp-content/uploads/2017/07/SSRN-id2983860.pdf&quot; /&gt;&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;Shiller’s CAPE ratio is a popular and useful metric for measuring whether stock prices are overvalued or undervalued relative to earnings. Recently, Vanguard analysts Haifeng Wang, Harshdeep Singh Ahluwalia, Roger A. Aliaga-Díaz, and Joseph H. Davis have written a very interesting paper on forecasting equity returns using Shiller’s CAPE and machine learning: “The Best of Both Worlds: Forecasting US Equity Market Returns using a Hybrid Machine Learning – Time Series Approach“, which effectively applies machine learning to an import investing problem. Image source: &lt;a href=&quot;https://www.valuewalk.com/wp-content/uploads/2017/07/SSRN-id2983860.pdf&quot;&gt;Improving U.S. stock return forecasts: A “fair-value” CAPE approach, Joseph Davis, et al. (2017)&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;!--more--&gt;

&lt;p&gt;Originally Published at &lt;a href=&quot;alphaarchitect.com&quot;&gt;Alpha Architect&lt;/a&gt;:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://alphaarchitect.com/2020/01/07/forecasting-us-equity-market-returns-using-machine-learning/&quot;&gt;Forecasting US Equity Market Returns with Machine Learning&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;By &lt;a href=&quot;https://alphaarchitect.com/user/druce.vertes/&quot;&gt;Druce Vertes&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;January 7, 2020&lt;/p&gt;

&lt;p&gt;Tags: Research Insights, Machine Learning&lt;/p&gt;</content><author><name>Druce Vertes</name></author><category term="datascience" /><category term="investing" /><category term="datascience" /><category term="investing" /><summary type="html">Shiller’s CAPE ratio is a popular and useful metric for measuring whether stock prices are overvalued or undervalued relative to earnings. Recently, Vanguard analysts Haifeng Wang, Harshdeep Singh Ahluwalia, Roger A. Aliaga-Díaz, and Joseph H. Davis have written a very interesting paper on forecasting equity returns using Shiller’s CAPE and machine learning: “The Best of Both Worlds: Forecasting US Equity Market Returns using a Hybrid Machine Learning – Time Series Approach“, which effectively applies machine learning to an import investing problem. Image source: Improving U.S. stock return forecasts: A “fair-value” CAPE approach, Joseph Davis, et al. (2017)</summary></entry><entry><title type="html">Understanding Classification Thresholds Using Isocurves</title><link href="http://0.0.0.0:4000/2019/10/understanding-classification-thresholds-using-isocurves" rel="alternate" type="text/html" title="Understanding Classification Thresholds Using Isocurves" /><published>2019-10-15T18:28:57-04:00</published><updated>2019-10-15T18:28:57-04:00</updated><id>http://0.0.0.0:4000/2019/10/understanding-classification-thresholds-using-isocurves</id><content type="html" xml:base="http://0.0.0.0:4000/2019/10/understanding-classification-thresholds-using-isocurves">&lt;p&gt;&lt;img src=&quot;/assets/2020/isocurves.png&quot; alt=&quot;Four isocurve plots&quot; /&gt;&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;As a data scientist, you might say…“A blog post about thresholds? It’s not even a data science problem, it’s more of a business decision.” And you would not be wrong! Threshold selection lacks the appeal of say, generative adversarial networks.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;!--more--&gt;

&lt;p&gt;Now you are in a conference room, presenting your work on a classification problem. You demonstrate all the magic you performed with feature engineering, predictor selection, model selection, hyperparameter tuning, and ensembling. You conclude your presentation with the predicted probabilities and the &lt;a href=&quot;https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_the_curve&quot;&gt;ROC curve&lt;/a&gt;, and an excellent AUC. You sit down confident in a job well done.&lt;/p&gt;

&lt;p&gt;And the manager says, “What am I supposed to do with all this probability and ROC stuff? I just want to know if I should do x or y.” Where x might be, should I classify this message as spam or not? Or should my Tesla hit the brakes faced with this road condition? Or should I approve this loan or not?&lt;/p&gt;

&lt;p&gt;This post is for you.&lt;/p&gt;

&lt;p&gt;Your job as a data scientist isn’t done until you explain how to interpret the model and apply it. That means threshold selection for the business decision that motivated the model.&lt;/p&gt;

&lt;p&gt;Here’s a deep dive into threshold selection, including the F1-score and how it compares to other metrics. Let’s dive in!&lt;/p&gt;

&lt;h2 id=&quot;1-you-need-a-metric-or-a-cost-function-to-optimize&quot;&gt;1. You need a metric or a cost function to optimize.&lt;/h2&gt;

&lt;p&gt;A business doesn’t care about winning a Kaggle contest. The only valid reasons for a business to spend money on data science are to:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Acquire and retain customers…&lt;/li&gt;
  &lt;li&gt;Reduce costs…&lt;/li&gt;
  &lt;li&gt;Otherwise execute the business better, which…&lt;/li&gt;
  &lt;li&gt;Increases profits.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;If you understand the business objective of the model, what key performance indicators (KPIs) the model can improve, and apply a scientific method to asking the right questions and improving those KPIs, you may be an A-team data scientist.&lt;/p&gt;

&lt;p&gt;Threshold selection is necessary in the context of an algorithm which predicts the probability that an observation will belong to the positive class. This is how most (but not all) classification algorithms work. The question then becomes, if we need to make discrete predictions based on the modeled probability, what is the best threshold to classify an observation as positive?&lt;/p&gt;

&lt;p&gt;The first question to ask is: Can you quantify the cost of &lt;a href=&quot;https://en.wikipedia.org/wiki/Type_I_and_type_II_errors&quot;&gt;type I and type II errors&lt;/a&gt; using KPIs the business cares about? What is the marginal cost of a false positive, and what is the marginal cost of a false negative? If you can identify the right cost function, your job is essentially done.&lt;/p&gt;

&lt;p&gt;Select the threshold that minimizes the total cost of false positives and false negatives in cross-validation. (Always in cross-validation: never select a hyperparameter or make any decision about your model using training data or test data; the classification threshold can be considered a hyperparameter.)&lt;/p&gt;

&lt;p&gt;Suppose we are predicting whether a borrower will default on a credit card. Extending credit that should have been denied costs $50,000 in credit losses and administrative costs. Denying credit that should have been extended results in a loss of the $10,000 lifetime value of the customer (LTV). Pick the threshold that gives the lowest total cost of false positives and false negatives over your cross-validation set (or folds).&lt;/p&gt;

&lt;h2 id=&quot;2-the-roc-curve-visualizes-the-set-of-feasible-solutions-as-you-vary-the-classification-threshold-implicitly-varying-the-cost-of-false-positives-relative-to-false-negatives&quot;&gt;2. The ROC curve visualizes the set of feasible solutions, as you vary the classification threshold, implicitly varying the cost of false positives relative to false negatives.&lt;/h2&gt;

&lt;p&gt;If the positive class represents the detection of a stop sign or a medical condition, the cost of a false negative is high. You need a low threshold to minimize false negatives, which cause you to blow through an intersection and collide with traffic or fail to obtain further life-saving diagnosis or treatment.&lt;/p&gt;

&lt;p&gt;If your problem is spam detection, blocking an important email is more costly than letting through a spam message. So you prefer to tilt toward a higher threshold to tag email as spam, to minimize false positives.&lt;/p&gt;

&lt;p&gt;This is the ROC plot from a Kaggle data set.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 1. The ROC Curve&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2019/fig1.png&quot; alt=&quot;Figure 1. The ROC Curve&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Let’s review how to interpret the ROC plot:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;A &lt;em&gt;true positive&lt;/em&gt; is a true observation correctly predicted to be true.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The &lt;em&gt;true-positive rate (TPR)&lt;/em&gt; is the number of &lt;em&gt;true positives / ground truth positives&lt;/em&gt; (also called &lt;em&gt;recall&lt;/em&gt; or &lt;em&gt;sensitivity&lt;/em&gt;). &lt;em&gt;Ground truth positives = true positives + false negatives&lt;/em&gt;:&lt;/p&gt;

\[TPR = \frac{tp}{tp+fn}\]
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A &lt;em&gt;false positive&lt;/em&gt; is a false observation incorrectly predicted to be true.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The &lt;em&gt;false-positive rate (FPR)&lt;/em&gt; is the number of &lt;em&gt;false positives / ground truth negatives&lt;/em&gt; (1 — &lt;em&gt;FPR&lt;/em&gt; is the &lt;em&gt;specificity&lt;/em&gt;). &lt;em&gt;Ground truth negatives = true negatives + false positives&lt;/em&gt;:&lt;/p&gt;

\[FPR = \frac{fp}{tn + fp}\]
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The best place to be on the ROC chart is the top left corner, with 100% TPR, sensitivity, or recall, and 0% FPR, or 100% specificity. This is usually not feasible.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The best &lt;em&gt;feasible&lt;/em&gt; points are on the ROC curve. As you move from left to right, you decrease your threshold to classify an observation as positive. You get more true positives, but also more false positives.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A good analogy is fishing with a net: As you use a finer net, a smaller number of fish slip through, but you also catch more seaweed and garbage. The $64,000 question is how fine a net to use for the best possible results.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;3-the-slope-of-the-roc-curve-reflects-how-many-true-positives-you-gain-for-each-false-positive-you-accept&quot;&gt;3. The slope of the ROC curve reflects how many true positives you gain for each false positive you accept.&lt;/h2&gt;

&lt;p&gt;Let’s discuss how to interpret the slope of the ROC curve.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 2. The Slope of the ROC Curve&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2019/fig2.png&quot; alt=&quot;Figure 2. The Slope of the ROC Curve&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We plot the TPR on the left axis and the FPR on the bottom axis. And we also plot the 45º line as a dashed line. The 45º line is the ROC curve you would get if you classified your observations randomly using the base rate. In this example, about 20.8% of observations are ground truth positive. If you use a random classifier which randomly classifies 20.8% as positive, your ROC curve will approximately follow the 45º line. It’s the best AUC (Area Under the ROC Curve) you can get without doing any modeling. Or the worst-case AUC of a failed attempt at modeling which is unable to find any predictive signal.&lt;/p&gt;

&lt;p&gt;Recalling that the TPR is true positives as a percentage of ground truth positive, we put the actual true positive count on the right axis. Likewise, since the FPR is false positives as a percent of ground truth negatives, the actual false positive count is on the top axis.&lt;/p&gt;

&lt;p&gt;This helps us interpret the slope: The slope is the number of additional true positives you gain by accepting one additional false positive. (An approximation, keeping in mind that the ROC curve is made up of discrete points).&lt;/p&gt;

&lt;p&gt;The 45º line has a slope of 1 in TPR/FPR space (percentage point increase in TPR per percentage point of FPR). In raw true positive/false positive space, the slope of the 45º line is the total number of positives/total number of negatives.&lt;/p&gt;

&lt;p&gt;If the positive class occurs with probability &lt;em&gt;p&lt;/em&gt;, the slope is: \(\frac{p}{1 - p}\)&lt;/p&gt;

&lt;p&gt;If you have an ideal ROC curve with a continuously decreasing slope (concave down), you can start at the bottom left and keep going toward the top right as long as the cost of additional true positives expressed in false negatives is acceptable.&lt;/p&gt;

&lt;p&gt;Real-world ROCs are often messier, and may not have a continuously decreasing slope as you modify your threshold. But the ROC curve is always positively sloped, and never flat or vertical except at the edges. That would imply two points where one is better on one dimension and equal on the other. Then only one can be an optimal solution, and only that one should be on the ROC curve. Furthermore, most reasonable threshold selection methodologies we discuss below will tend to ignore points in regions that aren’t on the ROC’s outer ‘hull’ but are in local dips toward the bottom right. So thinking about a ROC curve with continuously decreasing slope is reasonable.&lt;/p&gt;

&lt;h2 id=&quot;4-precision-recall-and-the-f-score&quot;&gt;4. Precision, recall, and the F-score&lt;/h2&gt;

&lt;p&gt;Now, what happens if you can’t pin down the real-world cost of false positives and false negatives?&lt;/p&gt;

&lt;p&gt;A frequently used metric is the &lt;em&gt;F1-score&lt;/em&gt;, which is the harmonic mean of precision and recall.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;Precision&lt;/em&gt; is true positives/predicted positives, i.e. % of our positive predictions that we predicted correctly.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;Recall&lt;/em&gt; is TPR: true positives/ground truth positive, i.e. % of ground truth positives that we predicted correctly.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;To calculate the harmonic mean, we invert the inputs, calculate their mean, then invert the mean:&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

\[F_{1} = \frac{1}{\frac{1}{2}\frac{1}{precision} + \frac{1}{2}\frac{1}{recall}}\]

\[= \frac{2}{\frac{1}{precision} + \frac{1}{recall}}\]

\[= 2 \frac{precision \cdot recall}{precision + recall}\]

&lt;p&gt;To gain intuition about the harmonic mean: If a vehicle travels a distance &lt;em&gt;d&lt;/em&gt; outbound at a speed &lt;em&gt;x&lt;/em&gt; (&lt;em&gt;e.g.&lt;/em&gt; 60 &lt;em&gt;km/h&lt;/em&gt;) and returns the same distance at a speed &lt;em&gt;y&lt;/em&gt; (&lt;em&gt;e.g.&lt;/em&gt; 20 &lt;em&gt;km/h&lt;/em&gt;), then its average speed is the harmonic mean of &lt;em&gt;x&lt;/em&gt; and &lt;em&gt;y&lt;/em&gt; (&lt;em&gt;30 km/h&lt;/em&gt;).&lt;/p&gt;

&lt;p&gt;This is &lt;u&gt;*not*&lt;/u&gt; equal to the arithmetic mean (40 &lt;em&gt;km/h&lt;/em&gt;), which would be applicable if you traveled an equal &lt;em&gt;time&lt;/em&gt; in 2 different-speed legs.&lt;/p&gt;

&lt;p&gt;If speed is near 0 on either leg, the harmonic mean will be near 0 for the whole distance regardless of the speed on the other leg. If the speed is exactly 0 on either leg, you never get to your destination, so the average speed for the whole trip is 0.&lt;/p&gt;

&lt;p&gt;The F1-score balances precision and recall and penalizes extreme weakness in either one.&lt;/p&gt;

&lt;p&gt;The F1-score is &lt;em&gt;not symmetric&lt;/em&gt;. We have an unbalanced 79.2%/20.8% ratio of negatives/positives. Suppose our classifier predicts everything as positive. Then recall is 1, precision is 0.208, F1 is 0.344. Now suppose the labels are reversed, you predict everything as positive, recall is 1, precision is 0.792, F1 is 0.884. This looks great but it’s not, since all we did was classify everything as the majority class. Due to this asymmetry, we are always careful to &lt;em&gt;label the minority class as the positive class&lt;/em&gt; when we calculate the F1-score.&lt;/p&gt;

&lt;p&gt;Asymmetry is sometimes fine. Sometimes the business problem is asymmetric. In a search engine application, we don’t care much about all the documents we didn’t retrieve.&lt;/p&gt;

&lt;p&gt;We can tilt the threshold toward precision or recall by using a weighted mean instead of an equal-weighted mean, in which case we call it F2-score etc.&lt;/p&gt;

&lt;p&gt;F-score seems reasonable, although the applicability of the harmonic mean to threshold selection is not completely intuitive to me. I’ve never encountered a business problem where the real-world cost function of false positives and false negatives is a harmonic mean.&lt;/p&gt;

&lt;h2 id=&quot;5-isocurves-help-you-visualize-any-scoring-or-cost-function-like-f1&quot;&gt;5. Isocurves help you visualize any scoring or cost function like F1&lt;/h2&gt;

&lt;p&gt;If you want the best tradeoff, it helps to visualize the tradeoff with isocurves. An iso-cost curve or indifference curve depicts a set of points that have the same cost or metric value: for instance all the points with an F1-score of 0.5. This lets you visualize how the metric evolves as you move along the ROC curve.&lt;/p&gt;

&lt;p&gt;For the F1 metric, the isocurve plot looks like this.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 3. F1-score Isocurves&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2019/fig3.png&quot; alt=&quot;Figure 3. F1-score Isocurves&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Bottom axis: recall = 0, F1 = 0&lt;/li&gt;
  &lt;li&gt;Top left: we classify perfectly: recall = 1, precision = 1, F1 = 1&lt;/li&gt;
  &lt;li&gt;Top right: we classify all positive: recall = 1, precision = 0.208, F1 = 0.344&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To choose the point with the highest F1, pick the point on the ROC curve which sits on the best F1 isocurve, the isocurve closest to top left.&lt;/p&gt;

&lt;h2 id=&quot;6-linear-cost-isocurves&quot;&gt;6. Linear cost isocurves&lt;/h2&gt;

&lt;p&gt;F-score is a nonlinear cost function. If the cost function is a linear function of TPR and FPR (or false positives and false negatives), the relative cost to trade one false positive for one false negative is &lt;em&gt;constant&lt;/em&gt;, and you have evenly spaced linear isocurves.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 4. Linear Cost Isocurves&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2019/fig4.png&quot; alt=&quot;Figure 4. Linear Cost Isocurves&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Again, choose the point on the ROC curve which is also on the isocurve closest to the top left. In this example, the relative price is &lt;em&gt;p/(1-p)&lt;/em&gt;. So the isocurves parallel the 45º line, which corresponds to a random classifier using the base rate &lt;em&gt;p&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Comparing to F1 isocurves, as you go up, F1s get higher faster on the left. So as you improve your ROC, all else being equal, the path connecting best F1 points will curve toward the left (concave down). The path connecting the best linear costs will be a straight line. Proof, as they say, left as an exercise.&lt;/p&gt;

&lt;p&gt;We can weight the relative cost of false positives to false negatives. As we do, the slope of the isocurves changes, and we will tend to favor precision over recall or vice versa.&lt;/p&gt;

&lt;h2 id=&quot;7-circular-or-closest-to-top-left-isocurves&quot;&gt;7. Circular or ‘closest to top left’ isocurves&lt;/h2&gt;

&lt;p&gt;Suppose we want to be as close as possible to top left by Euclidean distance. That corresponds to circular isocurves. The cost function is the distance from the top left:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 5. Closest to Top Left: Circular Isocurves&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2019/fig5.png&quot; alt=&quot;Figure 5. Circular Isocurves&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We could also weight the terms under the square root to favor precision or recall, and squash the circular curves into elliptical curves.&lt;/p&gt;

&lt;p&gt;Compared to constant cost isocurves, the circular isocurves will tend to keep the optimal point toward the middle, near the diagonal ridge, balancing precision and recall.&lt;/p&gt;

&lt;h2 id=&quot;8-mutual-information&quot;&gt;8. Mutual information&lt;/h2&gt;

&lt;p&gt;Finally, let’s look at &lt;em&gt;mutual information&lt;/em&gt;. Mutual information is an information theory metric, like &lt;a href=&quot;https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a&quot;&gt;&lt;em&gt;log loss&lt;/em&gt;&lt;/a&gt;. Log loss can be interpreted as the wrongness, or surprise in our prediction vs. the actual outcomes, measured in bits. &lt;a href=&quot;https://en.wikipedia.org/wiki/Mutual_information&quot;&gt;&lt;em&gt;Mutual information&lt;/em&gt;&lt;/a&gt; can be interpreted as the amount of correct information in our prediction, measured in bits.&lt;/p&gt;

&lt;p&gt;Mutual information is what we use to measure the bandwidth capacity of a signal over a telephone wire or radio channel. We can view a prediction as a signal about the future. In the same sense that mutual information measures how much useful signal can be transmitted on a channel, mutual information tells us how much useful information we have about the future.&lt;/p&gt;

&lt;p&gt;If we use mutual information as our cost function, we get isocurves that look like this.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 6. Mutual Information Isocurves&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2019/fig6.png&quot; alt=&quot;Figure 6. Mutual Information Isocurves&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Maximizing mutual information, in an information theory sense, minimizes the bits of surprise or wrongness in the prediction, and maximizes the bits of ‘correct’ information. Absent explicit costs for false positives and false negatives, mutual information seems like a natural metric choice.&lt;/p&gt;

&lt;p&gt;However, mutual information isocurves match the general shape of the ROC curve. So they may not be very stable in terms of choosing whether to err on the side of precision or recall.&lt;/p&gt;

&lt;p&gt;The choice to lean toward precision or recall is based on whether the algorithm is better at precision or recall, as opposed to any real-world cost. So the choice of threshold might be arbitrary and hard to interpret.&lt;/p&gt;

&lt;p&gt;It’s also worth noting that a perfectly wrong prediction contains the same mutual information as a perfectly correct prediction because you could extract the correct prediction from that signal. This too is a counterintuitive.&lt;/p&gt;

&lt;p&gt;Mutual information has theoretical appeal but interpretability is challenging.&lt;/p&gt;

&lt;h2 id=&quot;9-concluding-remarks&quot;&gt;9. Concluding remarks&lt;/h2&gt;

&lt;p&gt;When I started building classification models, I frequently used F1 scores. Now, I generally use AUC for model selection and hyperparameter tuning. For threshold selection, I try to identify the most relevant real-world metric.&lt;/p&gt;

&lt;p&gt;I find linear isocurves easiest to interpret. In the absence of a good prior cost function I also find it easier to justify &lt;em&gt;p/(1-p)&lt;/em&gt; than F1 or the other more complex metrics. Circular isocurves are harder to interpret but favor a balance between precision and recall. But empirically F1 performs well.&lt;/p&gt;

&lt;p&gt;To choose a threshold, you have to maximize something. The metric you maximize should be &lt;em&gt;interpretable and correlated to real-world costs&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Takeaways&lt;/em&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Threshold selection is where data science meets real-world decision-making.&lt;/li&gt;
  &lt;li&gt;Don’t automatically use a 50% threshold or even F1-score; consider the real-world costs of false negatives and false positives.&lt;/li&gt;
  &lt;li&gt;To select a threshold you have to optimize for some metric: a &lt;em&gt;cost function&lt;/em&gt;.&lt;/li&gt;
  &lt;li&gt;The business context determines the cost function you are optimizing: the real-world tradeoff you are willing to make between type I and type II error; how far you should reduce your threshold to get more true positives at the expense of more false positives.&lt;/li&gt;
  &lt;li&gt;An isocurve contour plot is a visualization that provides a good intuition of your cost function whenever you have to trade off two variables.&lt;/li&gt;
  &lt;li&gt;When you minimize the cost function, you choose the optimal classification threshold where the ROC curve intersects the lowest cost (or highest metric) isocurve.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Isocurves can be applied to make rational choices between any set of competing alternatives, not just classification thresholds. This post covers about 30% of the &lt;a href=&quot;http://www.limericksecon.com/p/ten-limerick-principles-of-economics.html&quot;&gt;most important principles of economics&lt;/a&gt;: People face tradeoffs; The cost of something is what you have give up to get it; Rational people think at the margin; You have to choose what to maximize.&lt;/p&gt;

&lt;p&gt;Selecting thresholds that optimize KPIs and achieve business objectives maximizes your value as a data scientist…a truly optimal outcome.&lt;/p&gt;

&lt;p&gt;The code for the visualizations is on &lt;a href=&quot;https://github.com/druce/threshold_selection&quot;&gt;GitHub&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;see-also&quot;&gt;See also:&lt;/h3&gt;

&lt;p&gt;Fawcett, Tom. “ROC graphs: Notes and practical considerations for researchers.” Machine learning 31.1 (2004): 1–38. https://www.hpl.hp.com/techreports/2003/HPL-2003-4.pdf&lt;/p&gt;

&lt;p&gt;Swets, John A., et al. “Better DECISIONS through SCIENCE.” Scientific American, vol. 283, no. 4, 2000, pp. 82–87. JSTOR, &lt;a href=&quot;http://www.jstor.org/stable/26058901&quot;&gt;www.jstor.org/stable/26058901&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Grandenberger, Greg, &lt;a href=&quot;https://medium.com/shoprunner/https-medium-com-shoprunner-evaluating-classification-models-3-cabb9660ea5b&quot;&gt;Evaluating Classification Models, Part 3: Fᵦ and Other Weighted Pythagorean Means of Precision and Recall&lt;/a&gt;&lt;/p&gt;</content><author><name>Druce Vertes</name></author><category term="datascience" /><category term="datascience" /><summary type="html">As a data scientist, you might say…“A blog post about thresholds? It’s not even a data science problem, it’s more of a business decision.” And you would not be wrong! Threshold selection lacks the appeal of say, generative adversarial networks.</summary></entry><entry><title type="html">Why Blockchain Is (Mostly) Useless</title><link href="http://0.0.0.0:4000/2019/09/why-blockchain-is-mostly-useless/" rel="alternate" type="text/html" title="Why Blockchain Is (Mostly) Useless" /><published>2019-09-07T08:31:36-04:00</published><updated>2019-09-07T08:31:36-04:00</updated><id>http://0.0.0.0:4000/2019/09/why-blockchain-is-mostly-useless</id><content type="html" xml:base="http://0.0.0.0:4000/2019/09/why-blockchain-is-mostly-useless/">&lt;blockquote&gt;
  &lt;p&gt;&lt;em&gt;Cryptocurrencies are useless. They’re only used by speculators looking for quick riches, people who don’t like government-backed currencies, and criminals who want a black-market way to exchange money._ _ &lt;a href=&quot;https://www.schneier.com/blog/archives/2019/02/blockchain_and_.html&quot;&gt;Bruce Schneier&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;!--more--&gt;

&lt;p&gt;The &lt;a href=&quot;https://www.schneier.com/blog/archives/2019/02/blockchain_and_.html&quot;&gt;column&lt;/a&gt; from which this quote is lifted is well worth reading and thinking about. The key feature of cryptocurrencies and blockchain apps is the distributed nature of the ledger, the idea that you can trust the system without having to trust anyone in the real world.&lt;/p&gt;

&lt;p&gt;But eventually, the system has to interact with the real world, where you do have to trust somebody to accomplish anything, and have meta-governance of the system as a whole in the real world. In the real world, there is no such thing as a ‘trustless system.’&lt;/p&gt;

&lt;p&gt;And once you have to trust someone, in nearly all cases you might as well build a much simpler and more effective centralized system with an appropriate trust model.&lt;/p&gt;

&lt;p&gt;That statement is a bit abstract, so here is a constructive example of how one can offer the key advantages of blockchain and distributed ledger technology, without blockchain.&lt;/p&gt;

&lt;p&gt;(skip this part if you’re not into architecture astronautics)&lt;/p&gt;

&lt;p&gt;AWS now offers a &lt;a href=&quot;https://aws.amazon.com/qldb/faqs/&quot;&gt;centralized immutable ledger&lt;/a&gt;, an append-only database similar to the blockchain’s immutable ledger history of all transactions. The difference is, unlike the Bitcoin blockchain, which is distributed among all the Bitcoin miners and uses the consensus algorithm to resolve any conflicts and commit updates permanently, the centralized immutable ledger has one endpoint that processes requests and commits valid updates in the order they are received.&lt;/p&gt;

&lt;p&gt;Any sufficiently advanced database system has a language, like SQL stored procedures, to extend it with more complex business logic. Let’s add a language to our centralized database. Suppose the language is Python. And suppose we make the codebase immutable. You can add new code, but you can’t delete any old code.&lt;/p&gt;

&lt;p&gt;In other words you can create a Python app, like a restaurant reservation service with a REST API, or a Web UI, and write code that supports all the necessary operations, like &lt;em&gt;add_restaurant&lt;/em&gt;, &lt;em&gt;add_diner&lt;/em&gt;, &lt;em&gt;add_reservation&lt;/em&gt;, &lt;em&gt;cancel_reservation&lt;/em&gt;, etc. And then once you publish the app, that code can never be changed.&lt;/p&gt;

&lt;p&gt;If that sounds weird, imagine you store the code in an append-only structure, the way Git version control keeps every version ever committed. And every time you commit a new version of the code, the immutable ledger, where it stores restaurant data, is forked. So the app is immutable in the sense that you can create new versions which inherit the underlying data, but the old version always remains.&lt;/p&gt;

&lt;p&gt;Now Bitcoin runs as long as it has enough miners to keep the ecosystem going. For argument’s sake, suppose our hypothetical AWS service says, the app users split the hosting bill, and users commit to some maximum monthly cost, $1, $10, whatever, and AWS keeps it running as long as the cost of hosting is covered.&lt;/p&gt;

&lt;p&gt;There could be more complex models, premium tiers, profits directed to the developers or a foundation, but the key points are: no one can extract rents beyond the hosting cost or what is initially agreed; no one can change the software unless everyone migrates to the new fork; and AWS is the trusted third party that guarantees that.&lt;/p&gt;

&lt;p&gt;The way this is &lt;em&gt;similar&lt;/em&gt; to a blockchain is that once an app is published, it can’t be changed unless all the users agree to fork it. If the developers write something new, they can publish it, but as long as any users keep using the old system, it’s still around and no one can force anyone to change.&lt;/p&gt;

&lt;p&gt;(end architecture astronautics)&lt;/p&gt;

&lt;p&gt;This contract offers the benefits of a distributed ledger app that no single individual can own or fork. The essential way it’s &lt;em&gt;different&lt;/em&gt; from blockchain is that you have to trust AWS. And pay them instead of paying to run distributed mining rigs. And running one centralized server is far simpler, cheaper, more efficient, you can have instantaneous transactions at a high rate for low cost.&lt;/p&gt;

&lt;p&gt;A key benefit of a blockchain app is the elimination of certain agent-principal problems. Back in the day, there was an open-source app called CDDB. You put a CD in your PC, and CDDB would read it, look up a hash in a remote database and give your audio client all the metadata, artist, album titles, song titles. And if your CD wasn’t in the master database you could contribute metadata and everyone else could use it.&lt;/p&gt;

&lt;p&gt;Then one day the developer cashed out and sold the database for a few million. All the work that everyone had done maintaining the open-source database was captured, and the people who had created it now had to pay license fees to access it. Classic bait-and-switch: gain lock-in on false promise of a free service, make it painful to switch all the apps that used the CDDB API, then extract maximum rent.&lt;/p&gt;

&lt;p&gt;With our immutable application architecture, that debacle could never happen. Everyone could just keep using the old version of the database. No one can ever start charging for access to the data, or change the deal à la Darth Vader, because no one individual has control. And that’s the primary benefit of blockchain for a lot of use cases. It’s an open-source cloud architecture, anyone can improve on it and fork it, but no one person can control it or fence in the commons.&lt;/p&gt;

&lt;p&gt;But eventually, you have to trust somebody where the ‘trustless’ part of the system interacts with the real world. The minute you are trading with a broker/exchange, or buying something with Bitcoin, you have to trust that the exchange won’t get hacked or the seller won’t scam your money, or the restaurant where you made a reservation won’t give your table away or shut down.&lt;/p&gt;

&lt;p&gt;Logically, if you trust someone like Amazon for e.g. payments/fulfillment at the edge of the system, then you lose nothing extending trust to having them run the blockchain infrastructure, and then you lose nothing if they build a high-performing centralized system with the same functionality but without all the blockchain hassles.&lt;/p&gt;

&lt;p&gt;As long as you trust AWS to be an honest broker and charge only reasonable hosting, and never try to extract the value of the underlying app, you don’t really care about distributed ledger vs. centralized ledger, you just want it to work. And centralized will always be simpler and have better throughput than distributed; it just works better. And if you can trust AWS and the real-world legal system, you can write a contract and governance structure that provides the benefits of a distributed ledger.&lt;/p&gt;

&lt;p&gt;Blockchain is &lt;em&gt;almost&lt;/em&gt; always useless. The exception is where you can’t trust AWS. More to the point, where you can’t trust the government. If you want to create an app that the government may want to block, collect taxes on, enforce capital controls, protect government monopolies, censor forms of expression, then you can never trust a central nexus. The government can show up and seize servers, shut it down and go after the users.&lt;/p&gt;

&lt;p&gt;But effective strong states can maybe block distributed ledger services too. China has shown that they can do a pretty good job of policing the decentralized Internet. Few things are so decentralized that a strong government can’t drive it deep underground. The US banned private gold ownership in 1933 and it was mostly effective. People could still have gold necklaces and maybe trade using gold jewelry, but without coins and bars it was no longer economical to store and exchange value on a large scale. If the motivation exists, governments can crack down on anything.&lt;/p&gt;

&lt;p&gt;In my opinion Bitcoin will never be a legitimate, largely unregulated medium of exchange for the masses in the US. There’s too much at stake for the government not to control it. It’s too important for the US government to maintain control of the payments system for taxation, seignorage (ability to print money), financial stability (FDIC, intermediaries that don’t go bust because someone hacked and made off with the money), and national security (i.e. leveraging the world’s dominant currency for political purposes).&lt;/p&gt;

&lt;p&gt;And the US is still a very high-trust society, so there’s not a lot of reason for most people to use a black-market type of payment system, without the benefit of the government to enforce it and backstop intermediaries with e.g. FDIC insurance.&lt;/p&gt;

&lt;p&gt;In a nutshell, the US government’s technical means and motive to regulate Bitcoin (or Libra or whatever) exceeds the average consumer’s means and motive to circumvent the US government.&lt;/p&gt;

&lt;p&gt;But this isn’t true everywhere. You can draw a 2×2 matrix:&lt;/p&gt;

&lt;div&gt;
  &lt;table style=&quot;font-family:arial, helvetica, sans-serif; font-size: 13px; border: 2px solid #FFFFFF; width: 100%; text-align: center; border-collapse: collapse; &quot;&gt;
    &lt;tr&gt;
      &lt;th&gt;
        &amp;nbsp;
      &lt;/th&gt;
      
      &lt;th&gt;
        Strong State&lt;br /&gt;Able to repress crypto
      &lt;/th&gt;
      
      &lt;th&gt;
        Weak State&lt;br /&gt;Unable to repress crypto
      &lt;/th&gt;
    &lt;/tr&gt;
    
    &lt;tr&gt;
      &lt;th&gt;
        High trust society&lt;br /&gt;Low demand for crypto
      &lt;/th&gt;
      
      &lt;td style=&quot;border: solid 2px black; background-color: #c9edff;&quot;&gt;
        USA&lt;br /&gt; Europe&lt;br /&gt; Japan
      &lt;/td&gt;
      
      &lt;td style=&quot;border: solid 2px black; background-color: #c9edff;&quot;&gt;
        ??? Island paradises ??? &lt;br /&gt;Micronesia? &lt;br /&gt;Bhutan?
      &lt;/td&gt;
    &lt;/tr&gt;
    
    &lt;tr&gt;
      &lt;th&gt;
        Low trust society&lt;br /&gt;High demand for crypto
      &lt;/th&gt;
      
      &lt;td style=&quot;border: solid 2px black; background-color: #c9edff;&quot;&gt;
        China &lt;br /&gt; North Korea
      &lt;/td&gt;
      
      &lt;td style=&quot;border: solid 2px black; background-color: #c9fc9d;&quot;&gt;
        Venezuela &lt;br /&gt; Somalia
      &lt;/td&gt;
    &lt;/tr&gt;
  &lt;/table&gt;
  
  &lt;p&gt;
    &amp;nbsp; &amp;lt;/div&amp;gt; 
    
    &lt;p&gt;
      I think only the Venezuelas of the world, with low trust in local currencies and financial institutions, which are also weak states hard-pressed to universally enforce their will, will see significant adoption of Bitcoin. And maybe gold and USD may be more practical for black markets.
    &lt;/p&gt;
    
    &lt;p&gt;
      Satoshi Nakamoto’s paper and Bitcoin launch happened in 2008…over 10 years ago. The iPhone was launched in 2007. If blockchain really is a platform that will change everything, it’s a real sleeper success story. Typically you see killer apps on a new platform a lot quicker. Web 1.0 launched Amazon and Netflix within the first couple of years. Where are all the blockchain apps?
    &lt;/p&gt;
    
    &lt;p&gt;
      The only industries that have really been impacted are ransomware, money laundering, and facilities for exchanging and speculating on tokens.
    &lt;/p&gt;
    
    &lt;p&gt;
      There are unloved monopoly ledger companies like OpenTable and Ticketmaster. Where has any centralized application actually been disrupted? If distributed ledger technology can’t disrupt Ticketmaster, what use is it?
    &lt;/p&gt;
    
    &lt;p&gt;
      My best guess is that blockchain and Bitcoin adoption will remain a curiosity and a niche phenomenon linked to black markets, illicit activities, weak states with unreliable payments and money.
    &lt;/p&gt;
    
    &lt;p&gt;
      And blockchain apps will migrate to more efficient centralized systems that use governance and trust architectures to offer benefits similar to distributed ledgers…for average Joes who have no choice but to trust the government.
    &lt;/p&gt;
    
    &lt;p&gt;
      TL; DR Anything you can do with blockchain you can do better without it. Except maybe in low-trust environments, but if the reason for low trust is strong state enemies, they can probably repress blockchain applications they don’t like. Eventually you have to trust somebody. So might as well pick the right parties to trust, and build applications people love.
    &lt;/p&gt;
    
    &lt;p&gt;
      &lt;em&gt;Bury it in the desert. Wear gloves. _ &lt;a href=&quot;https://xkcd.com/2030/&quot;&gt;xkcd&lt;/a&gt;&lt;/em&gt;
    &lt;/p&gt;
&lt;/p&gt;&lt;/div&gt;</content><author><name>Druce Vertes, CFA</name></author><category term="Uncategorized" /><category term="bitcoin" /><category term="blockchain" /><summary type="html">Cryptocurrencies are useless. They’re only used by speculators looking for quick riches, people who don’t like government-backed currencies, and criminals who want a black-market way to exchange money._ _ Bruce Schneier</summary></entry><entry><title type="html">There ain’t no such thing as a free option</title><link href="http://0.0.0.0:4000/2019/06/there-aint-no-such-thing-as-a-free-option/" rel="alternate" type="text/html" title="There ain’t no such thing as a free option" /><published>2019-06-07T17:56:13-04:00</published><updated>2019-06-07T17:56:13-04:00</updated><id>http://0.0.0.0:4000/2019/06/there-aint-no-such-thing-as-a-free-option</id><content type="html" xml:base="http://0.0.0.0:4000/2019/06/there-aint-no-such-thing-as-a-free-option/">&lt;blockquote&gt;
  &lt;p&gt;&lt;em&gt;I would not give a fig for the simplicity this side of complexity, but I would give my life for the simplicity on the other side of complexity. - Oliver Wendell Holmes&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;!--more--&gt;

&lt;p&gt;Keep it simple, stupid - Anonymous&lt;/p&gt;

&lt;p&gt;This post is motivated by this &lt;a href=&quot;https://www.nytimes.com/2019/06/06/climate/trump-auto-emissions-rollback-letter.html&quot;&gt;story of automakers up in arms about Trump deregulating emissions standards&lt;/a&gt;. Surely reducing regulation cannot harm automakers? What would that say about free markets?&lt;/p&gt;

&lt;p&gt;Absent the regulatory rollback, manufacturers have to make low-polluting vehicles. If the Feds deregulate, they have a choice of making low-polluting or high-polluting vehicles. And in any optimization problem, removing or easing constraints can only improve an outcome. Why would they prefer an outcome that doesn’t make them better off?&lt;/p&gt;

&lt;p&gt;Here’s the rub: California will keep stringent standards. Some manufacturers will produce high polluting vehicles and exit California, where the market will be less competitive and car prices may increase. The market will be split into California-legal and non-California-legal. People on the border will register cars with their friends in Oregon. There will be multiple regulatory regimes and an unholy mess, and the likelihood that future administrations will tinker with it.&lt;/p&gt;

&lt;p&gt;It would be better for everyone, including the car manufacturers, if everyone could find a way to agree on the same standard.&lt;/p&gt;

&lt;p&gt;Free-market fetishists are going to say, well, that’s an artifact of crazy regulation, a free option can only be a good thing, and if you give people freedom of choice, they will choose the right solution. WRONG! Time for some game theory…&lt;/p&gt;

&lt;p&gt;Consider &lt;a href=&quot;https://en.wikipedia.org/wiki/Braess%27s_paradox&quot;&gt;Braess’s Paradox&lt;/a&gt;. You have a road network and 4000 cars traveling from START to END over 2 different routes, say on either side of a body of water. Initially, the equilibrium is 2000 cars over each route and a travel time of 65 minutes. (Traffic/100 = 20 minutes + 45 minutes.) It’s a Nash equilibrium because no one has an incentive to switch. If for any reason more people take one route, it slows down and people have an incentive to switch back to the faster route.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/assets/wp-content/uploads/2019/06/1000px-Braess_paradox_road_example.svg_.png&quot;&gt;&lt;img src=&quot;/assets/wp-content/uploads/2019/06/1000px-Braess_paradox_road_example.svg_-300x78.png&quot; alt=&quot;&quot; width=&quot;600&quot; height=&quot;156&quot; class=&quot;alignright size-medium wp-image-6845&quot; srcset=&quot;/assets/wp-content/uploads/2019/06/1000px-Braess_paradox_road_example.svg_-300x78.png 300w, /assets/wp-content/uploads/2019/06/1000px-Braess_paradox_road_example.svg_-768x199.png 768w, /assets/wp-content/uploads/2019/06/1000px-Braess_paradox_road_example.svg_.png 1000w&quot; sizes=&quot;(max-width: 600px) 100vw, 600px&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;(&lt;a href=&quot;https://en.wikipedia.org/wiki/Braess%27s_paradox&quot;&gt;via Wikipedia&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;Then, we add a bridge between A and B. Suppose it’s instantaneous to keep things simple. You can even suppose it goes both ways, but no one will choose the 90 minute option.&lt;/p&gt;

&lt;p&gt;Initially, it’s faster to take the connection (40 minutes). But eventually, too many people take it. What happens when 3300 people take the new route? The time is 66 minutes, slower than before! And the people going along the other routes are even slower so they switch too and slow it down further! Amazingly, the new Nash equilibrium is 80 minutes, everyone takes the bridge.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;4000/100 + 4000/100 = 80 minutes (4000 people)&lt;/li&gt;
  &lt;li&gt;45 + 4000/100 = 85 minutes (0 people)&lt;/li&gt;
  &lt;li&gt;4000/100 = 85 minutes (0 people)&lt;/li&gt;
  &lt;li&gt;45 + 45 = 90 minutes (0 people)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Adding this route option cost everyone 15 minutes. (If you add more people, eventually above 4500 people all the new people switch to the 90 minute route. If you don’t follow or want to know more, read the &lt;a href=&quot;https://en.wikipedia.org/wiki/Braess%27s_paradox&quot;&gt;Wikipedia article&lt;/a&gt;, also &lt;a href=&quot;https://ncase.me/trust/&quot;&gt;Evolution of Trust&lt;/a&gt; is the most awesomest intro to game theory ever.)&lt;/p&gt;

&lt;p&gt;The thing is, &lt;em&gt;changing the rules changes the game, which can change the whole equilibrium.&lt;/em&gt; The law of unintended consequences. Letting everyone act freely according to their own best interest does not lead to the best outcome. You also need to set the game up right, with a market design that is engineered to be fit for purpose. (And even then, &lt;a href=&quot;https://plato.stanford.edu/entries/arrows-theorem/&quot;&gt;no guarantees of optimal outcome&lt;/a&gt;). In this case, everyone should agree to blow up that bridge between A and B.&lt;/p&gt;

&lt;p&gt;There ain’t no such thing as a totally free market. You have to come up with a market design that achieves the desired objectives. If you choose not to decide, and let the market be designed to protect the right of the stronger instead of other objectives, you still have made a choice. There is no &lt;a href=&quot;https://www.econlib.org/library/Essays/LtrLbrty/bryTSO.html&quot;&gt;spontaneous order&lt;/a&gt; except that which sensible people work very hard to engineer. (See also &lt;a href=&quot;/2017/01/why-do-tech-folks-identify-as-libertarian-and-further-strange-musings/&quot;&gt;this&lt;/a&gt; on Silicon Valley pseudo-libertarianism.)&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/assets/wp-content/uploads/2019/06/Cortes.png&quot;&gt;&lt;img src=&quot;/assets/wp-content/uploads/2019/06/Cortes.png&quot; alt=&quot;&quot; width=&quot;260&quot; height=&quot;194&quot; class=&quot;alignright size-full wp-image-6853&quot; /&gt;&lt;/a&gt; Apocryphally, Cortés burned his ships so he would not have the option to retreat. The history is &lt;a href=&quot;https://www.reddit.com/r/AskHistorians/comments/21hoe1/did_cortes_really_order_his_own_ships_burnt_did/&quot;&gt;more complicated&lt;/a&gt;, but &lt;a href=&quot;https://www.youtube.com/watch?reload=9&amp;amp;v=3-XEVC00szg&quot;&gt;eliminating your own options can be a winning strategy&lt;/a&gt;. If you are in a game of chicken with a maniac, both of you driving toward each other at 100 mph, the one who throws the steering wheel out the window first usually wins, by credibly eliminating the option to swerve.&lt;/p&gt;

&lt;p&gt;In college I took a course from Seymour Melman (Columbia’s Noam Chomsky) where I had to read the nuclear doomsday theorists, Herman Kahn, George Kennan and whatnot. I was a little shook up when I read about &lt;a href=&quot;https://www.armscontrolwonk.com/archive/402503/mirvs-and-remorse-sort-of/&quot;&gt;Kissinger saying&lt;/a&gt; &lt;em&gt;“I would say in retrospect that I wish I had thought through the implications of a MIRVed world more thoughtfully in 1969 and 1970 than I did.”&lt;/em&gt; and &lt;em&gt;“In retrospect, I think if one could have avoided the development of MIRVs, which means also the testing of MIRVs by the Soviets, we would both be better off.”&lt;/em&gt; You had one job, Henry. You’re not supposed to &lt;a href=&quot;https://theintercept.com/2016/02/12/henry-kissingers-war-crimes-are-central-to-the-divide-between-hillary-clinton-and-bernie-sanders/&quot;&gt;have a conscience&lt;/a&gt; but you’re supposed to understand strategy FFS.&lt;/p&gt;

&lt;p&gt;The thing about the MIRV (Multiple Independent Rentry Vehicle) is … suppose each side has 1000 missiles and they have 50% effectiveness. One side launches all its missiles, catches the other side napping… and only destroys 50% of the other side’s missiles, while using all of its own.&lt;/p&gt;

&lt;p&gt;Now suppose you have 10 warheads on each missile. The side that launches first destroys all of the opponent’s missiles with only half of its own.&lt;/p&gt;

&lt;p&gt;The Nash equilibrium shifts from, &lt;em&gt;‘nobody launches missiles’&lt;/em&gt;, to &lt;em&gt;‘everyone launch your missiles first, or immediately without fail at any detection of launch from the other side.’&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;If the &lt;a href=&quot;https://www.oshonews.com/2013/09/09/henry-kissinger/&quot;&gt;schmartest man in the world&lt;/a&gt; can screw this up, what hope is there for the rest of us?&lt;/p&gt;

&lt;p&gt;You can’t just assume that giving yourself more options is a good thing. You build a bomb because the Nazis might do it first. You use it because it might save millions of lives compared to an invasion. You build a powerful military because the world is a dangerous place, you don’t want Germany and Japan re-arming, and an overwhelmingly superior military in the hands of a stable democracy is a great thing for global security. Then one day you have a Điện Biên Phủ and a commander in chief can be faced a very tough choice. You have to do everything in your power to save American lives, right? Even at the cost of escalation and possible retaliation down the road, right? Or one day a stable Western democracy isn’t quite so stable, you have people in charge who say, what’s the point of having nuclear if you can’t use it to get what you want? A no-first-use commitment seems like a good idea, but then the other side has more tanks … it all gets very complicated.&lt;/p&gt;

&lt;p&gt;It pays to simplify.&lt;/p&gt;

&lt;p&gt;To make the right move, you have to understand the game, the meta-game, the game beyond the meta-game. You need second-level thinking to succeed, you need to think strategically. You also want positive convexity, situations that that have the potential to go really really well but don’t cost you much downside. (Think the poker equivalent of calling from the big blind with 6-7 suited).&lt;/p&gt;

&lt;p&gt;But in poker, in investing, in life, you also need keep it simple, stupid. You are often better off limiting your options. Even if you think you’re the smartest player at the table, you want to avoid marginal situations, where you may have to make a big decision in an unclear situation. And if you’re not the smartest or most confident person at the table, the people who are will force you to make a very tough decision at a time when they have the edge.&lt;/p&gt;

&lt;p&gt;Reality is too complex, best-laid plans of mice and men, a pound of principle is worth a ton of guile.&lt;/p&gt;

&lt;p&gt;And IMHO Western ethics are mostly strategic thinking carried to their ultimate conclusion. If you are strategic but your goal is in the Kantian sense to do what would be best if everyone did it, strategic thinking is indistinguishable from altruism. In an iterated game, on a long enough time horizon, the most altruistic is the most strategic.&lt;/p&gt;

&lt;p&gt;Ethics = strategic thinking + love. If you care so much about your fellow players that their payoff is your payoff, you get to altruism.&lt;/p&gt;

&lt;p&gt;In the real world, people are boundedly strategic and boundedly rational, but the more we can build market designs and institutions that unite the two, the better off we will be.&lt;/p&gt;

&lt;p&gt;If you want cooperation, security, stability, you need strength but also honesty and clarity of purpose and communication, looking for win-win situations… you don’t go to the mat against friends for small victories. What goes around, comes around. Sow the wind, reap the whirlwind. But that is a topic for another day.&lt;/p&gt;</content><author><name>Druce Vertes, CFA</name></author><category term="economics" /><category term="economics" /><summary type="html">I would not give a fig for the simplicity this side of complexity, but I would give my life for the simplicity on the other side of complexity. - Oliver Wendell Holmes</summary></entry></feed>