<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator><link href="http://0.0.0.0:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://0.0.0.0:4000/" rel="alternate" type="text/html" /><updated>2020-10-27T09:27:01-04:00</updated><id>http://0.0.0.0:4000/feed.xml</id><title type="html">Druce.ai</title><subtitle>Druce's Blog on Machine Learning, Tech, Markets and Economics</subtitle><author><name>Druce Vertes</name></author><entry><title type="html">Beyond Grid Search: Using Hyperopt, Optuna, and Ray Tune to hypercharge hyperparameter tuning for XGBoost and LightGBM</title><link href="http://0.0.0.0:4000/2020/10/hyperparameter-tuning-with-xgboost-ray-tune-hyperopt-and-optuna" rel="alternate" type="text/html" title="Beyond Grid Search: Using Hyperopt, Optuna, and Ray Tune to hypercharge hyperparameter tuning for XGBoost and LightGBM" /><published>2020-10-12T18:28:57-04:00</published><updated>2020-10-12T18:28:57-04:00</updated><id>http://0.0.0.0:4000/2020/10/hyperparameter-tuning-with-xgboost-ray-tune-hyperopt-and-optuna</id><content type="html" xml:base="http://0.0.0.0:4000/2020/10/hyperparameter-tuning-with-xgboost-ray-tune-hyperopt-and-optuna">&lt;p&gt;&lt;img src=&quot;/assets/2020/optuna_bayesian.png&quot; alt=&quot;Image source: Crissman Loomis, Using Optuna to Optimize XGBoost Hyperparameters (2020)&quot; /&gt;&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;&lt;em&gt;Bayesian optimization of machine learning model hyperparameters works faster and better than grid search. Here’s how we can speed up hyperparameter tuning with 1) Bayesian optimization with Hyperopt and Optuna, running on… 2) the &lt;a href=&quot;https://ray.io/&quot;&gt;Ray&lt;/a&gt; distributed machine learning framework, with a &lt;a href=&quot;https://medium.com/riselab/cutting-edge-hyperparameter-tuning-with-ray-tune-be6c0447afdf&quot;&gt;unified API to many hyperparameter search algos&lt;/a&gt; and early stopping schedulers, and… 3) a distributed cluster of cloud instances for even faster tuning. &lt;a href=&quot;https://medium.com/optuna/using-optuna-to-optimize-xgboost-hyperparameters-63bfcdfd3407&quot;&gt;Image source: Crissman Loomis&lt;/a&gt;&lt;/em&gt;
&lt;!--more--&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;outline&quot;&gt;Outline:&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;#1-results&quot;&gt;Results&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#2-hyperparameter-tuning-overview&quot;&gt;Hyperparameter tuning overview&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#3-bayesian-optimization&quot;&gt;Bayesian optimization&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#4-early-stopping&quot;&gt;Early stopping&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#5-implementation-details&quot;&gt;Implementation details&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#6-baseline-linear-regression&quot;&gt;Baseline linear regression&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#7-elasticnetcv&quot;&gt;ElasticNetCV (Linear regression with L1 and L2 regularization)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#8-gridsearchcv&quot;&gt;ElasticNet with GridSearchCV &lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#9-xgboost-with-sequential-grid-search&quot;&gt;XGBoost: sequential grid search over hyperparameter subsets with early stopping &lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#10-xgboost-with-hyperopt-optuna-and-ray&quot;&gt;XGBoost: Hyperopt and Optuna search algorithms&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#11-lightgbm-with-hyperopt-and-optuna&quot;&gt;LightGBM: Hyperopt and Optuna search algorithms&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#12-xgboost-on-a-ray-cluster&quot;&gt;XGBoost on a Ray cluster&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#13-lightgbm-on-a-ray-cluster&quot;&gt;LightGBM on a Ray cluster&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#14-concluding-remarks&quot;&gt;Concluding remarks&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;1-results&quot;&gt;1. Results&lt;/h2&gt;

&lt;p&gt;Bottom line up front: Here are results on the Ames housing data set, predicting Iowa home prices:&lt;/p&gt;

&lt;h3 id=&quot;xgb-and-lightgbm-using-various-hyperparameter-optimization-methodologies&quot;&gt;XGB and LightGBM using various hyperparameter optimization methodologies&lt;/h3&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;ML Algo&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Search algo&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;CV Error&lt;br /&gt; (RMSE in $)&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Time&lt;br /&gt; h:mm::ss&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;XGB&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Sequential Grid Search&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;18302&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1:27:14&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;XGB&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Hyperopt (1024 samples)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;18309&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0:53:41&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;XGB&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Optuna (1024 samples)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;18325&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0:48:02&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;XGB&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Hyperopt (2048 samples) - 32x cluster&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;18030&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1:30:58&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;XGB&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Optuna (2048 samples) - 32x cluster&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;18028&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1:29:57&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;LightGBM&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Hyperopt (1024 samples)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;18615&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1:13:40&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;LightGBM&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Optuna (1024 samples)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;18614&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1:08:40&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;LightGBM&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Hyperopt (2048 samples) - 32x cluster&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;18459&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1:05:19&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;LightGBM&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Optuna (2048 samples) - 32x cluster&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;18458&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0:48:16&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt; &lt;/p&gt;

&lt;h3 id=&quot;baseline-linear-models&quot;&gt;Baseline linear models&lt;/h3&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;ML Algo&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Search algo&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;CV Error (RMSE in $)&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Time mm::ss&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Linear Regression&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;–&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;18192&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0:01&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;ElasticNet&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;ElasticNetCV (Grid Search)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;18061&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0:02&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;ElasticNet&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;GridSearchCV&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;18061&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0:05&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;em&gt;Times for single-instance are on a local desktop with 12 threads, comparable to EC2 4xlarge. Times for cluster are on m5.large x 32 (1 head node + 31 workers).&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;We saw a big speedup when using Hyperopt and Optuna locally, compared to grid search. The sequential search performed about 261 trials, so the XGB/Optuna search performed about 3x as many trials in half the time and got a similar result.&lt;/li&gt;
  &lt;li&gt;The cluster of 32 instances (64 threads) gave very modest improvement vs. the local desktop with 12 threads. I attempted to set this up so we would get some improvement in RMSE (which we did with 2048 trials), and some speedup in training time (which we did not get with 64 threads). It ran twice the number of trials in slightly less than twice the time. The comparison is imperfect, local desktop vs. AWS, running Ray 1.0 on local and 1.1 on the cluster, different number of trials (better hyperparameter configs don’t get early-stopped and take longer to train). But the point was to see what kind of improvement one might obtain in practice, leveraging a cluster vs. a local desktop or laptop. Bottom line, modest benefit here from even a pretty large 32-node cluster.&lt;/li&gt;
  &lt;li&gt;RMSEs are similar across the board. XGB with 2048 trials is best by a small margin.&lt;/li&gt;
  &lt;li&gt;LightGBM doesn’t offer improvement over XGBoost here in RMSE or run time. In my experience LightGBM is often faster so  you can train and tune more in a given time. But we don’t see that here. Possibly XGB interacts better with ASHA early stopping.&lt;/li&gt;
  &lt;li&gt;Similar RMSE between Hyperopt and Optuna. Optuna is consistently slightly faster.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Our simple ElasticNet baseline yields similar results to boosting, in seconds. This may be because our feature engineering was intensive and designed to fit the linear model. Not shown, SVR and KernelRidge outperform ElasticNet, and an ensemble improves over all individual algos.&lt;/p&gt;

&lt;p&gt;Full notebooks are on &lt;a href=&quot;https://github.com/druce/iowa/blob/master/hyperparameter_optimization.ipynb&quot;&gt;GitHub&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;2-hyperparameter-tuning-overview&quot;&gt;2. Hyperparameter Tuning Overview&lt;/h2&gt;

&lt;p&gt;(If you are not a data scientist ninja, here is some context. If you are, you can safely skip to &lt;a href=&quot;#3-bayesian-optimization&quot;&gt;Bayesian optimization&lt;/a&gt;
and implementations below.)&lt;/p&gt;

&lt;p&gt;Any sufficiently advanced machine learning model is indistinguishable from magic, and any sufficiently advanced machine learning model needs good tuning.&lt;/p&gt;

&lt;p&gt;Backing up a step, here is a typical modeling workflow:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Exploratory data analysis: understand your data.&lt;/li&gt;
  &lt;li&gt;Feature engineering and feature selection: clean, transform and engineer the best possible features&lt;/li&gt;
  &lt;li&gt;Modeling: model selection and hyperparameter tuning to identify the best model architecture, and ensembling to combine multiple models&lt;/li&gt;
  &lt;li&gt;Evaluation: Describe the out-of-sample error and its expected distribution.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To minimize the out-of-sample error, you minimize the error from &lt;em&gt;bias&lt;/em&gt;, meaning the model isn’t sufficiently sensitive to the signal in the data, and &lt;em&gt;variance&lt;/em&gt;, meaning the model is too sensitive to the signal specific to the training data in ways that don’t generalize out-of-sample. Modeling is 90% data prep, the other half is all finding the &lt;a href=&quot;http://scott.fortmann-roe.com/docs/BiasVariance.html&quot;&gt;optimal bias-variance tradeoff&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Hyperparameters help you tune the bias-variance tradeoff. For a simple logistic regression predicting survival on the Titanic, a regularization parameter lets you control overfitting by penalizing sensitivity to any individual feature. For a massive neural network doing machine translation, the number and types of layers, units, activation function, in addition to regularization, are hyperparameters. We select the best hyperparameters using &lt;em&gt;&lt;a href=&quot;https://machinelearningmastery.com/k-fold-cross-validation/&quot;&gt;k-fold cross-validation&lt;/a&gt;&lt;/em&gt;; this is what we call hyperparameter tuning.&lt;/p&gt;

&lt;p&gt;The regression algorithms we use in this post are XGBoost and LightGBM, which are variations on  &lt;em&gt;gradient boosting&lt;/em&gt;. Gradient boosting is an ensembling method that usually involves decision trees. A decision tree constructs rules like, if the passenger is in first class and female, they probably survived the sinking of the Titanic. Trees are powerful, but a single deep decision tree with all your features will tend to overfit the training data. A &lt;em&gt;random forest&lt;/em&gt; algorithm builds many decision trees based on random subsets of observations and features which then vote (&lt;em&gt;bagging&lt;/em&gt;). The outcome of a vote by &lt;em&gt;weak learners&lt;/em&gt; is less overfitted than training on all the data rows and all the feature columns to generate a single strong learner, and performs better out-of-sample. Random forest hyperparameters include the number of trees, tree depth, and how many features and observations each tree should use.&lt;/p&gt;

&lt;p&gt;Instead of aggregating many independent learners working in parallel, &lt;em&gt;i.e.&lt;/em&gt; bagging, &lt;em&gt;boosting&lt;/em&gt; uses many learners in series:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Start with a simple estimate like the median or base rate.&lt;/li&gt;
  &lt;li&gt;Fit a tree to the &lt;em&gt;error&lt;/em&gt; in this prediction.&lt;/li&gt;
  &lt;li&gt;If you can &lt;em&gt;predict&lt;/em&gt; the error, you can &lt;em&gt;adjust&lt;/em&gt; for it and improve the prediction. Adjust the prediction not all the way to the tree prediction, but part of the way based on a &lt;em&gt;learning rate&lt;/em&gt; (a hyperparameter).&lt;/li&gt;
  &lt;li&gt;Fit another tree to the error in the updated prediction and adjust the prediction further based on the learning rate.&lt;/li&gt;
  &lt;li&gt;Iteratively continue reducing the error for a specified number of boosting rounds (another hyperparameter).&lt;/li&gt;
  &lt;li&gt;The final estimate is the initial prediction plus the sum of all the predicted necessary adjustments (weighted by the learning rate).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The learning rate performs a similar function to voting in random forest, in the sense that no single decision tree determines too much of the final estimate. This ‘wisdom of crowds’ approach helps prevent overfitting.&lt;/p&gt;

&lt;p&gt;Gradient boosting is the current state of the art for regression and classification on traditional structured tabular data (in contrast to less structured data like image/video/natural language processing, where deep learning, &lt;em&gt;i.e.&lt;/em&gt; deep neural net are state of the art).&lt;/p&gt;

&lt;p&gt;Gradient boosting algorithms like  &lt;a href=&quot;https://xgboost.readthedocs.io/en/latest/parameter.html&quot;&gt;XGBoost&lt;/a&gt;, &lt;a href=&quot;https://lightgbm.readthedocs.io/en/latest/Parameters.html&quot;&gt;LightGBM&lt;/a&gt;, and &lt;a href=&quot;https://catboost.ai/docs/concepts/python-reference_parameters-list.html&quot;&gt;CatBoost&lt;/a&gt; have a very large number of hyperparameters, and tuning is an important part of using them.&lt;/p&gt;

&lt;p&gt;These are &lt;a href=&quot;https://en.wikipedia.org/wiki/Hyperparameter_optimization&quot;&gt;the principal approaches to hyperparameter tuning&lt;/a&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;Grid search&lt;/em&gt;: Given a finite set of discrete values for each hyperparameter, exhaustively cross-validate all combinations.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;Random search&lt;/em&gt;: Given a discrete or continuous distribution for each hyperparameter, randomly sample from the joint distribution. Generally &lt;a href=&quot;https://dl.acm.org/doi/10.5555/2188385.2188395&quot;&gt;more efficient than exhaustive grid search.&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;Bayesian optimization&lt;/em&gt;: Sample like random search, but update the search space you sample from as you go, based on outcomes of prior searches.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;Gradient-based optimization&lt;/em&gt;: Attempt to estimate the gradient of the cross-validation metric with respect to the hyperparameters and ascend/descend the gradient.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;Evolutionary optimization&lt;/em&gt;: Sample the search space, discard combinations with poor metrics, and genetically evolve new combinations based on the successful combinations.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;Population-based training&lt;/em&gt;: A method of performing hyperparameter optimization at the same time as training.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In this post, we focus on Bayesian optimization with Hyperopt and Optuna.&lt;/p&gt;

&lt;h2 id=&quot;3-bayesian-optimization&quot;&gt;3. Bayesian Optimization&lt;/h2&gt;

&lt;p&gt;What is Bayesian optimization? When we perform a grid search, the search space is a &lt;a href=&quot;https://en.wikipedia.org/wiki/Prior_probability&quot;&gt;prior&lt;/a&gt;: we believe that the best hyperparameter vector is in this search space. And &lt;em&gt;a priori&lt;/em&gt; each hyperparameter combinations has equal probability of being the best combination (a uniform distribution). So we try them all and pick the best one.&lt;/p&gt;

&lt;p&gt;Perhaps we might do two passes of grid search. After an initial search on a broad, coarsely spaced grid, we do a deeper dive in a smaller area around the best metric from the first pass, with a more finely-spaced grid. In Bayesian terminology, we &lt;em&gt;updated our prior&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Bayesian optimization starts by sampling randomly, e.g. 30 combinations, and computes the cross-validation metric for each of the 30 randomly sampled combinations using &lt;em&gt;&lt;a href=&quot;https://machinelearningmastery.com/k-fold-cross-validation/&quot;&gt;k-fold cross-validation&lt;/a&gt;&lt;/em&gt;. Then the algorithm updates the distribution it samples from, so that it is more likely to sample combinations similar to the good metrics, and less likely to sample combinations similar to the poor metrics. As it continues to sample, it continues to update the search distribution it samples from, based on the metrics it finds.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2020/optuna_bayesian.png&quot; alt=&quot;Illustration of random search vs. Bayesian&quot; /&gt;
Source: Crissman Loomis, &lt;a href=&quot;https://medium.com/optuna/using-optuna-to-optimize-xgboost-hyperparameters-63bfcdfd3407&quot;&gt;Using Optuna to Optimize XGBoost Hyperparameters&lt;/a&gt; (2020)&lt;/p&gt;

&lt;p&gt;If good metrics are not uniformly distributed, but found close to one another in a Gaussian distribution or any distribution which we can model, then Bayesian optimization can exploit the underlying pattern, and is likely to be more efficient than grid search or naive random search.&lt;/p&gt;

&lt;h2 id=&quot;4-early-stopping&quot;&gt;4. Early Stopping&lt;/h2&gt;

&lt;p&gt;If, while evaluating a hyperparameter combination, the evaluation metric is not improving in training, or not improving fast enough to beat our best to date, we can discard a combination before fully training on it. &lt;em&gt;Early stopping&lt;/em&gt; of unsuccessful training runs increases the speed and effectiveness of our search.&lt;/p&gt;

&lt;p&gt;XGBoost and LightGBM helpfully provide early stopping callbacks to check on training progress and stop a training trial early (&lt;a href=&quot;https://xgboost.readthedocs.io/en/latest/python/callbacks.html&quot;&gt;XGBoost&lt;/a&gt; ; &lt;a href=&quot;https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.early_stopping.html#lightgbm.early_stopping&quot;&gt;LightGBM&lt;/a&gt;). Hyperopt, Optuna, and Ray use these callbacks to stop bad trials quickly and accelerate performance.&lt;/p&gt;

&lt;p&gt;In this post, we will use the &lt;a href=&quot;https://arxiv.org/abs/1810.05934&quot;&gt;Asynchronous Successive Halving Algorithm (ASHA)&lt;/a&gt; for early stopping, described in this &lt;a href=&quot;https://blog.ml.cmu.edu/2018/12/12/massively-parallel-hyperparameter-optimization/&quot;&gt;blog post&lt;/a&gt;.&lt;/p&gt;

&lt;h4 id=&quot;further-reading&quot;&gt;Further reading:&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2003.05689&quot;&gt;Hyper-Parameter Optimization: A Review of Algorithms and Applications&lt;/a&gt; Tong Yu, Hong Zhu (2020)&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1502.02127v2&quot;&gt;Hyperparameter Search in Machine Learning&lt;/a&gt;, Marc Claesen, Bart De Moor (2015)&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://link.springer.com/chapter/10.1007/978-3-030-05318-5_1&quot;&gt;Hyperparameter Optimization&lt;/a&gt;, Matthias Feurer, Frank Hutter (2019)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;5-implementation-details&quot;&gt;5. Implementation Details&lt;/h2&gt;

&lt;p&gt;We use data from the &lt;a href=&quot;https://www.kaggle.com/c/house-prices-advanced-regression-techniques&quot;&gt;Ames Housing Dataset&lt;/a&gt;. The original data set has 79 raw features. The data we will use has 100 features with a fair amount of feature engineering from &lt;a href=&quot;https://github.com/druce/iowa&quot;&gt;my own attempt at modeling&lt;/a&gt;, which was in the top 5% or so when I submitted it to Kaggle. We model the log of the sale price, and use RMSE as our metric for model selection. We convert the RMSE back to raw dollar units for easier interpretability.&lt;/p&gt;

&lt;p&gt;We use 4 regression algorithms:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;LinearRegression&lt;/em&gt;: baseline with no hyperparameters&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;ElasticNet&lt;/em&gt;: Linear regression with L1 and L2 regularization (2 hyperparameters).&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;XGBoost&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;LightGBM&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We use 5 approaches:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Native CV&lt;/em&gt;: In sklearn if an algorithm &lt;em&gt;xxx&lt;/em&gt; has hyperparameters it will often have an &lt;em&gt;xxxCV&lt;/em&gt; version, like ElasticNetCV, which performs automated grid search over hyperparameter iterators with specified kfolds.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;GridSearchCV&lt;/em&gt;: Abstract grid search that can wrap around any sklearn algorithm, running multithreaded trials over specified kfolds.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Manual sequential grid search&lt;/em&gt;: How we typically implement grid search with XGBoost, which doesn’t play very well with GridSearchCV and has too many hyperparameters to tune in one pass.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Ray on local desktop&lt;/em&gt;: Hyperopt and Optuna with ASHA early stopping.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Ray on AWS cluster&lt;/em&gt;: Additionally scale out to run a single hyperparameter optimization task over many instances in a cluster.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;6-baseline-linear-regression&quot;&gt;6. Baseline linear regression&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Use the same kfolds for each run so the variation in the RMSE metric is not due to variation in kfolds.&lt;/li&gt;
  &lt;li&gt;We fit on the log response, so we convert error back to dollar units, for interpretability.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sklearn.model_selection.cross_val_score&lt;/code&gt; for evaluation&lt;/li&gt;
  &lt;li&gt;Jupyter &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;%%time&lt;/code&gt; magic for wall time&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;n_jobs=-1&lt;/code&gt; to run folds in parallel using all CPU cores available.&lt;/li&gt;
  &lt;li&gt;Note the wall time &amp;lt; 1 second and RMSE of 18192.&lt;/li&gt;
  &lt;li&gt;Full notebooks are on &lt;a href=&quot;https://github.com/druce/iowa/blob/master/hyperparameter_optimization.ipynb&quot;&gt;GitHub&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;o&quot;&gt;%%&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# always use same RANDOM_STATE k-folds for comparability between tests, reproducibility
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;RANDOMSTATE&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;42&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;RANDOMSTATE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;kfolds&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;KFold&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_splits&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shuffle&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random_state&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;RANDOMSTATE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;MEAN_RESPONSE&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;response&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;cv_to_raw&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cv_val&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mean_response&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;MEAN_RESPONSE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;convert log1p rmse to underlying SalePrice error&quot;&quot;&quot;&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# MEAN_RESPONSE assumes folds have same mean response, which is true in expectation but not in each fold
&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;# we can also pass the mean response for each fold
&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;# but we're really just looking to consistently convert the log value to a more meaningful unit
&lt;/span&gt;    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;expm1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean_response&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cv_val&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;expm1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean_response&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	
&lt;span class=&quot;n&quot;&gt;lr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;LinearRegression&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# compute CV metric for each fold
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cross_val_score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predictors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;response&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
                          &lt;span class=&quot;n&quot;&gt;scoring&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;neg_root_mean_squared_error&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                          &lt;span class=&quot;n&quot;&gt;cv&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kfolds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                          &lt;span class=&quot;n&quot;&gt;n_jobs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;raw_scores&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cv_to_raw&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Raw CV RMSE %.0f (STD %.0f)&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;raw_scores&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;raw_scores&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Raw CV RMSE 18192 (STD 1839)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Wall time: 65.4 ms&lt;/code&gt;&lt;/p&gt;

&lt;h2 id=&quot;7-elasticnetcv&quot;&gt;7. ElasticNetCV&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;ElasticNet is linear regression with L1 and L2 &lt;a href=&quot;https://en.wikipedia.org/wiki/Regularization_(mathematics)&quot;&gt;regularization&lt;/a&gt; (2 hyperparameters).&lt;/li&gt;
  &lt;li&gt;When we use regularization, we need to scale our data so that the coefficient penalty has a similar impact across features. We use a pipeline with RobustScaler for scaling.&lt;/li&gt;
  &lt;li&gt;Fit a model and extract hyperparameters from the fitted model.&lt;/li&gt;
  &lt;li&gt;Then we do &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cross_val_score&lt;/code&gt; with reported hyperparams (There doesn’t appear to be a way to extract the score from the fitted model without refitting)&lt;/li&gt;
  &lt;li&gt;Verbose output reports 130 tasks, for full grid search on 10 folds we would expect 13x9x10=1170. Apparently a clever optimization.&lt;/li&gt;
  &lt;li&gt;Note the modest reduction in RMSE vs. linear regression without regularization.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;elasticnetcv&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;make_pipeline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;RobustScaler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt;
                             &lt;span class=&quot;n&quot;&gt;ElasticNetCV&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;max_iter&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
                                          &lt;span class=&quot;n&quot;&gt;l1_ratio&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.01&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.05&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.95&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.99&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
                                          &lt;span class=&quot;n&quot;&gt;alphas&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;logspace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
                                          &lt;span class=&quot;n&quot;&gt;cv&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kfolds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                          &lt;span class=&quot;n&quot;&gt;n_jobs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                          &lt;span class=&quot;n&quot;&gt;verbose&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                         &lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;#train and get hyperparams
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;elasticnetcv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predictors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;response&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;l1_ratio&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;elasticnetcv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_final_estimator&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l1_ratio_&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;elasticnetcv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_final_estimator&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alpha_&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'l1_ratio'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l1_ratio&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'alpha'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# evaluate using kfolds on full dataset
# I don't see API to get CV error from elasticnetcv, so we use cross_val_score
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;elasticnet&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ElasticNet&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                        &lt;span class=&quot;n&quot;&gt;l1_ratio&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l1_ratio&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                        &lt;span class=&quot;n&quot;&gt;max_iter&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cross_val_score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;elasticnet&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predictors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;response&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
                          &lt;span class=&quot;n&quot;&gt;scoring&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;neg_root_mean_squared_error&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                          &lt;span class=&quot;n&quot;&gt;cv&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kfolds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                          &lt;span class=&quot;n&quot;&gt;n_jobs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;raw_scores&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cv_to_raw&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Log1p CV RMSE %.04f (STD %.04f)&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Raw CV RMSE %.0f (STD %.0f)&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;raw_scores&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;raw_scores&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;l1_ratio 0.01
alpha 0.0031622776601683794

Log1p CV RMSE 0.1030 (STD 0.0109)
Raw CV RMSE 18061 (STD 2008)
CPU times: user 5.93 s, sys: 3.67 s, total: 9.6 s
Wall time: 1.61 s

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;h2 id=&quot;8-gridsearchcv&quot;&gt;8. GridSearchCV&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Identical result, runs a little slower.&lt;/li&gt;
  &lt;li&gt;GridSearchCV verbose output shows 1170 jobs, which is the expected number 13x9x10.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;gs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;make_pipeline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;RobustScaler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt;
                   &lt;span class=&quot;n&quot;&gt;GridSearchCV&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ElasticNet&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;max_iter&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
                                &lt;span class=&quot;n&quot;&gt;param_grid&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'l1_ratio'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.01&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.05&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.95&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.99&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
                                            &lt;span class=&quot;s&quot;&gt;'alpha'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;logspace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
                                           &lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;
                                &lt;span class=&quot;n&quot;&gt;scoring&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'neg_root_mean_squared_error'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                &lt;span class=&quot;n&quot;&gt;refit&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                &lt;span class=&quot;n&quot;&gt;cv&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kfolds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                &lt;span class=&quot;n&quot;&gt;n_jobs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                &lt;span class=&quot;n&quot;&gt;verbose&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
                               &lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# do cv using kfolds on full dataset
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predictors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;response&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'best params'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_final_estimator&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;best_params_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'best score'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_final_estimator&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;best_score_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;l1_ratio&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_final_estimator&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;best_params_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'l1_ratio'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_final_estimator&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;best_params_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'alpha'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# eval similarly to before
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;elasticnet&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ElasticNet&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                        &lt;span class=&quot;n&quot;&gt;l1_ratio&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l1_ratio&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                        &lt;span class=&quot;n&quot;&gt;max_iter&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;elasticnet&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cross_val_score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;elasticnet&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predictors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;response&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
                          &lt;span class=&quot;n&quot;&gt;scoring&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;neg_root_mean_squared_error&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                          &lt;span class=&quot;n&quot;&gt;cv&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kfolds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                          &lt;span class=&quot;n&quot;&gt;n_jobs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;raw_scores&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cv_to_raw&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Log1p CV RMSE %.06f (STD %.04f)&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Raw CV RMSE %.0f (STD %.0f)&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;raw_scores&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;raw_scores&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;best params {'alpha': 0.0031622776601683794, 'l1_ratio': 0.01}
best score 0.10247177583755482
ElasticNet(alpha=0.0031622776601683794, l1_ratio=0.01, max_iter=100000)

Log1p CV RMSE 0.103003 (STD 0.0109)
Raw CV RMSE 18061 (STD 2008)

Wall time: 5 s
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;9-xgboost-with-sequential-grid-search&quot;&gt;9. XGBoost with sequential grid search&lt;/h2&gt;

&lt;p&gt;It &lt;em&gt;should&lt;/em&gt; be possible to use GridSearchCV with XGBoost. But when we also try to use early stopping, XGBoost wants an eval set. OK, we can give it a static eval set held out from GridSearchCV. Now, GridSearchCV does k-fold cross-validation in the training set but XGBoost uses a separate dedicated eval set for early stopping. It seems like a bit of a Frankenstein methodology. See the &lt;a href=&quot;https://github.com/druce/iowa/blob/master/hyperparameter_optimization.ipynb&quot;&gt;notebook&lt;/a&gt; for the attempt at GridSearchCV with XGBoost and early stopping if you’re really interested.&lt;/p&gt;

&lt;p&gt;Instead we write our own grid search that gives XGBoost the correct hold-out set for each CV fold:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;EARLY_STOPPING_ROUNDS&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# stop if no improvement after 100 rounds
&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;my_cv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;predictors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;response&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kfolds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;regressor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;verbose&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;Roll our own CV 
    train each kfold with early stopping
    return average metric, sd over kfolds, average best round&quot;&quot;&quot;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;metrics&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;best_iterations&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_fold&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv_fold&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kfolds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; 
        &lt;span class=&quot;n&quot;&gt;fold_X_train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predictors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_fold&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;fold_y_train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;response&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_fold&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;fold_X_test&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predictors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cv_fold&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;fold_y_test&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;response&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cv_fold&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;regressor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fold_X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fold_y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                      &lt;span class=&quot;n&quot;&gt;early_stopping_rounds&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;EARLY_STOPPING_ROUNDS&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                      &lt;span class=&quot;n&quot;&gt;eval_set&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fold_X_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fold_y_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)],&lt;/span&gt;
                      &lt;span class=&quot;n&quot;&gt;eval_metric&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'rmse'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                      &lt;span class=&quot;n&quot;&gt;verbose&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;verbose&lt;/span&gt;
                     &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;y_pred_test&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;regressor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fold_X_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;metrics&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean_squared_error&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fold_y_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_pred_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;best_iterations&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;regressor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;best_iteration&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;average&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;metrics&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;metrics&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;average&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;best_iterations&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;XGBoost has many tuning parameters so an exhaustive grid search has an unreasonable number of combinations. Instead, we tune reduced sets sequentially using grid search and use early stopping.&lt;/p&gt;

&lt;p&gt;This is the typical grid search methodology to tune XGBoost:&lt;/p&gt;

&lt;h4 id=&quot;xgboost-tuning-methodology&quot;&gt;XGBoost tuning methodology&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;Set an initial set of starting parameters.&lt;/li&gt;
  &lt;li&gt;Tune sequentially on groups of hyperparameters that don’t interact too much between groups, to reduce the number of combinations tested.
    &lt;ul&gt;
      &lt;li&gt;First, tune &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;max_depth&lt;/code&gt;.&lt;/li&gt;
      &lt;li&gt;Then tune &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;subsample&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;colsample_bytree&lt;/code&gt;, and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;colsample_bylevel&lt;/code&gt;.&lt;/li&gt;
      &lt;li&gt;Finally, tune &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;learning rate&lt;/code&gt;: a lower learning rate will need more boosting rounds (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;n_estimators&lt;/code&gt;).&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Do 10-fold cross-validation on each hyperparameter combination. Pick hyperparameters to minimize average RMSE over kfolds.&lt;/li&gt;
  &lt;li&gt;Use XGboost early stopping to halt training in each fold if no improvement after 100 rounds.&lt;/li&gt;
  &lt;li&gt;After tuning and selecting the best hyperparameters, retrain and evaluate on the full dataset without early stopping, using the average boosting rounds across xval kfolds.&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
  &lt;li&gt;As discussed, we use the XGBoost sklearn API and roll our own grid search which understands early stopping with k-folds, instead of GridSearchCV. (An alternative would be to use native xgboost .cv which understands early stopping but doesn’t use sklearn API (uses DMatrix, not numpy array or dataframe))&lt;/li&gt;
  &lt;li&gt;We write a helper function &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cv_over_param_dict&lt;/code&gt; which takes a list of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;param_dict&lt;/code&gt; dictionaries, runs trials over all dictionaries, and returns the best &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;param_dict&lt;/code&gt; dictionary plus a dataframe of results.&lt;/li&gt;
  &lt;li&gt;We run &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cv_over_param_dict&lt;/code&gt; 3 times to do 3 grid searches over our 3 tuning rounds.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;BOOST_ROUNDS&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;50000&lt;/span&gt;   &lt;span class=&quot;c1&quot;&gt;# we use early stopping so make this arbitrarily high
&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;cv_over_param_dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;param_dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;predictors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;response&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kfolds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;verbose&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;given a list of dictionaries of xgb params
    run my_cv on params, store result in array
    return updated param_dict, results dataframe
    &quot;&quot;&quot;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;start_time&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;datetime&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;now&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;%-20s %s&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Start Time&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;start_time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;results&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;enumerate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;param_dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;xgb&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;XGBRegressor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;objective&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'reg:squarederror'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;n_estimators&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;BOOST_ROUNDS&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;random_state&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;RANDOMSTATE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;    
            &lt;span class=&quot;n&quot;&gt;verbosity&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;n_jobs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;booster&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'gbtree'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;   
            &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;    

        &lt;span class=&quot;n&quot;&gt;metric_rmse&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;metric_std&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;best_iteration&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;my_cv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;predictors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;response&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kfolds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xgb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;verbose&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;    
        &lt;span class=&quot;n&quot;&gt;results&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;metric_rmse&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;metric_std&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;best_iteration&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
    
        &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;%s %3d result mean: %.6f std: %.6f, iter: %.2f&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;datetime&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;strftime&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;datetime&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;now&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;%T&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;metric_rmse&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;metric_std&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;best_iteration&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
        
    &lt;span class=&quot;n&quot;&gt;end_time&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;datetime&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;now&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;%-20s %s&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Start Time&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;start_time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;%-20s %s&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;End Time&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;end_time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;timedelta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seconds&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;end_time&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;start_time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seconds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
    
    &lt;span class=&quot;n&quot;&gt;results_df&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DataFrame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;results&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;columns&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'rmse'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'std'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'best_iter'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'param_dict'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sort_values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'rmse'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;display&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;results_df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;head&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
    
    &lt;span class=&quot;n&quot;&gt;best_params&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;results_df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iloc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'param_dict'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;best_params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;results_df&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# initial hyperparams
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;current_params&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;'max_depth'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;'colsample_bytree'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;'colsample_bylevel'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;'subsample'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;'learning_rate'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.01&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;##################################################
# round 1: tune depth
##################################################
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;max_depths&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;grid_search_dicts&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'max_depth'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;md&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;md&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;max_depths&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# merge into full param dicts
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;full_search_dicts&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[{&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;current_params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grid_search_dicts&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# cv and get best params
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;current_params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;results_df&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv_over_param_dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;full_search_dicts&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;predictors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;response&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kfolds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;##################################################
# round 2: tune subsample, colsample_bytree, colsample_bylevel
##################################################
# subsamples = np.linspace(0.01, 1.0, 10)
# colsample_bytrees = np.linspace(0.1, 1.0, 10)
# colsample_bylevel = np.linspace(0.1, 1.0, 10)
# narrower search
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;subsamples&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linspace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.25&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.75&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;11&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;colsample_bytrees&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linspace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;colsample_bylevel&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linspace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# subsamples = np.linspace(0.4, 0.9, 11)
# colsample_bytrees = np.linspace(0.05, 0.25, 5)
&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;grid_search_dicts&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;zip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'subsample'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'colsample_bytree'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'colsample_bylevel'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]))&lt;/span&gt; 
                     &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;product&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;subsamples&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;colsample_bytrees&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;colsample_bylevel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# merge into full param dicts
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;full_search_dicts&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[{&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;current_params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grid_search_dicts&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# cv and get best params
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;current_params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;results_df&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv_over_param_dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;full_search_dicts&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;predictors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;response&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kfolds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# round 3: learning rate
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;learning_rates&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;logspace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;grid_search_dicts&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'learning_rate'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lr&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;learning_rates&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# merge into full param dicts
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;full_search_dicts&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[{&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;current_params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grid_search_dicts&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# cv and get best params
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;current_params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;results_df&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv_over_param_dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;full_search_dicts&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;predictors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;response&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kfolds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;verbose&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The total training duration (the sum of times over the 3 iterations) is 1:24:22. This time may be an underestimate, since this search space is based on prior experience.&lt;/p&gt;

&lt;p&gt;Finally, we refit using the best hyperparameters and evaluate:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;xgb&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;XGBRegressor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;objective&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'reg:squarederror'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;n_estimators&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3438&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;random_state&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;RANDOMSTATE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;    
    &lt;span class=&quot;n&quot;&gt;verbosity&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;n_jobs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;booster&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'gbtree'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;   
    &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;current_params&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;    

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xgb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cross_val_score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xgb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predictors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;response&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
                          &lt;span class=&quot;n&quot;&gt;scoring&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;neg_root_mean_squared_error&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                          &lt;span class=&quot;n&quot;&gt;cv&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kfolds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                          &lt;span class=&quot;n&quot;&gt;n_jobs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;raw_scores&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cv_to_raw&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Log1p CV RMSE %.06f (STD %.04f)&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Raw CV RMSE %.0f (STD %.0f)&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;raw_scores&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;raw_scores&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The result essentially matches linear regression but is not as good as ElasticNet.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Raw CV RMSE 18193 (STD 2461)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;10-xgboost-with-hyperopt-optuna-and-ray&quot;&gt;10. XGBoost with Hyperopt, Optuna, and Ray&lt;/h2&gt;

&lt;p&gt;The steps to run a Ray tuning job with Hyperopt are:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Set up a Ray search space as a config dict.&lt;/li&gt;
  &lt;li&gt;Refactor the training loop into a function which takes the config dict as an argument and calls &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tune.report(rmse=rmse)&lt;/code&gt; to optimize a metric like RMSE.&lt;/li&gt;
  &lt;li&gt;Call &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ray.tune&lt;/code&gt; with the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;config&lt;/code&gt; and a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;num_samples&lt;/code&gt; argument which specifies how many times to sample.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Set up the Ray search space:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;xgb_tune_kwargs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;n_estimators&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tune&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loguniform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;max_depth&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tune&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randint&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;subsample&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tune&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;quniform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.25&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.75&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.01&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;colsample_bytree&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tune&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;quniform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.05&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.01&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;colsample_bylevel&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tune&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;quniform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.05&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.01&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;    
    &lt;span class=&quot;s&quot;&gt;&quot;learning_rate&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tune&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;quniform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;3.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# powers of 10
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;xgb_tune_params&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;k&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xgb_tune_kwargs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;keys&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;k&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;!=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'wandb'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;xgb_tune_params&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Set up the training function. Note that some search algos expect all hyperparameters to be floats and some search intervals to start at 0. So we convert params as necessary.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;my_xgb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# fix these configs to match calling convention
&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;# search wants to pass in floats but xgb wants ints
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'n_estimators'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'n_estimators'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;   &lt;span class=&quot;c1&quot;&gt;# pass float eg loguniform distribution, use int
&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;# hyperopt needs left to start at 0 but we want to start at 2    
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'max_depth'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'max_depth'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'learning_rate'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'learning_rate'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    
    &lt;span class=&quot;n&quot;&gt;xgb&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;XGBRegressor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;objective&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'reg:squarederror'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;n_jobs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;random_state&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;RANDOMSTATE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;booster&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'gbtree'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;   
        &lt;span class=&quot;n&quot;&gt;scale_pos_weight&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
        &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cross_val_score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xgb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predictors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;response&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
                                      &lt;span class=&quot;n&quot;&gt;scoring&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;neg_root_mean_squared_error&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                      &lt;span class=&quot;n&quot;&gt;cv&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kfolds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;rmse&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;tune&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;report&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rmse&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rmse&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;rmse&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rmse&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Run Ray Tune:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;algo&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;HyperOptSearch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random_state_seed&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;RANDOMSTATE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# ASHA
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scheduler&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;AsyncHyperBandScheduler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;analysis&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tune&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;my_xgb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                    &lt;span class=&quot;n&quot;&gt;num_samples&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;NUM_SAMPLES&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                    &lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xgb_tune_kwargs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;                    
                    &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;hyperopt_xgb&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                    &lt;span class=&quot;n&quot;&gt;metric&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;rmse&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                    &lt;span class=&quot;n&quot;&gt;mode&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;min&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                    &lt;span class=&quot;n&quot;&gt;search_alg&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;algo&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                    &lt;span class=&quot;n&quot;&gt;scheduler&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scheduler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                    &lt;span class=&quot;n&quot;&gt;verbose&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                   &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Extract the best hyperparameters, and evaluate a model using them:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# results dataframe sorted by best metric
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;param_cols&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'config.'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;k&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;k&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xgb_tune_params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;analysis_results_df&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;analysis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;results_df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'rmse'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'date'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'time_this_iter_s'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;param_cols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sort_values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'rmse'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# extract top row
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;best_config&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;analysis_results_df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iloc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'config.'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xgb_tune_params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;xgb&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;XGBRegressor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;objective&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'reg:squarederror'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;random_state&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;RANDOMSTATE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;    
    &lt;span class=&quot;n&quot;&gt;verbosity&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;n_jobs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;best_config&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xgb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cross_val_score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xgb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predictors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;response&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
                          &lt;span class=&quot;n&quot;&gt;scoring&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;neg_root_mean_squared_error&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                          &lt;span class=&quot;n&quot;&gt;cv&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kfolds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;raw_scores&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cv_to_raw&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Log1p CV RMSE %.06f (STD %.04f)&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Raw CV RMSE %.0f (STD %.0f)&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;raw_scores&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;raw_scores&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;With NUM_SAMPLES=1024 we obtain:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;Raw&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;CV&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;RMSE&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;18309&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;STD&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2428&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We can swap out Hyperopt for Optuna as simply as:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;algo&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;OptunaSearch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;With NUM_SAMPLES=1024 we obtain:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;Raw&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;CV&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;RMSE&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;18325&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;STD&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2473&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;11-lightgbm-with-hyperopt-and-optuna&quot;&gt;11. LightGBM with Hyperopt and Optuna&lt;/h2&gt;

&lt;p&gt;We can also easily swap out XGBoost for LightGBM.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Update the search space using &lt;a href=&quot;https://sites.google.com/view/lauraepp/parameters&quot;&gt;LightGBM equivalents&lt;/a&gt;.&lt;/p&gt;

    &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;lgbm_tune_kwargs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;n_estimators&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tune&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loguniform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;max_depth&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tune&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randint&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;'num_leaves'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tune&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;quniform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;               &lt;span class=&quot;c1&quot;&gt;# xgb max_leaves
&lt;/span&gt;    &lt;span class=&quot;s&quot;&gt;&quot;bagging_fraction&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tune&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;quniform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.01&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;# xgb subsample
&lt;/span&gt;    &lt;span class=&quot;s&quot;&gt;&quot;feature_fraction&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tune&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;quniform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.05&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.01&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;   &lt;span class=&quot;c1&quot;&gt;# xgb colsample_bytree
&lt;/span&gt;    &lt;span class=&quot;s&quot;&gt;&quot;learning_rate&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tune&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;quniform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;3.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Update training function:&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;my_lgbm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    
    &lt;span class=&quot;c1&quot;&gt;# fix these configs 
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'n_estimators'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'n_estimators'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;   &lt;span class=&quot;c1&quot;&gt;# pass float eg loguniform distribution, use int
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'num_leaves'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'num_leaves'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'learning_rate'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'learning_rate'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    
    &lt;span class=&quot;n&quot;&gt;lgbm&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;LGBMRegressor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;objective&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'regression'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                         &lt;span class=&quot;n&quot;&gt;max_bin&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;200&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                         &lt;span class=&quot;n&quot;&gt;feature_fraction_seed&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                         &lt;span class=&quot;n&quot;&gt;min_data_in_leaf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                         &lt;span class=&quot;n&quot;&gt;verbose&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                         &lt;span class=&quot;n&quot;&gt;n_jobs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                         &lt;span class=&quot;c1&quot;&gt;# these are specified to suppress warnings
&lt;/span&gt;                         &lt;span class=&quot;n&quot;&gt;colsample_bytree&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                         &lt;span class=&quot;n&quot;&gt;min_child_samples&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                         &lt;span class=&quot;n&quot;&gt;subsample&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                         &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                         &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    
    &lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cross_val_score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lgbm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predictors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;response&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
                              &lt;span class=&quot;n&quot;&gt;scoring&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;neg_root_mean_squared_error&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                              &lt;span class=&quot;n&quot;&gt;cv&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kfolds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;rmse&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  
    &lt;span class=&quot;n&quot;&gt;tune&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;report&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rmse&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rmse&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'rmse'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)}&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;and run as before, swapping my_lgbm in place of my_xgb. Results for LGBM: (NUM_SAMPLES=1024):&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;Raw&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;CV&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;RMSE&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;18615&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;STD&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2356&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Swapping out Hyperopt for Optuna:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;Raw&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;CV&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;RMSE&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;18614&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;STD&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2423&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;12-xgboost-on-a-ray-cluster&quot;&gt;12. XGBoost on a Ray cluster&lt;/h2&gt;

&lt;p&gt;Ray is a distributed framework. We can run a Ray Tune job over many instances using a cluster with a &lt;em&gt;head node&lt;/em&gt; and many &lt;em&gt;worker nodes&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Launching Ray is straightforward. On the head node we run &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ray start&lt;/code&gt;. On each worker node we run &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ray start --address x.x.x.x&lt;/code&gt;  with the address of the head node. Then in python we call &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ray.init()&lt;/code&gt; to connect to the head node. Everything else proceeds as before, and the head node runs trials using all instances in the cluster and stores results in Redis.&lt;/p&gt;

&lt;p&gt;Where it gets more complicated is specifying all the AWS details, instance types, regions, subnets, etc.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Clusters are defined in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ray1.1.yaml&lt;/code&gt;. (So far in this notebook we have been using the current production ray 1.0, but I had difficulty getting a cluster to run with ray 1.0 so I switched to the dev nightly. YMMV.)&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;boto3&lt;/code&gt; and AWS CLI configured credentials are used to spawn instances, so &lt;a href=&quot;https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-install.html&quot;&gt;install and configure AWS CLI&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Edit &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ray1.1.yaml&lt;/code&gt; file with, at a minimum, your AWS region and availability zone. Imageid may vary across regions, search for the current Deep Learning AMI (Ubuntu 18.04). You may not need to specify subnet, I had an issue with an inaccessible subnet when I let Ray default the subnet, possibly bad defaults somewhere.
    &lt;ul&gt;
      &lt;li&gt;To obtain those variables, launch the latest Deep Learning AMI (Ubuntu 18.04) currently Version 35.0 into a small instance in your favorite region/zone&lt;/li&gt;
      &lt;li&gt;Test that it works&lt;/li&gt;
      &lt;li&gt;Note the 4 variables: region, availability zone, subnet, AMI imageid&lt;/li&gt;
      &lt;li&gt;Terminate the instance and edit &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ray1.1.yaml&lt;/code&gt; with your region, availability zone, AMI imageid, optionally subnet&lt;/li&gt;
      &lt;li&gt;It may be advisable create your own image with all updates and requirements pre-installed and specify its AMI imageid, instead of using the generic image and installing everything at launch.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;To run the cluster: 
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ray up ray1.1.yaml&lt;/code&gt;
    &lt;ul&gt;
      &lt;li&gt;Creates head instance using AMI specified.&lt;/li&gt;
      &lt;li&gt;Installs Ray and related requirements including XGBoost from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;requirements.txt&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;Clones the druce/iowa repo from GitHub&lt;/li&gt;
      &lt;li&gt;Launches worker nodes per auto-scaling parameters (currently we fix the number of nodes because we’re not benchmarking the time the cluster will take to auto-scale)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;After the cluster starts you can check the AWS console and note that several instances were launched.&lt;/li&gt;
  &lt;li&gt;Check &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ray monitor ray1.1.yaml&lt;/code&gt; for any error messages&lt;/li&gt;
  &lt;li&gt;Run Jupyter on the cluster with port forwarding
 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ray exec ray1.1.yaml --port-forward=8899 'jupyter notebook --port=8899'&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Open the notebook on the generated URL which is printed on the console at startup e.g. http://localhost:8899/?token=5f46d4355ae7174524ba71f30ef3f0633a20b19a204b93b4&lt;/li&gt;
  &lt;li&gt;You can run a terminal on the head node of the cluster with
 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ray attach /Users/drucev/projects/iowa/ray1.1.yaml&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;You can ssh explicitly with the IP address and the generated private key
 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ssh -o IdentitiesOnly=yes -i ~/.ssh/ray-autoscaler_1_us-east-1.pem ubuntu@54.161.200.54&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Run port forwarding to the Ray dashboard with &lt;br /&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ray dashboard ray1.1.yaml&lt;/code&gt;
and then open
 http://localhost:8265/&lt;/li&gt;
  &lt;li&gt;Make sure to choose the default kernel in Jupyter to run in the correct conda environment with all installs&lt;/li&gt;
  &lt;li&gt;Make sure to use the ray.init() command given in the startup messages.
 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ray.init(address='localhost:6379', _redis_password='5241590000000000')&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;The cluster will incur AWS charges so &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ray down ray1.1.yaml&lt;/code&gt; when complete&lt;/li&gt;
  &lt;li&gt;See &lt;a href=&quot;https://github.com/druce/iowa/blob/master/hyperparameter_optimization_cluster.ipynb&quot;&gt;hyperparameter_optimization_cluster.ipynb&lt;/a&gt;, separated out so each notebook can be run end-to-end with/without cluster setup&lt;/li&gt;
  &lt;li&gt;See &lt;a href=&quot;https://docs.ray.io/en/latest/cluster/launcher.html&quot;&gt;Ray docs&lt;/a&gt; for additional info on Ray clusters.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Besides connecting to the cluster instead of running Ray Tune locally, no other change to code is needed to run on the cluster&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;analysis&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tune&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;my_xgb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                    &lt;span class=&quot;n&quot;&gt;num_samples&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;NUM_SAMPLES&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                    &lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xgb_tune_kwargs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;                    
                    &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;hyperopt_xgb&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                    &lt;span class=&quot;n&quot;&gt;metric&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;rmse&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                    &lt;span class=&quot;n&quot;&gt;mode&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;min&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                    &lt;span class=&quot;n&quot;&gt;search_alg&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;algo&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                    &lt;span class=&quot;n&quot;&gt;scheduler&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scheduler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                    &lt;span class=&quot;c1&quot;&gt;# add this because distributed jobs occasionally error out
&lt;/span&gt;                    &lt;span class=&quot;n&quot;&gt;raise_on_failed_trial&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# otherwise no reults df returned if any trial error           
&lt;/span&gt;                    &lt;span class=&quot;n&quot;&gt;verbose&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                   &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Results for XGBM on cluster (2048 samples, cluster is 32 m5.large instances):&lt;/p&gt;

&lt;p&gt;Hyperopt (time 1:30:58)&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Raw CV RMSE 18030 (STD 2356)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Optuna (time 1:29:57)&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Raw CV RMSE 18028 (STD 2353)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;13-lightgbm-on-a-ray-cluster&quot;&gt;13. LightGBM on a Ray cluster&lt;/h2&gt;

&lt;p&gt;Similarly for LightGBM:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;analysis&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tune&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;my_lgbm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                    &lt;span class=&quot;n&quot;&gt;num_samples&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;NUM_SAMPLES&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                    &lt;span class=&quot;n&quot;&gt;config&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lgbm_tune_kwargs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                    &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;hyperopt_lgbm&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                    &lt;span class=&quot;n&quot;&gt;metric&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;rmse&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                    &lt;span class=&quot;n&quot;&gt;mode&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;min&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                    &lt;span class=&quot;n&quot;&gt;search_alg&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;algo&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                    &lt;span class=&quot;n&quot;&gt;scheduler&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scheduler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                    &lt;span class=&quot;n&quot;&gt;raise_on_failed_trial&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# otherwise no reults df returned if any trial error                                                            
&lt;/span&gt;                    &lt;span class=&quot;n&quot;&gt;verbose&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                   &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Results for LightGBM on cluster (2048 samples, cluster is 32 m5.large instances):&lt;/p&gt;

&lt;p&gt;Hyperopt (time: 1:05:19) :&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Raw CV RMSE 18459 (STD 2511)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Optuna (time 0:48:16):&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Raw CV RMSE 18458 (STD 2511)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;14-concluding-remarks&quot;&gt;14. Concluding remarks&lt;/h2&gt;

&lt;p&gt;In every case I’ve applied them, Hyperopt and Optuna have given me at least a small improvement in the best metrics I found using grid search methods. Bayesian optimization tunes faster with a less manual process vs. sequential tuning.  It’s fire-and-forget.&lt;/p&gt;

&lt;p&gt;Is Ray Tune the way to go for hyperparameter tuning? Provisionally, yes. Ray provides integration between the underlying ML (e.g. XGBoost), the Bayesian search (e.g. Hyperopt), and early stopping (ASHA). It allows us to easily swap search algorithms.&lt;/p&gt;

&lt;p&gt;There are other alternative search algorithms in the &lt;a href=&quot;https://docs.ray.io/en/master/tune/api_docs/suggestion.html&quot;&gt;Ray docs&lt;/a&gt; but these seem to be the most popular, and I haven’t got the others to run yet. If after a while I find I am always using e.g. Hyperopt and never use clusters, I might use the native Hyperopt/XGBoost integration without Ray, to access any native Hyperopt features and because it’s one less technology in the stack.&lt;/p&gt;

&lt;p&gt;Clusters? Most of the time I don’t have a need, costs add up, did not see as large a speedup as expected. I only see  ~2x speedup on the 32-instance cluster.  Setting up the test I expected a bit less than 4x speedup accounting for slightly less-than-linear scaling.. The longest run I have tried, with 4096 samples, ran overnight on desktop. My MacBook Pro w/16 threads and desktop with 12 threads and GPU are plenty powerful for this data set. Still, it’s useful to have the clustering option in the back pocket. In production, it may be more standard and maintainable to deploy with e.g. Terraform, Kubernetes than the Ray native YAML cluster config file. If you want to train big data at scale you need to really understand and streamline your pipeline.&lt;/p&gt;

&lt;p&gt;It continues to surprise me that ElasticNet, i.e. regularized linear regression, performs similar to boosting on this dataset. I heavily engineered features so that linear methods work well. Predictors were chosen using Lasso/ElasticNet and I used log and Box-Cox transforms to force predictors to follow assumptions of least-squares.&lt;/p&gt;

&lt;p&gt;This may tend to validate one of the &lt;a href=&quot;https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3624052&quot;&gt;critiques of machine learning&lt;/a&gt;, that the most powerful machine learning methods don’t necessarily always converge all the way to the best solution. If you have a ground truth that is linear plus noise, a complex XGBoost or neural network algorithm should get arbitrarily close to the closed-form optimal solution, but will probably never match the optimal solution exactly. XGBoost regression is piecewise constant and the complex neural network is subject to the vagaries of stochastic gradient descent. I thought arbitrarily close meant almost indistinguishable, given that gradient boosting is the gold standard for tabular data. But clearly this is not always the case.&lt;/p&gt;

&lt;p&gt;ElasticNet with L1 + L2 regularization plus gradient descent and hyperparameter optimization is still machine learning. It’s simply a form of ML better matched to this problem. In the real world where data sets don’t match assumptions of OLS, gradient boosting generally performs extremely well. And even on this dataset, engineered for success with the linear models, SVR and KernelRidge performed better than ElasticNet (not shown) and ensembling ElasticNet with XGBoost, LightGBM, SVR, neural networks worked best of all.&lt;/p&gt;

&lt;p&gt;To paraphrase Casey Stengel, clever feature engineering will always outperform clever model algorithms and vice-versa&lt;sup id=&quot;fnref:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;. But improving your hyperparameters will always improve your results. Bayesian optimization can be considered a best practice.&lt;/p&gt;

&lt;p&gt;Again, the full code is on &lt;a href=&quot;https://github.com/druce/iowa&quot;&gt;GitHub&lt;/a&gt;&lt;/p&gt;

&lt;script&gt;
    var headings = document.querySelectorAll(&quot;h2[id]&quot;);

    for (var i = 0; i &lt; headings.length; i++) {
        headings[i].innerHTML =
            '&lt;a name=&quot;' + headings[i].id + '&quot;&gt;' +
                headings[i].innerText +
            '&lt;/a&gt;';
    }
&lt;/script&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;It would be more sound to separately tune the stopping rounds. Just averaging the best stopping time across kfolds is questionable. In a real world scenario, we should keep a holdout test set. We should retrain on the full training dataset (not kfolds) with early stopping to get the best number of boosting rounds. Then we should measure RMSE in the test set using all the cross-validated parameters including number of boosting rounds for the expected OOS RMSE. However, for the purpose of comparing tuning methods, the CV error is OK. We just want to look at how we would make model decisions using CV and not worry too much about the generalization error. One could even argue it adds a little more noise to the comparison of hyperparameter selection. But a test set would be the correct methodology in practice. It wouldn’t change conclusions directionally and I’m not going to rerun everything but if I were to start over I would do it that way. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;This is not intended to make sense. &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Druce Vertes</name></author><category term="datascience" /><category term="datascience" /><summary type="html">Bayesian optimization of machine learning model hyperparameters works faster and better than grid search. Here’s how we can speed up hyperparameter tuning with 1) Bayesian optimization with Hyperopt and Optuna, running on… 2) the Ray distributed machine learning framework, with a unified API to many hyperparameter search algos and early stopping schedulers, and… 3) a distributed cluster of cloud instances for even faster tuning. Image source: Crissman Loomis</summary></entry><entry><title type="html">The Biggest Bluff: She Stoops To Conquer</title><link href="http://0.0.0.0:4000/2020/07/the-biggest-bluff-she-stoops-to-conquer/" rel="alternate" type="text/html" title="The Biggest Bluff: She Stoops To Conquer" /><published>2020-07-09T18:28:57-04:00</published><updated>2020-07-09T18:28:57-04:00</updated><id>http://0.0.0.0:4000/2020/07/the-biggest-bluff-she-stoops-to-conquer</id><content type="html" xml:base="http://0.0.0.0:4000/2020/07/the-biggest-bluff-she-stoops-to-conquer/">&lt;p&gt;&lt;img src=&quot;/assets/2020/biggestbluff.jpg&quot; alt=&quot;The Biggest Bluff, by Maria Konnikova (cover art)&quot; /&gt;&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;&lt;em&gt;I loved &lt;a href=&quot;https://www.amazon.com/Biggest-Bluff-Learned-Attention-Master/dp/052552262X/&quot;&gt;The Biggest Bluff: How I Learned to Pay Attention, Master Myself, and Win, by Maria Konnikova&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;!--more--&gt;

&lt;p&gt;Like Maria Konnikova, I think poker is a great laboratory for decision-making under uncertainty. Personal story: I had a job as a market analyst, but I was a terrible trader. One issue I had is I couldn’t pull the trigger. For example, in 2002 after Apple introduced the iPod and the stock market was crashing I wrote something about how with their brand they could own this consumer media space. I had it on a list to buy as the market was crashing but the volatility spooked me. Then when the market stabilized the stock was up 50% and I was like, naw, too much. This was at a price of about $12 at the time but split-adjusted about $1, so about 400x ago.&lt;/p&gt;

&lt;p&gt;I was a casual poker player, would play a few times a year for pennies with friends, it was more of an excuse to shoot the shit, not a serious game. Then the big poker boom came along and I thought I should try to beat these small games. Started playing $5 tourneys on PartyPoker and built a bankroll of a couple of thousand dollars, enough to buy some Apple devices and enter a couple of WSOP satellites, which I failed to cash.&lt;/p&gt;

&lt;p&gt;It made me a much better investor because I no longer had a problem pulling the trigger. If I had an idea, I was able to get comfortable with a quick upside/downside +EV calculation and bet a small enough 1/n part of my bankroll that intuitively I knew if I did that n times, on average I would do well and never go bust. I internalized &lt;a href=&quot;https://medium.com/@nickyoder/the-kelly-criterion-cd986d037d87&quot;&gt;Kelly&lt;/a&gt; and what loss I was willing to take and the discipline to take and cut the loss. Poker developed confidence. Now, getting a good investment idea is still rare but I’ve had a few and have done OK overall.&lt;/p&gt;

&lt;p&gt;So anyway, I like poker, am casually serious about understanding the game, I have a bookshelf with about 3 feet of poker books, and have &lt;a href=&quot;/2013/08/risk-arbitrage-investing-and-poker/&quot;&gt;blogged about the lessons poker teaches about investing&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The Biggest Bluff fits in with &lt;a href=&quot;https://www.amazon.com/Positively-Fifth-Street-Murderers-Cheetahs/dp/0312422520/&quot;&gt;Positively 4th Street&lt;/a&gt;, &lt;a href=&quot;https://www.amazon.com/Noble-Hustle-Poker-Jerky-Death/dp/0345804333/&quot;&gt;The Noble Hustle&lt;/a&gt;, &lt;a href=&quot;https://www.amazon.com/Professor-Banker-Suicide-King-Richest/dp/0446694975&quot;&gt;The Professor, The Banker and the Suicide King&lt;/a&gt;, &lt;a href=&quot;https://www.amazon.com/Biggest-Game-Town-Al-Alvarez/dp/0312428421/&quot;&gt;The Biggest Game In Town&lt;/a&gt;, as a very readable memoir of a poker journey.&lt;/p&gt;

&lt;p&gt;It stands apart because Maria really took it seriously, she is a scientist and invested 100% effort to learn the game. She enlists all-time great Erik Seidel (made famous by the WSOP highlight in Rounders, with Matt Damon) to coach her on her journey. She talks to many great players, poker psychologists, uses training/simulation software and really goes all out to win. And she wins a live tournament for an $80,000 payout.&lt;/p&gt;

&lt;p&gt;She puts me to shame because she read (really?) Von Neumann and Morgenstern’s &lt;a href=&quot;https://www.amazon.com/Theory-Games-Economic-Behavior-Commemorative/dp/0691130612/&quot;&gt;Theory of Games&lt;/a&gt;, which is a fairly impenetrable tome. It would literally take me a year. It’s like reading the Feynman lectures on physics to master hacky-sack. All I ever did was take a &lt;a href=&quot;https://www.coursera.org/learn/game-theory-1&quot;&gt;Coursera&lt;/a&gt; on game theory (recommended), work through my own hand-written sims in Java, and work through most of Chen’s &lt;a href=&quot;https://www.amazon.com/Mathematics-Poker-Bill-Chen/dp/1886070253&quot;&gt;Mathematics of Poker&lt;/a&gt;. In my defense when I got into poker some of the modern tools were not available, Pio Solver, PokerTracker, FlopZilla, SnapShove.&lt;/p&gt;

&lt;p&gt;The book succeeds as, what you have to sacrifice to become good at something. She’s very honest. It’s a tough game, the struggle is real, and you feel it. There’s a huge element of mastering yourself. I personally do this by not playing stakes above $1/2 level, or tournaments for much over $100. In other words, being a coward. She really challenged herself to play some of the biggest, toughest tournaments. I’m in it mostly for the cheap thrills, the social element, the mental and self-mastery challenge, and proving I can be good at something.&lt;/p&gt;

&lt;p&gt;Why did she do it, really? Was it worth it? I have questions.&lt;/p&gt;

&lt;p&gt;I have a great deal of sympathy for Maria’s grandmother, who is dismayed that, as a made woman of the intelligentsia, Columbia PhD, multiple books and New Yorker bylines, she would stoop to become a gambler. Like Baba Anya, I think poker is a fun but somewhat pointless exercise. Like Shaw said about chess, a foolish expedient for making idle people think they are doing something clever when they are only wasting their time.&lt;/p&gt;

&lt;p&gt;After all that studying, why does she bet 2x the big blind in early position with AA?&lt;/p&gt;

&lt;p&gt;And did Von Neumann, the guy who invented game theory, really think he could beat roulette? Unlikely.&lt;/p&gt;

&lt;p&gt;In her early ‘failure’, doesn’t she get all in with the best of it and lose to a flush suck-out? And in her big success, doesn’t she benefit from a miracle suck-out? As a player, I wish she would walk through the math and give examples of breakthroughs that made her play better. It’s not a book for a hard-core poker audience. And even though avoiding the skill/luck fallacy is a theme of the book, I think that in order to make the book into a readable narrative, she still falls victim to it.&lt;/p&gt;

&lt;p&gt;When I occasionally read about some poker player of the year in &lt;a href=&quot;https://www.cardplayer.com/poker-players/player-of-the-year/&quot;&gt;Card Player&lt;/a&gt; magazine, I’m reminded a little of tongue-in-cheek &lt;a href=&quot;https://twitter.com/davdchristmas/status/1239196410857340933&quot;&gt;marble racing videos&lt;/a&gt;, a &lt;a href=&quot;https://time.com/5803841/marble-racing-videos/&quot;&gt;thing&lt;/a&gt; apparently. Or &lt;a href=&quot;https://twitter.com/joonlee/status/808415700905787392?lang=en&quot;&gt;Janken Queen&lt;/a&gt;. (Or &lt;a href=&quot;https://www.youtube.com/watch?v=kB5XXn0eKow&quot;&gt;World Series of Dice&lt;/a&gt;, but that’s a bit different.) I would love to see a ‘Best In Show’ style spoof of the poker world. The winner of the WSOP is usually a very good player who went on a lucky heater, or at least dodged an unlucky streak.&lt;/p&gt;

&lt;p&gt;One thing poker teaches you is, variance is really going to vary. While I was a consistently winning micro-stakes online player, I once had a big drawdown over a month when it seemed like &lt;em&gt;every time&lt;/em&gt; I went in with the best of it, I lost. I was astounded. People attribute far too much to skill. Taleb may be an ass, but ‘Fooled by Randomness’ is a classic. Nevertheless what you learn about luck from playing poker, you cannot learn from a book.&lt;/p&gt;

&lt;p&gt;Ultimately poker is a fascinating, but slightly silly game. In tennis, golf, chess, a pro cannot lose to an amateur at the turn of an unfriendly card. You can’t have a Jamie Gold going on a huge heater, getting hit by the deck, and winning the WSOP, despite being an apparently mediocre player. All the hard work you can muster creates a couple of big blinds an hour of edge in a game where you have to risk your whole stack to cash in that edge.&lt;/p&gt;

&lt;p&gt;Maybe that’s one reason why it’s a metaphor for life? An endlessly fascinating but perhaps ultimately pointless exercise, which we have a capacity for making great or terrible by investing with manufactured meaning?&lt;/p&gt;

&lt;p&gt;Sometimes poker is how life works. You have to work hard every day to get a little better, exercise great judgment to stay in the game. And be all-in where you have the best of it. And if you play your cards right you’ll do well. But a couple of bad breaks, and you’re screwed. Graduate at the wrong time in the teeth of recession, pick a company or industry that goes belly-up and people no longer value skills that took years to master, get cancer. Hell, we’re lucky to be born in the right place at the right time and be at the table. All you can do is make good decisions and master yourself, which is the true object of the game. And let the cards fall how they may.&lt;/p&gt;

&lt;p&gt;It was interesting to see &lt;a href=&quot;https://www.youtube.com/watch?v=srQnnJtjJ2k&quot;&gt;Doug Polk&lt;/a&gt; quit the game. Burnout, depression, a more mature perspective on life? I wonder what Maria thinks about that?&lt;/p&gt;

&lt;p&gt;The book alludes to the moral degeneracy of the poker world, but maybe pulls a few punches. If you view poker with detachment as an exercise in understanding risk, probability, yourself, and making good decisions at small stakes, it’s fun and healthy. When you make gambling your profession, taking others’ money by your wits, there can be something fundamentally problematic about the zero-sum aspect of it, about staking your worth on the outcome of the turn of a card, about the dark side of angle shooters, of substance abusers.&lt;/p&gt;

&lt;p&gt;A used-car salesman can say he is providing a service, helping people get into the right car. Being a poker pro is a bit hard to rationalize. You’re providing an entertainment service, letting people challenge their wits against the best?&lt;/p&gt;

&lt;p&gt;The jailbird Ed Norton character in Rounders illustrates the darker side of the game. Not a great movie but &lt;a href=&quot;https://www.youtube.com/watch?v=i4A4TzHHdko&quot;&gt;High Roller: The Stu Ungar Story&lt;/a&gt; illustrates it. There’s something weirdly intoxicating and addictive to gambling, to be good at poker you have to be obsessive, and these guys are often prone to addictive behaviors and not very healthy mentally or physically. The household names who become stars on ESPN2 go bust and beg strangers to stake them. I’ve seen ridiculous tantrums and chair-throwing (which does get you banned LOL). I’ve seen college students fresh off the bus tilt and lose a semester of savings in 30 minutes.&lt;/p&gt;

&lt;p&gt;Poker is a great training ground for investing, but investing is a positive-sum game in the long run. Ultimately there is a social purpose, to &lt;a href=&quot;https://www.brainyquote.com/quotes/john_maynard_keynes_152044#:~:text=John%20Maynard%20Keynes%20Quotes&amp;amp;text=The%20social%20object%20of%20skilled%20investment%20should%20be%20to%20defeat,ignorance%20which%20envelope%20our%20future.&quot;&gt;defeat the dark forces of time and ignorance that envelop our future.&lt;/a&gt; The market is not always as fun and you can’t bluff the market, and it has its share of con artists and angle-shooters, but I think it’s a more proper object of talent for people who understand risk, expected value, human nature… and luck.&lt;/p&gt;</content><author><name>Druce Vertes, CFA</name></author><category term="poker" /><category term="poker" /><category term="books" /><summary type="html">I loved The Biggest Bluff: How I Learned to Pay Attention, Master Myself, and Win, by Maria Konnikova.</summary></entry><entry><title type="html">Deep Reinforcement Learning For Trading Applications (Alpha Architect)</title><link href="http://0.0.0.0:4000/2020/02/deep-reinforcement-learning-for-trading-applications" rel="alternate" type="text/html" title="Deep Reinforcement Learning For Trading Applications (Alpha Architect)" /><published>2020-02-26T17:28:57-05:00</published><updated>2020-02-26T17:28:57-05:00</updated><id>http://0.0.0.0:4000/2020/02/deep-reinforcement-learning-for-trading-applications</id><content type="html" xml:base="http://0.0.0.0:4000/2020/02/deep-reinforcement-learning-for-trading-applications">&lt;p&gt;&lt;img src=&quot;/assets/2020/rl.png&quot; alt=&quot;Observing one reinforcement learning episode of stock trading&quot; /&gt;&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;Reinforcement learning is a machine learning paradigm that can learn behavior to achieve maximum reward in complex dynamic environments, as simple as Tic-Tac-Toe, or as complex as Go, and options trading. In this post, we will try to explain what reinforcement learning is, share code to apply it, and references to learn more about it. First, we’ll learn a simple algorithm to play Tic-Tac-Toe, then learn to trade a non-random price series. Finally, we’ll talk about how reinforcement learning can master complex financial concepts like option pricing and optimal diversification.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;!--more--&gt;

&lt;p&gt;Originally Published at &lt;a href=&quot;alphaarchitect.com&quot;&gt;Alpha Architect&lt;/a&gt;:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://alphaarchitect.com/2020/02/26/reinforcement-learning-for-trading/&quot;&gt;Deep Reinforcement Learning For Trading Applications&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;By &lt;a href=&quot;https://alphaarchitect.com/user/druce.vertes/&quot;&gt;Druce Vertes&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;February 26, 2020&lt;/p&gt;

&lt;p&gt;Tags: Research Insights, Machine Learning&lt;/p&gt;</content><author><name>Druce Vertes</name></author><category term="datascience" /><category term="investing" /><category term="datascience" /><category term="investing" /><summary type="html">Reinforcement learning is a machine learning paradigm that can learn behavior to achieve maximum reward in complex dynamic environments, as simple as Tic-Tac-Toe, or as complex as Go, and options trading. In this post, we will try to explain what reinforcement learning is, share code to apply it, and references to learn more about it. First, we’ll learn a simple algorithm to play Tic-Tac-Toe, then learn to trade a non-random price series. Finally, we’ll talk about how reinforcement learning can master complex financial concepts like option pricing and optimal diversification.</summary></entry><entry><title type="html">Forecasting US Equity Market Returns with Machine Learning (Alpha Architect)</title><link href="http://0.0.0.0:4000/2020/01/forecasting-us-equity-market-returns-with-machine-learning" rel="alternate" type="text/html" title="Forecasting US Equity Market Returns with Machine Learning (Alpha Architect)" /><published>2020-01-07T17:28:57-05:00</published><updated>2020-01-07T17:28:57-05:00</updated><id>http://0.0.0.0:4000/2020/01/forecasting-us-equity-market-returns-with-machine-learning</id><content type="html" xml:base="http://0.0.0.0:4000/2020/01/forecasting-us-equity-market-returns-with-machine-learning">&lt;p&gt;&lt;img src=&quot;/assets/2020/cape.png&quot; alt=&quot;Image source: https://www.valuewalk.com/wp-content/uploads/2017/07/SSRN-id2983860.pdf&quot; /&gt;&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;Shiller’s CAPE ratio is a popular and useful metric for measuring whether stock prices are overvalued or undervalued relative to earnings. Recently, Vanguard analysts Haifeng Wang, Harshdeep Singh Ahluwalia, Roger A. Aliaga-Díaz, and Joseph H. Davis have written a very interesting paper on forecasting equity returns using Shiller’s CAPE and machine learning: “The Best of Both Worlds: Forecasting US Equity Market Returns using a Hybrid Machine Learning – Time Series Approach“, which effectively applies machine learning to an import investing problem. Image source: &lt;a href=&quot;https://www.valuewalk.com/wp-content/uploads/2017/07/SSRN-id2983860.pdf&quot;&gt;Improving U.S. stock return forecasts: A “fair-value” CAPE approach, Joseph Davis, et al. (2017)&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;!--more--&gt;

&lt;p&gt;Originally Published at &lt;a href=&quot;alphaarchitect.com&quot;&gt;Alpha Architect&lt;/a&gt;:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://alphaarchitect.com/2020/01/07/forecasting-us-equity-market-returns-using-machine-learning/&quot;&gt;Forecasting US Equity Market Returns with Machine Learning&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;By &lt;a href=&quot;https://alphaarchitect.com/user/druce.vertes/&quot;&gt;Druce Vertes&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;January 7, 2020&lt;/p&gt;

&lt;p&gt;Tags: Research Insights, Machine Learning&lt;/p&gt;</content><author><name>Druce Vertes</name></author><category term="datascience" /><category term="investing" /><category term="datascience" /><category term="investing" /><summary type="html">Shiller’s CAPE ratio is a popular and useful metric for measuring whether stock prices are overvalued or undervalued relative to earnings. Recently, Vanguard analysts Haifeng Wang, Harshdeep Singh Ahluwalia, Roger A. Aliaga-Díaz, and Joseph H. Davis have written a very interesting paper on forecasting equity returns using Shiller’s CAPE and machine learning: “The Best of Both Worlds: Forecasting US Equity Market Returns using a Hybrid Machine Learning – Time Series Approach“, which effectively applies machine learning to an import investing problem. Image source: Improving U.S. stock return forecasts: A “fair-value” CAPE approach, Joseph Davis, et al. (2017)</summary></entry><entry><title type="html">Understanding Classification Thresholds Using Isocurves</title><link href="http://0.0.0.0:4000/2019/10/understanding-classification-thresholds-using-isocurves" rel="alternate" type="text/html" title="Understanding Classification Thresholds Using Isocurves" /><published>2019-10-15T18:28:57-04:00</published><updated>2019-10-15T18:28:57-04:00</updated><id>http://0.0.0.0:4000/2019/10/understanding-classification-thresholds-using-isocurves</id><content type="html" xml:base="http://0.0.0.0:4000/2019/10/understanding-classification-thresholds-using-isocurves">&lt;p&gt;&lt;img src=&quot;/assets/2020/isocurves.png&quot; alt=&quot;Four isocurve plots&quot; /&gt;&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;As a data scientist, you might say…“A blog post about thresholds? It’s not even a data science problem, it’s more of a business decision.” And you would not be wrong! Threshold selection lacks the appeal of say, generative adversarial networks.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;!--more--&gt;

&lt;p&gt;Now you are in a conference room, presenting your work on a classification problem. You demonstrate all the magic you performed with feature engineering, predictor selection, model selection, hyperparameter tuning, and ensembling. You conclude your presentation with the predicted probabilities and the &lt;a href=&quot;https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_the_curve&quot;&gt;ROC curve&lt;/a&gt;, and an excellent AUC. You sit down confident in a job well done.&lt;/p&gt;

&lt;p&gt;And the manager says, “What am I supposed to do with all this probability and ROC stuff? I just want to know if I should do x or y.” Where x might be, should I classify this message as spam or not? Or should my Tesla hit the brakes faced with this road condition? Or should I approve this loan or not?&lt;/p&gt;

&lt;p&gt;This post is for you.&lt;/p&gt;

&lt;p&gt;Your job as a data scientist isn’t done until you explain how to interpret the model and apply it. That means threshold selection for the business decision that motivated the model.&lt;/p&gt;

&lt;p&gt;Here’s a deep dive into threshold selection, including the F1-score and how it compares to other metrics. Let’s dive in!&lt;/p&gt;

&lt;h2 id=&quot;1-you-need-a-metric-or-a-cost-function-to-optimize&quot;&gt;1. You need a metric or a cost function to optimize.&lt;/h2&gt;

&lt;p&gt;A business doesn’t care about winning a Kaggle contest. The only valid reasons for a business to spend money on data science are to:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Acquire and retain customers…&lt;/li&gt;
  &lt;li&gt;Reduce costs…&lt;/li&gt;
  &lt;li&gt;Otherwise execute the business better, which…&lt;/li&gt;
  &lt;li&gt;Increases profits.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;If you understand the business objective of the model, what key performance indicators (KPIs) the model can improve, and apply a scientific method to asking the right questions and improving those KPIs, you may be an A-team data scientist.&lt;/p&gt;

&lt;p&gt;Threshold selection is necessary in the context of an algorithm which predicts the probability that an observation will belong to the positive class. This is how most (but not all) classification algorithms work. The question then becomes, if we need to make discrete predictions based on the modeled probability, what is the best threshold to classify an observation as positive?&lt;/p&gt;

&lt;p&gt;The first question to ask is: Can you quantify the cost of &lt;a href=&quot;https://en.wikipedia.org/wiki/Type_I_and_type_II_errors&quot;&gt;type I and type II errors&lt;/a&gt; using KPIs the business cares about? What is the marginal cost of a false positive, and what is the marginal cost of a false negative? If you can identify the right cost function, your job is essentially done.&lt;/p&gt;

&lt;p&gt;Select the threshold that minimizes the total cost of false positives and false negatives in cross-validation. (Always in cross-validation: never select a hyperparameter or make any decision about your model using training data or test data; the classification threshold can be considered a hyperparameter.)&lt;/p&gt;

&lt;p&gt;Suppose we are predicting whether a borrower will default on a credit card. Extending credit that should have been denied costs $50,000 in credit losses and administrative costs. Denying credit that should have been extended results in a loss of the $10,000 lifetime value of the customer (LTV). Pick the threshold that gives the lowest total cost of false positives and false negatives over your cross-validation set (or folds).&lt;/p&gt;

&lt;h2 id=&quot;2-the-roc-curve-visualizes-the-set-of-feasible-solutions-as-you-vary-the-classification-threshold-implicitly-varying-the-cost-of-false-positives-relative-to-false-negatives&quot;&gt;2. The ROC curve visualizes the set of feasible solutions, as you vary the classification threshold, implicitly varying the cost of false positives relative to false negatives.&lt;/h2&gt;

&lt;p&gt;If the positive class represents the detection of a stop sign or a medical condition, the cost of a false negative is high. You need a low threshold to minimize false negatives, which cause you to blow through an intersection and collide with traffic or fail to obtain further life-saving diagnosis or treatment.&lt;/p&gt;

&lt;p&gt;If your problem is spam detection, blocking an important email is more costly than letting through a spam message. So you prefer to tilt toward a higher threshold to tag email as spam, to minimize false positives.&lt;/p&gt;

&lt;p&gt;This is the ROC plot from a Kaggle data set.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 1. The ROC Curve&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2019/fig1.png&quot; alt=&quot;Figure 1. The ROC Curve&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Let’s review how to interpret the ROC plot:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;A &lt;em&gt;true positive&lt;/em&gt; is a true observation correctly predicted to be true.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The &lt;em&gt;true-positive rate (TPR)&lt;/em&gt; is the number of &lt;em&gt;true positives / ground truth positives&lt;/em&gt; (also called &lt;em&gt;recall&lt;/em&gt; or &lt;em&gt;sensitivity&lt;/em&gt;). &lt;em&gt;Ground truth positives = true positives + false negatives&lt;/em&gt;:&lt;/p&gt;

\[TPR = \frac{tp}{tp+fn}\]
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A &lt;em&gt;false positive&lt;/em&gt; is a false observation incorrectly predicted to be true.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The &lt;em&gt;false-positive rate (FPR)&lt;/em&gt; is the number of &lt;em&gt;false positives / ground truth negatives&lt;/em&gt; (1 — &lt;em&gt;FPR&lt;/em&gt; is the &lt;em&gt;specificity&lt;/em&gt;). &lt;em&gt;Ground truth negatives = true negatives + false positives&lt;/em&gt;:&lt;/p&gt;

\[FPR = \frac{fp}{tn + fp}\]
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The best place to be on the ROC chart is the top left corner, with 100% TPR, sensitivity, or recall, and 0% FPR, or 100% specificity. This is usually not feasible.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The best &lt;em&gt;feasible&lt;/em&gt; points are on the ROC curve. As you move from left to right, you decrease your threshold to classify an observation as positive. You get more true positives, but also more false positives.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A good analogy is fishing with a net: As you use a finer net, a smaller number of fish slip through, but you also catch more seaweed and garbage. The $64,000 question is how fine a net to use for the best possible results.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;3-the-slope-of-the-roc-curve-reflects-how-many-true-positives-you-gain-for-each-false-positive-you-accept&quot;&gt;3. The slope of the ROC curve reflects how many true positives you gain for each false positive you accept.&lt;/h2&gt;

&lt;p&gt;Let’s discuss how to interpret the slope of the ROC curve.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 2. The Slope of the ROC Curve&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2019/fig2.png&quot; alt=&quot;Figure 2. The Slope of the ROC Curve&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We plot the TPR on the left axis and the FPR on the bottom axis. And we also plot the 45º line as a dashed line. The 45º line is the ROC curve you would get if you classified your observations randomly using the base rate. In this example, about 20.8% of observations are ground truth positive. If you use a random classifier which randomly classifies 20.8% as positive, your ROC curve will approximately follow the 45º line. It’s the best AUC (Area Under the ROC Curve) you can get without doing any modeling. Or the worst-case AUC of a failed attempt at modeling which is unable to find any predictive signal.&lt;/p&gt;

&lt;p&gt;Recalling that the TPR is true positives as a percentage of ground truth positive, we put the actual true positive count on the right axis. Likewise, since the FPR is false positives as a percent of ground truth negatives, the actual false positive count is on the top axis.&lt;/p&gt;

&lt;p&gt;This helps us interpret the slope: The slope is the number of additional true positives you gain by accepting one additional false positive. (An approximation, keeping in mind that the ROC curve is made up of discrete points).&lt;/p&gt;

&lt;p&gt;The 45º line has a slope of 1 in TPR/FPR space (percentage point increase in TPR per percentage point of FPR). In raw true positive/false positive space, the slope of the 45º line is the total number of positives/total number of negatives.&lt;/p&gt;

&lt;p&gt;If the positive class occurs with probability &lt;em&gt;p&lt;/em&gt;, the slope is: \(\frac{p}{1 - p}\)&lt;/p&gt;

&lt;p&gt;If you have an ideal ROC curve with a continuously decreasing slope (concave down), you can start at the bottom left and keep going toward the top right as long as the cost of additional true positives expressed in false negatives is acceptable.&lt;/p&gt;

&lt;p&gt;Real-world ROCs are often messier, and may not have a continuously decreasing slope as you modify your threshold. But the ROC curve is always positively sloped, and never flat or vertical except at the edges. That would imply two points where one is better on one dimension and equal on the other. Then only one can be an optimal solution, and only that one should be on the ROC curve. Furthermore, most reasonable threshold selection methodologies we discuss below will tend to ignore points in regions that aren’t on the ROC’s outer ‘hull’ but are in local dips toward the bottom right. So thinking about a ROC curve with continuously decreasing slope is reasonable.&lt;/p&gt;

&lt;h2 id=&quot;4-precision-recall-and-the-f-score&quot;&gt;4. Precision, recall, and the F-score&lt;/h2&gt;

&lt;p&gt;Now, what happens if you can’t pin down the real-world cost of false positives and false negatives?&lt;/p&gt;

&lt;p&gt;A frequently used metric is the &lt;em&gt;F1-score&lt;/em&gt;, which is the harmonic mean of precision and recall.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;Precision&lt;/em&gt; is true positives/predicted positives, i.e. % of our positive predictions that we predicted correctly.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;Recall&lt;/em&gt; is TPR: true positives/ground truth positive, i.e. % of ground truth positives that we predicted correctly.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;To calculate the harmonic mean, we invert the inputs, calculate their mean, then invert the mean:&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

\[F_{1} = \frac{1}{\frac{1}{2}\frac{1}{precision} + \frac{1}{2}\frac{1}{recall}}\]

\[= \frac{2}{\frac{1}{precision} + \frac{1}{recall}}\]

\[= 2 \frac{precision \cdot recall}{precision + recall}\]

&lt;p&gt;To gain intuition about the harmonic mean: If a vehicle travels a distance &lt;em&gt;d&lt;/em&gt; outbound at a speed &lt;em&gt;x&lt;/em&gt; (&lt;em&gt;e.g.&lt;/em&gt; 60 &lt;em&gt;km/h&lt;/em&gt;) and returns the same distance at a speed &lt;em&gt;y&lt;/em&gt; (&lt;em&gt;e.g.&lt;/em&gt; 20 &lt;em&gt;km/h&lt;/em&gt;), then its average speed is the harmonic mean of &lt;em&gt;x&lt;/em&gt; and &lt;em&gt;y&lt;/em&gt; (&lt;em&gt;30 km/h&lt;/em&gt;).&lt;/p&gt;

&lt;p&gt;This is &lt;u&gt;*not*&lt;/u&gt; equal to the arithmetic mean (40 &lt;em&gt;km/h&lt;/em&gt;), which would be applicable if you traveled an equal &lt;em&gt;time&lt;/em&gt; in 2 different-speed legs.&lt;/p&gt;

&lt;p&gt;If speed is near 0 on either leg, the harmonic mean will be near 0 for the whole distance regardless of the speed on the other leg. If the speed is exactly 0 on either leg, you never get to your destination, so the average speed for the whole trip is 0.&lt;/p&gt;

&lt;p&gt;The F1-score balances precision and recall and penalizes extreme weakness in either one.&lt;/p&gt;

&lt;p&gt;The F1-score is &lt;em&gt;not symmetric&lt;/em&gt;. We have an unbalanced 79.2%/20.8% ratio of negatives/positives. Suppose our classifier predicts everything as positive. Then recall is 1, precision is 0.208, F1 is 0.344. Now suppose the labels are reversed, you predict everything as positive, recall is 1, precision is 0.792, F1 is 0.884. This looks great but it’s not, since all we did was classify everything as the majority class. Due to this asymmetry, we are always careful to &lt;em&gt;label the minority class as the positive class&lt;/em&gt; when we calculate the F1-score.&lt;/p&gt;

&lt;p&gt;Asymmetry is sometimes fine. Sometimes the business problem is asymmetric. In a search engine application, we don’t care much about all the documents we didn’t retrieve.&lt;/p&gt;

&lt;p&gt;We can tilt the threshold toward precision or recall by using a weighted mean instead of an equal-weighted mean, in which case we call it F2-score etc.&lt;/p&gt;

&lt;p&gt;F-score seems reasonable, although the applicability of the harmonic mean to threshold selection is not completely intuitive to me. I’ve never encountered a business problem where the real-world cost function of false positives and false negatives is a harmonic mean.&lt;/p&gt;

&lt;h2 id=&quot;5-isocurves-help-you-visualize-any-scoring-or-cost-function-like-f1&quot;&gt;5. Isocurves help you visualize any scoring or cost function like F1&lt;/h2&gt;

&lt;p&gt;If you want the best tradeoff, it helps to visualize the tradeoff with isocurves. An iso-cost curve or indifference curve depicts a set of points that have the same cost or metric value: for instance all the points with an F1-score of 0.5. This lets you visualize how the metric evolves as you move along the ROC curve.&lt;/p&gt;

&lt;p&gt;For the F1 metric, the isocurve plot looks like this.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 3. F1-score Isocurves&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2019/fig3.png&quot; alt=&quot;Figure 3. F1-score Isocurves&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Bottom axis: recall = 0, F1 = 0&lt;/li&gt;
  &lt;li&gt;Top left: we classify perfectly: recall = 1, precision = 1, F1 = 1&lt;/li&gt;
  &lt;li&gt;Top right: we classify all positive: recall = 1, precision = 0.208, F1 = 0.344&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To choose the point with the highest F1, pick the point on the ROC curve which sits on the best F1 isocurve, the isocurve closest to top left.&lt;/p&gt;

&lt;h2 id=&quot;6-linear-cost-isocurves&quot;&gt;6. Linear cost isocurves&lt;/h2&gt;

&lt;p&gt;F-score is a nonlinear cost function. If the cost function is a linear function of TPR and FPR (or false positives and false negatives), the relative cost to trade one false positive for one false negative is &lt;em&gt;constant&lt;/em&gt;, and you have evenly spaced linear isocurves.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 4. Linear Cost Isocurves&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2019/fig4.png&quot; alt=&quot;Figure 4. Linear Cost Isocurves&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Again, choose the point on the ROC curve which is also on the isocurve closest to the top left. In this example, the relative price is &lt;em&gt;p/(1-p)&lt;/em&gt;. So the isocurves parallel the 45º line, which corresponds to a random classifier using the base rate &lt;em&gt;p&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Comparing to F1 isocurves, as you go up, F1s get higher faster on the left. So as you improve your ROC, all else being equal, the path connecting best F1 points will curve toward the left (concave down). The path connecting the best linear costs will be a straight line. Proof, as they say, left as an exercise.&lt;/p&gt;

&lt;p&gt;We can weight the relative cost of false positives to false negatives. As we do, the slope of the isocurves changes, and we will tend to favor precision over recall or vice versa.&lt;/p&gt;

&lt;h2 id=&quot;7-circular-or-closest-to-top-left-isocurves&quot;&gt;7. Circular or ‘closest to top left’ isocurves&lt;/h2&gt;

&lt;p&gt;Suppose we want to be as close as possible to top left by Euclidean distance. That corresponds to circular isocurves. The cost function is the distance from the top left:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 5. Closest to Top Left: Circular Isocurves&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2019/fig5.png&quot; alt=&quot;Figure 5. Circular Isocurves&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We could also weight the terms under the square root to favor precision or recall, and squash the circular curves into elliptical curves.&lt;/p&gt;

&lt;p&gt;Compared to constant cost isocurves, the circular isocurves will tend to keep the optimal point toward the middle, near the diagonal ridge, balancing precision and recall.&lt;/p&gt;

&lt;h2 id=&quot;8-mutual-information&quot;&gt;8. Mutual information&lt;/h2&gt;

&lt;p&gt;Finally, let’s look at &lt;em&gt;mutual information&lt;/em&gt;. Mutual information is an information theory metric, like &lt;a href=&quot;https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a&quot;&gt;&lt;em&gt;log loss&lt;/em&gt;&lt;/a&gt;. Log loss can be interpreted as the wrongness, or surprise in our prediction vs. the actual outcomes, measured in bits. &lt;a href=&quot;https://en.wikipedia.org/wiki/Mutual_information&quot;&gt;&lt;em&gt;Mutual information&lt;/em&gt;&lt;/a&gt; can be interpreted as the amount of correct information in our prediction, measured in bits.&lt;/p&gt;

&lt;p&gt;Mutual information is what we use to measure the bandwidth capacity of a signal over a telephone wire or radio channel. We can view a prediction as a signal about the future. In the same sense that mutual information measures how much useful signal can be transmitted on a channel, mutual information tells us how much useful information we have about the future.&lt;/p&gt;

&lt;p&gt;If we use mutual information as our cost function, we get isocurves that look like this.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 6. Mutual Information Isocurves&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2019/fig6.png&quot; alt=&quot;Figure 6. Mutual Information Isocurves&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Maximizing mutual information, in an information theory sense, minimizes the bits of surprise or wrongness in the prediction, and maximizes the bits of ‘correct’ information. Absent explicit costs for false positives and false negatives, mutual information seems like a natural metric choice.&lt;/p&gt;

&lt;p&gt;However, mutual information isocurves match the general shape of the ROC curve. So they may not be very stable in terms of choosing whether to err on the side of precision or recall.&lt;/p&gt;

&lt;p&gt;The choice to lean toward precision or recall is based on whether the algorithm is better at precision or recall, as opposed to any real-world cost. So the choice of threshold might be arbitrary and hard to interpret.&lt;/p&gt;

&lt;p&gt;It’s also worth noting that a perfectly wrong prediction contains the same mutual information as a perfectly correct prediction because you could extract the correct prediction from that signal. This too is a counterintuitive.&lt;/p&gt;

&lt;p&gt;Mutual information has theoretical appeal but interpretability is challenging.&lt;/p&gt;

&lt;h2 id=&quot;9-concluding-remarks&quot;&gt;9. Concluding remarks&lt;/h2&gt;

&lt;p&gt;When I started building classification models, I frequently used F1 scores. Now, I generally use AUC for model selection and hyperparameter tuning. For threshold selection, I try to identify the most relevant real-world metric.&lt;/p&gt;

&lt;p&gt;I find linear isocurves easiest to interpret. In the absence of a good prior cost function I also find it easier to justify &lt;em&gt;p/(1-p)&lt;/em&gt; than F1 or the other more complex metrics. Circular isocurves are harder to interpret but favor a balance between precision and recall. But empirically F1 performs well.&lt;/p&gt;

&lt;p&gt;To choose a threshold, you have to maximize something. The metric you maximize should be &lt;em&gt;interpretable and correlated to real-world costs&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Takeaways&lt;/em&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Threshold selection is where data science meets real-world decision-making.&lt;/li&gt;
  &lt;li&gt;Don’t automatically use a 50% threshold or even F1-score; consider the real-world costs of false negatives and false positives.&lt;/li&gt;
  &lt;li&gt;To select a threshold you have to optimize for some metric: a &lt;em&gt;cost function&lt;/em&gt;.&lt;/li&gt;
  &lt;li&gt;The business context determines the cost function you are optimizing: the real-world tradeoff you are willing to make between type I and type II error; how far you should reduce your threshold to get more true positives at the expense of more false positives.&lt;/li&gt;
  &lt;li&gt;An isocurve contour plot is a visualization that provides a good intuition of your cost function whenever you have to trade off two variables.&lt;/li&gt;
  &lt;li&gt;When you minimize the cost function, you choose the optimal classification threshold where the ROC curve intersects the lowest cost (or highest metric) isocurve.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Isocurves can be applied to make rational choices between any set of competing alternatives, not just classification thresholds. This post covers about 30% of the &lt;a href=&quot;http://www.limericksecon.com/p/ten-limerick-principles-of-economics.html&quot;&gt;most important principles of economics&lt;/a&gt;: People face tradeoffs; The cost of something is what you have give up to get it; Rational people think at the margin; You have to choose what to maximize.&lt;/p&gt;

&lt;p&gt;Selecting thresholds that optimize KPIs and achieve business objectives maximizes your value as a data scientist…a truly optimal outcome.&lt;/p&gt;

&lt;p&gt;The code for the visualizations is on &lt;a href=&quot;https://github.com/druce/threshold_selection&quot;&gt;GitHub&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;see-also&quot;&gt;See also:&lt;/h3&gt;

&lt;p&gt;Fawcett, Tom. “ROC graphs: Notes and practical considerations for researchers.” Machine learning 31.1 (2004): 1–38. https://www.hpl.hp.com/techreports/2003/HPL-2003-4.pdf&lt;/p&gt;

&lt;p&gt;Swets, John A., et al. “Better DECISIONS through SCIENCE.” Scientific American, vol. 283, no. 4, 2000, pp. 82–87. JSTOR, &lt;a href=&quot;http://www.jstor.org/stable/26058901&quot;&gt;www.jstor.org/stable/26058901&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Grandenberger, Greg, &lt;a href=&quot;https://medium.com/shoprunner/https-medium-com-shoprunner-evaluating-classification-models-3-cabb9660ea5b&quot;&gt;Evaluating Classification Models, Part 3: Fᵦ and Other Weighted Pythagorean Means of Precision and Recall&lt;/a&gt;&lt;/p&gt;</content><author><name>Druce Vertes</name></author><category term="datascience" /><category term="datascience" /><summary type="html">As a data scientist, you might say…“A blog post about thresholds? It’s not even a data science problem, it’s more of a business decision.” And you would not be wrong! Threshold selection lacks the appeal of say, generative adversarial networks.</summary></entry><entry><title type="html">Why Blockchain Is (Mostly) Useless</title><link href="http://0.0.0.0:4000/2019/09/why-blockchain-is-mostly-useless/" rel="alternate" type="text/html" title="Why Blockchain Is (Mostly) Useless" /><published>2019-09-07T08:31:36-04:00</published><updated>2019-09-07T08:31:36-04:00</updated><id>http://0.0.0.0:4000/2019/09/why-blockchain-is-mostly-useless</id><content type="html" xml:base="http://0.0.0.0:4000/2019/09/why-blockchain-is-mostly-useless/">&lt;blockquote&gt;
  &lt;p&gt;&lt;em&gt;Cryptocurrencies are useless. They’re only used by speculators looking for quick riches, people who don’t like government-backed currencies, and criminals who want a black-market way to exchange money._ _ &lt;a href=&quot;https://www.schneier.com/blog/archives/2019/02/blockchain_and_.html&quot;&gt;Bruce Schneier&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;!--more--&gt;

&lt;p&gt;The &lt;a href=&quot;https://www.schneier.com/blog/archives/2019/02/blockchain_and_.html&quot;&gt;column&lt;/a&gt; from which this quote is lifted is well worth reading and thinking about. The key feature of cryptocurrencies and blockchain apps is the distributed nature of the ledger, the idea that you can trust the system without having to trust anyone in the real world.&lt;/p&gt;

&lt;p&gt;But eventually, the system has to interact with the real world, where you do have to trust somebody to accomplish anything, and have meta-governance of the system as a whole in the real world. In the real world, there is no such thing as a ‘trustless system.’&lt;/p&gt;

&lt;p&gt;And once you have to trust someone, in nearly all cases you might as well build a much simpler and more effective centralized system with an appropriate trust model.&lt;/p&gt;

&lt;p&gt;That statement is a bit abstract, so here is a constructive example of how one can offer the key advantages of blockchain and distributed ledger technology, without blockchain.&lt;/p&gt;

&lt;p&gt;(skip this part if you’re not into architecture astronautics)&lt;/p&gt;

&lt;p&gt;AWS now offers a &lt;a href=&quot;https://aws.amazon.com/qldb/faqs/&quot;&gt;centralized immutable ledger&lt;/a&gt;, an append-only database similar to the blockchain’s immutable ledger history of all transactions. The difference is, unlike the Bitcoin blockchain, which is distributed among all the Bitcoin miners and uses the consensus algorithm to resolve any conflicts and commit updates permanently, the centralized immutable ledger has one endpoint that processes requests and commits valid updates in the order they are received.&lt;/p&gt;

&lt;p&gt;Any sufficiently advanced database system has a language, like SQL stored procedures, to extend it with more complex business logic. Let’s add a language to our centralized database. Suppose the language is Python. And suppose we make the codebase immutable. You can add new code, but you can’t delete any old code.&lt;/p&gt;

&lt;p&gt;In other words you can create a Python app, like a restaurant reservation service with a REST API, or a Web UI, and write code that supports all the necessary operations, like &lt;em&gt;add_restaurant&lt;/em&gt;, &lt;em&gt;add_diner&lt;/em&gt;, &lt;em&gt;add_reservation&lt;/em&gt;, &lt;em&gt;cancel_reservation&lt;/em&gt;, etc. And then once you publish the app, that code can never be changed.&lt;/p&gt;

&lt;p&gt;If that sounds weird, imagine you store the code in an append-only structure, the way Git version control keeps every version ever committed. And every time you commit a new version of the code, the immutable ledger, where it stores restaurant data, is forked. So the app is immutable in the sense that you can create new versions which inherit the underlying data, but the old version always remains.&lt;/p&gt;

&lt;p&gt;Now Bitcoin runs as long as it has enough miners to keep the ecosystem going. For argument’s sake, suppose our hypothetical AWS service says, the app users split the hosting bill, and users commit to some maximum monthly cost, $1, $10, whatever, and AWS keeps it running as long as the cost of hosting is covered.&lt;/p&gt;

&lt;p&gt;There could be more complex models, premium tiers, profits directed to the developers or a foundation, but the key points are: no one can extract rents beyond the hosting cost or what is initially agreed; no one can change the software unless everyone migrates to the new fork; and AWS is the trusted third party that guarantees that.&lt;/p&gt;

&lt;p&gt;The way this is &lt;em&gt;similar&lt;/em&gt; to a blockchain is that once an app is published, it can’t be changed unless all the users agree to fork it. If the developers write something new, they can publish it, but as long as any users keep using the old system, it’s still around and no one can force anyone to change.&lt;/p&gt;

&lt;p&gt;(end architecture astronautics)&lt;/p&gt;

&lt;p&gt;This contract offers the benefits of a distributed ledger app that no single individual can own or fork. The essential way it’s &lt;em&gt;different&lt;/em&gt; from blockchain is that you have to trust AWS. And pay them instead of paying to run distributed mining rigs. And running one centralized server is far simpler, cheaper, more efficient, you can have instantaneous transactions at a high rate for low cost.&lt;/p&gt;

&lt;p&gt;A key benefit of a blockchain app is the elimination of certain agent-principal problems. Back in the day, there was an open-source app called CDDB. You put a CD in your PC, and CDDB would read it, look up a hash in a remote database and give your audio client all the metadata, artist, album titles, song titles. And if your CD wasn’t in the master database you could contribute metadata and everyone else could use it.&lt;/p&gt;

&lt;p&gt;Then one day the developer cashed out and sold the database for a few million. All the work that everyone had done maintaining the open-source database was captured, and the people who had created it now had to pay license fees to access it. Classic bait-and-switch: gain lock-in on false promise of a free service, make it painful to switch all the apps that used the CDDB API, then extract maximum rent.&lt;/p&gt;

&lt;p&gt;With our immutable application architecture, that debacle could never happen. Everyone could just keep using the old version of the database. No one can ever start charging for access to the data, or change the deal à la Darth Vader, because no one individual has control. And that’s the primary benefit of blockchain for a lot of use cases. It’s an open-source cloud architecture, anyone can improve on it and fork it, but no one person can control it or fence in the commons.&lt;/p&gt;

&lt;p&gt;But eventually, you have to trust somebody where the ‘trustless’ part of the system interacts with the real world. The minute you are trading with a broker/exchange, or buying something with Bitcoin, you have to trust that the exchange won’t get hacked or the seller won’t scam your money, or the restaurant where you made a reservation won’t give your table away or shut down.&lt;/p&gt;

&lt;p&gt;Logically, if you trust someone like Amazon for e.g. payments/fulfillment at the edge of the system, then you lose nothing extending trust to having them run the blockchain infrastructure, and then you lose nothing if they build a high-performing centralized system with the same functionality but without all the blockchain hassles.&lt;/p&gt;

&lt;p&gt;As long as you trust AWS to be an honest broker and charge only reasonable hosting, and never try to extract the value of the underlying app, you don’t really care about distributed ledger vs. centralized ledger, you just want it to work. And centralized will always be simpler and have better throughput than distributed; it just works better. And if you can trust AWS and the real-world legal system, you can write a contract and governance structure that provides the benefits of a distributed ledger.&lt;/p&gt;

&lt;p&gt;Blockchain is &lt;em&gt;almost&lt;/em&gt; always useless. The exception is where you can’t trust AWS. More to the point, where you can’t trust the government. If you want to create an app that the government may want to block, collect taxes on, enforce capital controls, protect government monopolies, censor forms of expression, then you can never trust a central nexus. The government can show up and seize servers, shut it down and go after the users.&lt;/p&gt;

&lt;p&gt;But effective strong states can maybe block distributed ledger services too. China has shown that they can do a pretty good job of policing the decentralized Internet. Few things are so decentralized that a strong government can’t drive it deep underground. The US banned private gold ownership in 1933 and it was mostly effective. People could still have gold necklaces and maybe trade using gold jewelry, but without coins and bars it was no longer economical to store and exchange value on a large scale. If the motivation exists, governments can crack down on anything.&lt;/p&gt;

&lt;p&gt;In my opinion Bitcoin will never be a legitimate, largely unregulated medium of exchange for the masses in the US. There’s too much at stake for the government not to control it. It’s too important for the US government to maintain control of the payments system for taxation, seignorage (ability to print money), financial stability (FDIC, intermediaries that don’t go bust because someone hacked and made off with the money), and national security (i.e. leveraging the world’s dominant currency for political purposes).&lt;/p&gt;

&lt;p&gt;And the US is still a very high-trust society, so there’s not a lot of reason for most people to use a black-market type of payment system, without the benefit of the government to enforce it and backstop intermediaries with e.g. FDIC insurance.&lt;/p&gt;

&lt;p&gt;In a nutshell, the US government’s technical means and motive to regulate Bitcoin (or Libra or whatever) exceeds the average consumer’s means and motive to circumvent the US government.&lt;/p&gt;

&lt;p&gt;But this isn’t true everywhere. You can draw a 2×2 matrix:&lt;/p&gt;

&lt;div&gt;
  &lt;table style=&quot;font-family:arial, helvetica, sans-serif; font-size: 13px; border: 2px solid #FFFFFF; width: 100%; text-align: center; border-collapse: collapse; &quot;&gt;
    &lt;tr&gt;
      &lt;th&gt;
        &amp;nbsp;
      &lt;/th&gt;
      
      &lt;th&gt;
        Strong State&lt;br /&gt;Able to repress crypto
      &lt;/th&gt;
      
      &lt;th&gt;
        Weak State&lt;br /&gt;Unable to repress crypto
      &lt;/th&gt;
    &lt;/tr&gt;
    
    &lt;tr&gt;
      &lt;th&gt;
        High trust society&lt;br /&gt;Low demand for crypto
      &lt;/th&gt;
      
      &lt;td style=&quot;border: solid 2px black; background-color: #c9edff;&quot;&gt;
        USA&lt;br /&gt; Europe&lt;br /&gt; Japan
      &lt;/td&gt;
      
      &lt;td style=&quot;border: solid 2px black; background-color: #c9edff;&quot;&gt;
        ??? Island paradises ??? &lt;br /&gt;Micronesia? &lt;br /&gt;Bhutan?
      &lt;/td&gt;
    &lt;/tr&gt;
    
    &lt;tr&gt;
      &lt;th&gt;
        Low trust society&lt;br /&gt;High demand for crypto
      &lt;/th&gt;
      
      &lt;td style=&quot;border: solid 2px black; background-color: #c9edff;&quot;&gt;
        China &lt;br /&gt; North Korea
      &lt;/td&gt;
      
      &lt;td style=&quot;border: solid 2px black; background-color: #c9fc9d;&quot;&gt;
        Venezuela &lt;br /&gt; Somalia
      &lt;/td&gt;
    &lt;/tr&gt;
  &lt;/table&gt;
  
  &lt;p&gt;
    &amp;nbsp; &amp;lt;/div&amp;gt; 
    
    &lt;p&gt;
      I think only the Venezuelas of the world, with low trust in local currencies and financial institutions, which are also weak states hard-pressed to universally enforce their will, will see significant adoption of Bitcoin. And maybe gold and USD may be more practical for black markets.
    &lt;/p&gt;
    
    &lt;p&gt;
      Satoshi Nakamoto’s paper and Bitcoin launch happened in 2008…over 10 years ago. The iPhone was launched in 2007. If blockchain really is a platform that will change everything, it’s a real sleeper success story. Typically you see killer apps on a new platform a lot quicker. Web 1.0 launched Amazon and Netflix within the first couple of years. Where are all the blockchain apps?
    &lt;/p&gt;
    
    &lt;p&gt;
      The only industries that have really been impacted are ransomware, money laundering, and facilities for exchanging and speculating on tokens.
    &lt;/p&gt;
    
    &lt;p&gt;
      There are unloved monopoly ledger companies like OpenTable and Ticketmaster. Where has any centralized application actually been disrupted? If distributed ledger technology can’t disrupt Ticketmaster, what use is it?
    &lt;/p&gt;
    
    &lt;p&gt;
      My best guess is that blockchain and Bitcoin adoption will remain a curiosity and a niche phenomenon linked to black markets, illicit activities, weak states with unreliable payments and money.
    &lt;/p&gt;
    
    &lt;p&gt;
      And blockchain apps will migrate to more efficient centralized systems that use governance and trust architectures to offer benefits similar to distributed ledgers…for average Joes who have no choice but to trust the government.
    &lt;/p&gt;
    
    &lt;p&gt;
      TL; DR Anything you can do with blockchain you can do better without it. Except maybe in low-trust environments, but if the reason for low trust is strong state enemies, they can probably repress blockchain applications they don’t like. Eventually you have to trust somebody. So might as well pick the right parties to trust, and build applications people love.
    &lt;/p&gt;
    
    &lt;p&gt;
      &lt;em&gt;Bury it in the desert. Wear gloves. _ &lt;a href=&quot;https://xkcd.com/2030/&quot;&gt;xkcd&lt;/a&gt;&lt;/em&gt;
    &lt;/p&gt;
&lt;/p&gt;&lt;/div&gt;</content><author><name>Druce Vertes, CFA</name></author><category term="Uncategorized" /><category term="bitcoin" /><category term="blockchain" /><summary type="html">Cryptocurrencies are useless. They’re only used by speculators looking for quick riches, people who don’t like government-backed currencies, and criminals who want a black-market way to exchange money._ _ Bruce Schneier</summary></entry><entry><title type="html">There ain’t no such thing as a free option</title><link href="http://0.0.0.0:4000/2019/06/there-aint-no-such-thing-as-a-free-option/" rel="alternate" type="text/html" title="There ain’t no such thing as a free option" /><published>2019-06-07T17:56:13-04:00</published><updated>2019-06-07T17:56:13-04:00</updated><id>http://0.0.0.0:4000/2019/06/there-aint-no-such-thing-as-a-free-option</id><content type="html" xml:base="http://0.0.0.0:4000/2019/06/there-aint-no-such-thing-as-a-free-option/">&lt;blockquote&gt;
  &lt;p&gt;&lt;em&gt;I would not give a fig for the simplicity this side of complexity, but I would give my life for the simplicity on the other side of complexity. - Oliver Wendell Holmes&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;!--more--&gt;

&lt;p&gt;Keep it simple, stupid - Anonymous&lt;/p&gt;

&lt;p&gt;This post is motivated by this &lt;a href=&quot;https://www.nytimes.com/2019/06/06/climate/trump-auto-emissions-rollback-letter.html&quot;&gt;story of automakers up in arms about Trump deregulating emissions standards&lt;/a&gt;. Surely reducing regulation cannot harm automakers? What would that say about free markets?&lt;/p&gt;

&lt;p&gt;Absent the regulatory rollback, manufacturers have to make low-polluting vehicles. If the Feds deregulate, they have a choice of making low-polluting or high-polluting vehicles. And in any optimization problem, removing or easing constraints can only improve an outcome. Why would they prefer an outcome that doesn’t make them better off?&lt;/p&gt;

&lt;p&gt;Here’s the rub: California will keep stringent standards. Some manufacturers will produce high polluting vehicles and exit California, where the market will be less competitive and car prices may increase. The market will be split into California-legal and non-California-legal. People on the border will register cars with their friends in Oregon. There will be multiple regulatory regimes and an unholy mess, and the likelihood that future administrations will tinker with it.&lt;/p&gt;

&lt;p&gt;It would be better for everyone, including the car manufacturers, if everyone could find a way to agree on the same standard.&lt;/p&gt;

&lt;p&gt;Free-market fetishists are going to say, well, that’s an artifact of crazy regulation, a free option can only be a good thing, and if you give people freedom of choice, they will choose the right solution. WRONG! Time for some game theory…&lt;/p&gt;

&lt;p&gt;Consider &lt;a href=&quot;https://en.wikipedia.org/wiki/Braess%27s_paradox&quot;&gt;Braess’s Paradox&lt;/a&gt;. You have a road network and 4000 cars traveling from START to END over 2 different routes, say on either side of a body of water. Initially, the equilibrium is 2000 cars over each route and a travel time of 65 minutes. (Traffic/100 = 20 minutes + 45 minutes.) It’s a Nash equilibrium because no one has an incentive to switch. If for any reason more people take one route, it slows down and people have an incentive to switch back to the faster route.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/assets/wp-content/uploads/2019/06/1000px-Braess_paradox_road_example.svg_.png&quot;&gt;&lt;img src=&quot;/assets/wp-content/uploads/2019/06/1000px-Braess_paradox_road_example.svg_-300x78.png&quot; alt=&quot;&quot; width=&quot;600&quot; height=&quot;156&quot; class=&quot;alignright size-medium wp-image-6845&quot; srcset=&quot;/assets/wp-content/uploads/2019/06/1000px-Braess_paradox_road_example.svg_-300x78.png 300w, /assets/wp-content/uploads/2019/06/1000px-Braess_paradox_road_example.svg_-768x199.png 768w, /assets/wp-content/uploads/2019/06/1000px-Braess_paradox_road_example.svg_.png 1000w&quot; sizes=&quot;(max-width: 600px) 100vw, 600px&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;(&lt;a href=&quot;https://en.wikipedia.org/wiki/Braess%27s_paradox&quot;&gt;via Wikipedia&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;Then, we add a bridge between A and B. Suppose it’s instantaneous to keep things simple. You can even suppose it goes both ways, but no one will choose the 90 minute option.&lt;/p&gt;

&lt;p&gt;Initially, it’s faster to take the connection (40 minutes). But eventually, too many people take it. What happens when 3300 people take the new route? The time is 66 minutes, slower than before! And the people going along the other routes are even slower so they switch too and slow it down further! Amazingly, the new Nash equilibrium is 80 minutes, everyone takes the bridge.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;4000/100 + 4000/100 = 80 minutes (4000 people)&lt;/li&gt;
  &lt;li&gt;45 + 4000/100 = 85 minutes (0 people)&lt;/li&gt;
  &lt;li&gt;4000/100 = 85 minutes (0 people)&lt;/li&gt;
  &lt;li&gt;45 + 45 = 90 minutes (0 people)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Adding this route option cost everyone 15 minutes. (If you add more people, eventually above 4500 people all the new people switch to the 90 minute route. If you don’t follow or want to know more, read the &lt;a href=&quot;https://en.wikipedia.org/wiki/Braess%27s_paradox&quot;&gt;Wikipedia article&lt;/a&gt;, also &lt;a href=&quot;https://ncase.me/trust/&quot;&gt;Evolution of Trust&lt;/a&gt; is the most awesomest intro to game theory ever.)&lt;/p&gt;

&lt;p&gt;The thing is, &lt;em&gt;changing the rules changes the game, which can change the whole equilibrium.&lt;/em&gt; The law of unintended consequences. Letting everyone act freely according to their own best interest does not lead to the best outcome. You also need to set the game up right, with a market design that is engineered to be fit for purpose. (And even then, &lt;a href=&quot;https://plato.stanford.edu/entries/arrows-theorem/&quot;&gt;no guarantees of optimal outcome&lt;/a&gt;). In this case, everyone should agree to blow up that bridge between A and B.&lt;/p&gt;

&lt;p&gt;There ain’t no such thing as a totally free market. You have to come up with a market design that achieves the desired objectives. If you choose not to decide, and let the market be designed to protect the right of the stronger instead of other objectives, you still have made a choice. There is no &lt;a href=&quot;https://www.econlib.org/library/Essays/LtrLbrty/bryTSO.html&quot;&gt;spontaneous order&lt;/a&gt; except that which sensible people work very hard to engineer. (See also &lt;a href=&quot;/2017/01/why-do-tech-folks-identify-as-libertarian-and-further-strange-musings/&quot;&gt;this&lt;/a&gt; on Silicon Valley pseudo-libertarianism.)&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/assets/wp-content/uploads/2019/06/Cortes.png&quot;&gt;&lt;img src=&quot;/assets/wp-content/uploads/2019/06/Cortes.png&quot; alt=&quot;&quot; width=&quot;260&quot; height=&quot;194&quot; class=&quot;alignright size-full wp-image-6853&quot; /&gt;&lt;/a&gt; Apocryphally, Cortés burned his ships so he would not have the option to retreat. The history is &lt;a href=&quot;https://www.reddit.com/r/AskHistorians/comments/21hoe1/did_cortes_really_order_his_own_ships_burnt_did/&quot;&gt;more complicated&lt;/a&gt;, but &lt;a href=&quot;https://www.youtube.com/watch?reload=9&amp;amp;v=3-XEVC00szg&quot;&gt;eliminating your own options can be a winning strategy&lt;/a&gt;. If you are in a game of chicken with a maniac, both of you driving toward each other at 100 mph, the one who throws the steering wheel out the window first usually wins, by credibly eliminating the option to swerve.&lt;/p&gt;

&lt;p&gt;In college I took a course from Seymour Melman (Columbia’s Noam Chomsky) where I had to read the nuclear doomsday theorists, Herman Kahn, George Kennan and whatnot. I was a little shook up when I read about &lt;a href=&quot;https://www.armscontrolwonk.com/archive/402503/mirvs-and-remorse-sort-of/&quot;&gt;Kissinger saying&lt;/a&gt; &lt;em&gt;“I would say in retrospect that I wish I had thought through the implications of a MIRVed world more thoughtfully in 1969 and 1970 than I did.”&lt;/em&gt; and &lt;em&gt;“In retrospect, I think if one could have avoided the development of MIRVs, which means also the testing of MIRVs by the Soviets, we would both be better off.”&lt;/em&gt; You had one job, Henry. You’re not supposed to &lt;a href=&quot;https://theintercept.com/2016/02/12/henry-kissingers-war-crimes-are-central-to-the-divide-between-hillary-clinton-and-bernie-sanders/&quot;&gt;have a conscience&lt;/a&gt; but you’re supposed to understand strategy FFS.&lt;/p&gt;

&lt;p&gt;The thing about the MIRV (Multiple Independent Rentry Vehicle) is … suppose each side has 1000 missiles and they have 50% effectiveness. One side launches all its missiles, catches the other side napping… and only destroys 50% of the other side’s missiles, while using all of its own.&lt;/p&gt;

&lt;p&gt;Now suppose you have 10 warheads on each missile. The side that launches first destroys all of the opponent’s missiles with only half of its own.&lt;/p&gt;

&lt;p&gt;The Nash equilibrium shifts from, &lt;em&gt;‘nobody launches missiles’&lt;/em&gt;, to &lt;em&gt;‘everyone launch your missiles first, or immediately without fail at any detection of launch from the other side.’&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;If the &lt;a href=&quot;https://www.oshonews.com/2013/09/09/henry-kissinger/&quot;&gt;schmartest man in the world&lt;/a&gt; can screw this up, what hope is there for the rest of us?&lt;/p&gt;

&lt;p&gt;You can’t just assume that giving yourself more options is a good thing. You build a bomb because the Nazis might do it first. You use it because it might save millions of lives compared to an invasion. You build a powerful military because the world is a dangerous place, you don’t want Germany and Japan re-arming, and an overwhelmingly superior military in the hands of a stable democracy is a great thing for global security. Then one day you have a Điện Biên Phủ and a commander in chief can be faced a very tough choice. You have to do everything in your power to save American lives, right? Even at the cost of escalation and possible retaliation down the road, right? Or one day a stable Western democracy isn’t quite so stable, you have people in charge who say, what’s the point of having nuclear if you can’t use it to get what you want? A no-first-use commitment seems like a good idea, but then the other side has more tanks … it all gets very complicated.&lt;/p&gt;

&lt;p&gt;It pays to simplify.&lt;/p&gt;

&lt;p&gt;To make the right move, you have to understand the game, the meta-game, the game beyond the meta-game. You need second-level thinking to succeed, you need to think strategically. You also want positive convexity, situations that that have the potential to go really really well but don’t cost you much downside. (Think the poker equivalent of calling from the big blind with 6-7 suited).&lt;/p&gt;

&lt;p&gt;But in poker, in investing, in life, you also need keep it simple, stupid. You are often better off limiting your options. Even if you think you’re the smartest player at the table, you want to avoid marginal situations, where you may have to make a big decision in an unclear situation. And if you’re not the smartest or most confident person at the table, the people who are will force you to make a very tough decision at a time when they have the edge.&lt;/p&gt;

&lt;p&gt;Reality is too complex, best-laid plans of mice and men, a pound of principle is worth a ton of guile.&lt;/p&gt;

&lt;p&gt;And IMHO Western ethics are mostly strategic thinking carried to their ultimate conclusion. If you are strategic but your goal is in the Kantian sense to do what would be best if everyone did it, strategic thinking is indistinguishable from altruism. In an iterated game, on a long enough time horizon, the most altruistic is the most strategic.&lt;/p&gt;

&lt;p&gt;Ethics = strategic thinking + love. If you care so much about your fellow players that their payoff is your payoff, you get to altruism.&lt;/p&gt;

&lt;p&gt;In the real world, people are boundedly strategic and boundedly rational, but the more we can build market designs and institutions that unite the two, the better off we will be.&lt;/p&gt;

&lt;p&gt;If you want cooperation, security, stability, you need strength but also honesty and clarity of purpose and communication, looking for win-win situations… you don’t go to the mat against friends for small victories. What goes around, comes around. Sow the wind, reap the whirlwind. But that is a topic for another day.&lt;/p&gt;</content><author><name>Druce Vertes, CFA</name></author><category term="economics" /><category term="economics" /><summary type="html">I would not give a fig for the simplicity this side of complexity, but I would give my life for the simplicity on the other side of complexity. - Oliver Wendell Holmes</summary></entry><entry><title type="html">On the end of the StreetEYE experiment.</title><link href="http://0.0.0.0:4000/2019/03/6809/" rel="alternate" type="text/html" title="On the end of the StreetEYE experiment." /><published>2019-03-31T14:56:05-04:00</published><updated>2019-03-31T14:56:05-04:00</updated><id>http://0.0.0.0:4000/2019/03/6809</id><content type="html" xml:base="http://0.0.0.0:4000/2019/03/6809/">&lt;blockquote&gt;
  &lt;p&gt;&lt;em&gt;The StreetEYE news aggregator experiment came to an end on 3/31/2019. Many thanks for supporting StreetEYE over the years!&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;!--more--&gt;

&lt;p&gt;A few people have asked about alternatives.&lt;/p&gt;

&lt;p&gt;&lt;a&gt;&lt;img class=&quot;alignright size-medium wp-image-6803&quot; src=&quot;/assets/wp-content/uploads/2019/03/Screen-Shot-2019-03-31-at-Mar-31-2019-3.29.29-PM-203x300.png&quot; alt=&quot;&quot; width=&quot;203&quot; height=&quot;300&quot; srcset=&quot;/assets/wp-content/uploads/2019/03/Screen-Shot-2019-03-31-at-Mar-31-2019-3.29.29-PM-203x300.png 203w, /assets/wp-content/uploads/2019/03/Screen-Shot-2019-03-31-at-Mar-31-2019-3.29.29-PM-768x1132.png 768w, /assets/wp-content/uploads/2019/03/Screen-Shot-2019-03-31-at-Mar-31-2019-3.29.29-PM-694x1024.png 694w, /assets/wp-content/uploads/2019/03/Screen-Shot-2019-03-31-at-Mar-31-2019-3.29.29-PM.png 940w&quot; sizes=&quot;(max-width: 203px) 100vw, 203px&quot; /&gt;&lt;/a&gt;The closest alternative is Nuzzel. Nuzzel is essentially what Twitter Moments should have been. See the Nuzzel page for &lt;a href=&quot;https://nuzzel.com/TheLinkfest?sort=friends&quot;&gt;TheLinkfest&lt;/a&gt;, log in with your own Twitter account, scroll down to ‘news from’.&lt;/p&gt;

&lt;p&gt;Or follow some or all the top people &lt;a href=&quot;https://twitter.com/streeteye&quot;&gt;@StreetEYE&lt;/a&gt; follows (or &lt;a href=&quot;https://twitter.com/TheLinkfest&quot;&gt;@TheLinkfest&lt;/a&gt;, the short list) _ &lt;a href=&quot;http://media.streeteye.com/static/fintwit201901/&quot;&gt;browse them with our visualization tool here&lt;/a&gt; _ and make your own custom newsfeed.&lt;/p&gt;

&lt;p&gt;StreetEYE used an algo to find people to follow, and that part is missing, along with a bit of spam-filtering, but the Nuzzel experience is very good.&lt;/p&gt;

&lt;p&gt;A few people asked about the reasons for shutdown. Answer: lack of traction, technical debt, platform shenanigans (sites blocking StreetEYE from spidering/scraping, reducing API limits). It needed a reboot but it really didn’t have enough traction to justify it. Also, the platforms might have killed it off regardless. It was always playing in someone else’s garden.&lt;/p&gt;

&lt;p&gt;Why now and not a few years ago? Because I loved the FinTwit community.&lt;/p&gt;

&lt;p&gt;I hope StreetEYE let FinTwit find people to follow, learn from, build professional relationships with, and brought a smart audience to the best news stories, publishers and social accounts. Thanks especially to people who reshared and evangelized it.&lt;/p&gt;

&lt;p&gt;If StreetEYE let people find news that mattered, credit belongs to the people who created and shared stuff that mattered. If it didn’t, sorry! Regrets to everyone who didn’t like being aggregated, thought it followed the wrong people, blocked me for being a jerk, or otherwise thought it sucked.&lt;/p&gt;

&lt;p&gt;I believe it was Clay Shirky who said, if your business model involves big expensive buildings with Greek columns, there’s a good chance you are about to be disrupted. Because fundamentally businesses like that are about trust and an aura of exclusivity, and the Internet allows new social and trust hierarchies.&lt;/p&gt;

&lt;p&gt;But a lot of incumbents that maybe seem ripe for disruption aren’t cooperating with that theory: Harvard, Bloomberg, Goldman Sachs.&lt;/p&gt;

&lt;p&gt;In fact new entrants are building big trophy buildings, Bloomberg, GS. Web companies are hiring starchitects, paying for naming rights, building brick-and-mortar presence as an attractive customer acquisition proposition.&lt;/p&gt;

&lt;p&gt;One of the things Harvard, Bloomberg, GS, YC have in common is they are all essentially social networks, with rites of admission, imprimatur of approval. There’s a lot of proof of work in &lt;a href=&quot;https://www.eugenewei.com/blog/2019/2/19/status-as-a-service&quot;&gt;Eugene Wei’s words&lt;/a&gt;, if you are an employee or even a mere paying client.&lt;/p&gt;

&lt;p&gt;And they have been disrupted, in the sense that if you look at the people who are there today vs. 30 years ago, they are a lot smarter, harder-working, tougher. A lot more people know how the sausage is made, and don’t just buy something because Merrill said so.&lt;/p&gt;

&lt;p&gt;It’s a better world. Some of the monopoly rents may still be there, but a lot of the prestige has gone out, and it’s a lot more competitive. Look at research reports from the early 1980s, good grief.&lt;/p&gt;

&lt;p&gt;Social status is a classic bias/variance problem. If you get a Harvard MBA or Ph.D. your proof of work is the ability to jump through academic hoops over the long haul, and look and sound a certain way. That leads to strong biases in the types of people who successfully navigate them, or &lt;a href=&quot;https://www.nytimes.com/2018/05/21/books/review/bad-blood-john-carreyrou.html&quot;&gt;exploit them, like Elizabeth Holmes&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Disruption, new social networks, allow people who don’t fit the mold rise to the top. But things can go to the other extreme. When you have social network ‘influencers’ who do it all on their own, sometimes they have a lot to offer, sometimes very little.&lt;/p&gt;

&lt;p&gt;For a functioning society, you want status indicators and ‘proof of work’ that allow new people to rise to the top. But institutions also have to select people who work hard, have skills, decency, judgment. Criteria developed over centuries, like nobility, or all their white male ancestors went to Harvard, aren’t fit for purpose when the world is rapidly changing.&lt;/p&gt;

&lt;p&gt;But there’s definitely an adverse selection problem that social networks haven’t really figured out, how to make the good stuff rise to the top, filter the noise, and also let new voices be heard. And that’s something I tried to work on.&lt;/p&gt;

&lt;p&gt;Marty Baron who edits the Washington Post is a 1000x curator. He’s a rare breed, and there are a lot of great curators out there. Social media doesn’t really reward the great curators. On Reddit, if you’re first to post something that gets traction, you get a lot of karma. You don’t get any karma for voting early for quality winners. And quality curators don’t get more influence.&lt;/p&gt;

&lt;p&gt;We have an Idiocracy problem, and 0.1x curators drown out the Marty Barons with cat pictures. Or worse, Jacob Wohl or the Infowars guy with manufactured viral nonsense.&lt;/p&gt;

&lt;p&gt;(I hasten to add, Reddit lets good stuff float to the top and kills a lot of bad stuff and has proved resilient where its Usenet ancestor collapsed. But &lt;a href=&quot;https://slatestarcodex.com/2014/12/17/the-toxoplasma-of-rage/&quot;&gt;toxoplasma of rage&lt;/a&gt; ends up being what it’s famous for.)&lt;/p&gt;

&lt;p&gt;Anyway, I think that’s a big challenge, and &lt;a href=&quot;https://qz.com/1071749/bridgewater-associates-ceo-ray-dalio-explains-the-dot-collector-feedback-tool-his-company-uses-to-rate-employees/&quot;&gt;Ray Dalio&lt;/a&gt; is onto something important. We need to figure out what real trust looks like in the social media age. We need better filters that identify quality content and curators. And I think StreetEYE actually worked pretty well most of the time.&lt;/p&gt;

&lt;p&gt;Another challenge is the user experience. Being ‘too online’ is a little crazy-making.&lt;/p&gt;

&lt;p&gt;There’s too much stuff. There’s a time dimension, a topic/context dimension, and a quality dimension. I think this UX trilemma screws it all up. On Reddit, if you post a brilliant reply deep in a thread, it gets buried despite being high quality. And on Twitter, the algorithmic feed surfaces tweets out of context when they get a lot of engagement. Sometimes they get totally misunderstood and ratioed, or worse. So the ideal tweet becomes this self-contained &lt;a href=&quot;https://twitter.com/eugenewei/status/998714814749802496?lang=en&quot;&gt;‘fortune cookie’&lt;/a&gt;. The medium truly is the message and people adapt to it. And in a lot of cases it’s completely crazy-making, toxic. &lt;a href=&quot;https://www.ted.com/talks/zeynep_tufekci_we_re_building_a_dystopia_just_to_make_people_click_on_ads?language=en&quot;&gt;We are building a dystopia just to make people click on ads&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Anyway, software is eating the world, and a lot of the social experience is mediated by these devices, and we will need to get better at figuring it out. AI is amazing and algorithmic feeds seem to be how the world is going.But I wonder if it might be time for some of those weird sci-fi 3D user interface visualizations to navigate social, and the massive amounts of online info and complex relationships.&lt;/p&gt;

&lt;p&gt;Looking forward, at one end of the spectrum you have crypto, which is kind of about making do without trust. I think it’s useful in some cases, like Venezuela, or some coordination problems where you don’t want a central authority and all the agent-principal problems that go with it. But mostly I think the idea of a trustless society is completely dystopic.&lt;/p&gt;

&lt;p&gt;At the other end you have a surveillance state, where we are all watched over by central machines of loving grace. Social combines the worst of 1984 and Brave New World. We don’t just have Siri and telescreens in our bedrooms, we carry them everywhere and are addicted to them. And then the algos rank our ‘social credit.’&lt;/p&gt;

&lt;p&gt;I think innovation that finds great content and people, connects them and builds high-trust communities is possible. It made FinTwit possible. But I think we kind of messed it up. I have no doubt the next iteration, maybe Social 2.0, will be better than the last.&lt;/p&gt;

&lt;p&gt;If you want to build something great, the first step is to build something that sucks a little. I think that’s true for StreetEYE, and for social in general. May all our next iterations be better!&lt;/p&gt;

&lt;p&gt;Mostly, many thanks for being part of this experiment, and look forward to continuing to interact in other ways online, and in person!&lt;/p&gt;</content><author><name>Druce Vertes, CFA</name></author><category term="tech" /><category term="tech" /><summary type="html">The StreetEYE news aggregator experiment came to an end on 3/31/2019. Many thanks for supporting StreetEYE over the years!</summary></entry><entry><title type="html">How I learned to stop worrying and love quantitative tightening</title><link href="http://0.0.0.0:4000/2019/02/how-i-learned-to-stop-worrying-and-love-quantitative-tightening/" rel="alternate" type="text/html" title="How I learned to stop worrying and love quantitative tightening" /><published>2019-02-04T12:29:12-05:00</published><updated>2019-02-04T12:29:12-05:00</updated><id>http://0.0.0.0:4000/2019/02/how-i-learned-to-stop-worrying-and-love-quantitative-tightening</id><content type="html" xml:base="http://0.0.0.0:4000/2019/02/how-i-learned-to-stop-worrying-and-love-quantitative-tightening/">&lt;blockquote&gt;
  &lt;p&gt;&lt;em&gt;Many people are talking about ‘quantitative tightening’ and ‘balance sheet reduction’, and some people are blaming it for market volatility, discussed &lt;a href=&quot;https://www.nytimes.com/2019/01/30/business/fed-quantitative-tightening.html&quot;&gt;here&lt;/a&gt;, &lt;a href=&quot;https://www.bloomberg.com/opinion/articles/2019-01-29/the-fed-s-balance-sheet-is-a-misunderstood-policy-tool&quot;&gt;here&lt;/a&gt;, &lt;a href=&quot;https://www.wsj.com/articles/fed-officials-weigh-earlier-than-expected-end-to-bond-portfolio-runoff-11548412201&quot;&gt;here&lt;/a&gt;. IMHO, blaming balance sheet reduction for market volatility is cargo cult mumbo jumbo.&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;!--more--&gt;

&lt;p&gt;What is QT? During the crisis, a lot of bad loans were made and a lot of risky bonds got issued. When the market had its Minsky moment in 2008, everybody simultaneously realized they had too many risky assets and needed to adjust portfolios. When everyone tried to sell at once, prices overcorrected. Worse, there was the risk of a debt-deflation spiral, where the financial crisis tanks the economy, making even more mortgages and bonds unsound, in turn further worsening the crisis and tanking the economy.&lt;/p&gt;

&lt;p&gt;Driving base rates to zero seemed inadequate to the task of restoring equilibrium. So the Fed stepped in with quantitative easing: buying the securities no one else wanted to own, taking toxic assets off of private sector balance sheets onto its own balance sheet. Effectively out of the system.&lt;/p&gt;

&lt;p&gt;By pegging overnight rates at 0, committing to keeping them at 0 for an extended period, and buying at the long end, the Fed was expanding the zero interest rate policy (ZIRP) down the yield curve.&lt;/p&gt;

&lt;p&gt;Today, when the Fed has stopped doing QE, every day the portfolio on the balance sheet shortens by a day, and some of it rolls off of the short end.&lt;/p&gt;

&lt;p&gt;When a bond on the Fed’s balance sheet matures, it’s (sort of) the reverse of QE, so some people call it quantitative tightening. (A more exact opposite of QE would be selling long-term securities into the market.) The maturity is a forced sale of an overnight obligation. The maturing bond used to be a long-term instrument, but now it has rolled down the yield curve so it’s short-term. The Fed gets cash, the bond obligation is canceled.&lt;/p&gt;

&lt;p&gt;It’s a little like when a bond matures in your brokerage account, broker calls and ask what you want to do.&lt;/p&gt;

&lt;p&gt;Maybe you say, roll it into a T-bill or money market fund, which is the most neutral thing to do.&lt;/p&gt;

&lt;p&gt;Technically, when the bond matured, it changed your positioning.&lt;/p&gt;

&lt;p&gt;But the maturity didn’t constrain your position at any time, you could offset it any way you wanted, you could sell it, swap it. Rolling into another short-term position would be the most neutral action.&lt;/p&gt;

&lt;p&gt;It’s the same for the Fed. In the Fed’s case the maturity drained liquidity. The market lost a cash asset when it paid off the bond, and lost a corresponding cash obligation when the bond was paid off.&lt;/p&gt;

&lt;p&gt;The maturity reduces the Fed’s balance sheet. But doesn’t constrain monetary policy in any way. The Fed pegs base rates like Fed Funds. Every day it can add or drain liquidity to keep short rates close to target.&lt;/p&gt;

&lt;p&gt;So, to sum up: QT is like a scheduled sale of a short-term security.&lt;/p&gt;

&lt;p&gt;And short rates are pegged.&lt;/p&gt;

&lt;p&gt;So QT doesn’t affect short rates. The Fed will do whatever is necessary to maintain the peg, including adding liquidity to offset any drain from QT.&lt;/p&gt;

&lt;p&gt;And QT doesn’t really affect long rates.&lt;sup&gt;&lt;small&gt;&lt;a href=&quot;#1&quot;&gt;1&lt;/a&gt;&lt;/small&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;Monetary policy affects the market and the economy through interest rates, which tell you everything you need to know about liquidity and expectations. How else would it work? If it’s not impacting rates, it’s not impacting policy, the economy, the stock market.&lt;/p&gt;

&lt;p&gt;So why are people hand-wringing about QT? It’s nonsense. Voodoo economics.&lt;/p&gt;

&lt;p&gt;What do QT hand-wringers want the Fed to do, not let the drain happen at the short end of the curve? Indeed, the Fed &lt;em&gt;doesn’t&lt;/em&gt; necessarily drain, because they peg the short end of the curve. They will end up buying back whatever is needed to maintain the peg, adding liquidity.&lt;/p&gt;

&lt;p&gt;And there are &lt;a href=&quot;https://fred.stlouisfed.org/series/EXCSRESNS&quot;&gt;$1.5t in excess reserves&lt;/a&gt;, and via &lt;a href=&quot;https://fred.stlouisfed.org/series/IOER&quot;&gt;interest on excess reserves&lt;/a&gt;, any bank can increase or reduce deposits to the Fed at a fixed rate. If the Fed does nothing to offset the drain when a bond matures, all that happens is excess reserves get drawn down. (Bank customer who issued the bond instructs the bank to pay the Fed. Bank instructs the Fed to debit their excess reserve account at the Fed. Deposits go down, excess reserves go down.)&lt;/p&gt;

&lt;p&gt;A (partially valid&lt;sup&gt;&lt;small&gt;&lt;a href=&quot;#2&quot;&gt;2&lt;/a&gt;&lt;/small&gt;&lt;/sup&gt;) criticism of QE was that if the Fed adds liquidity, but people just park it back at the Fed in the form of excess reserves, it doesn’t impact the real economy. The same applies when the process is reversed. If the Fed drains liquidity, but people just draw down their excess reserves, it doesn’t impact the real economy.&lt;/p&gt;

&lt;p&gt;Do QT hand-wringers want the Fed to buy at the long end? That’s ridiculous, the yield curve is flat. It would be nutty to be tightening and simultaneously doing QE.&lt;/p&gt;

&lt;p&gt;Do the QT hand-wringers think Fed tightened too much and should ease? There they may have a point, but that has absolutely nothing to do with balance sheet reduction and QT &lt;em&gt;per se&lt;/em&gt;. For any given tightening posture, it’s essentially a technical matter whether the Fed implements it by automatically rolling stuff off the short end of the balance sheet, or automatic adjustments via excess reserves and IOER, or discretionary open market operations at the short end.&lt;/p&gt;

&lt;p&gt;The &lt;a href=&quot;https://www.federalreserve.gov/monetarypolicy/policy-normalization.htm&quot;&gt;Fed statement&lt;/a&gt; responding to market concerns about the balance sheet reduction and QT seemed a little like telling the market ‘oh yes dear, anything you say’ but just doing same thing they’ve been doing all along.&lt;/p&gt;

&lt;p&gt;It seems like some board members thought they had to say something but there’s nothing to say, so they’re like ‘ok fine whatever’.&lt;/p&gt;

&lt;p&gt;I think Timaraos in WSJ and Duy on Bloomberg are on the money. There are always talking heads blaming something for market gyrations. If it’s not QT, then it’s ETFs, risk parity, algos. It’s just mumbo jumbo to explain movements that are pretty stochastic. And QT is not really a thing, except to the extent that a technical side effect of policy normalization is balance sheet reduction.&lt;/p&gt;

&lt;p&gt;The worst part is, a lot of the QT hand-wringers are free market fetishists who were probably complaining about QE in the first place. They sure seem to think the market is pretty effing fragile… OMG QE! OMG QT! Algos! Risk parity! ETFs! They are are messing with the function of the free market! If the market has a cow and can’t deal with this crap and has a crisis every 10 years, maybe it’s not all it’s cracked up to be.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.nytimes.com/2014/10/19/business/economy/when-a-stock-market-theory-is-contagious.html?ref=business&quot;&gt;The market runs on bullshit&lt;/a&gt;. It goes up, people find a reason. It goes down, people find a reason. Today, QT is the story it fixates on, but it’s mostly &lt;a href=&quot;https://giphy.com/explore/bullshit&quot;&gt;bullshit&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I believe in complementarity, the idea that you can’t necessarily have one deep consistent model of reality, possibly the best you can do is many shallow models that are applicable in overlapping circumstances.&lt;/p&gt;

&lt;p&gt;But it’s still pretty impressive that markets work as well as they do most of the time, when so many people are mostly applying simplistic heuristics that don’t reflect underlying reality.&lt;/p&gt;

&lt;p&gt;I could write a series a la Penn Jillette’s “Bullshit!” and go all Lewis Black/Andy Rooney on why easing does not cause deflation, there is no such thing as a chronic safe asset shortage, and MMT is not really a new thing.&lt;/p&gt;

&lt;p&gt;But I won’t. Just stay off my lawn!&lt;/p&gt;

&lt;p&gt;&lt;small&gt;&lt;sup&gt;&lt;a name=&quot;1&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;Suppose the bond that matures is a 10-year, and the issuer floats a new 10-year. If you have a lot of new issuance, that would pressure long rates. Hence the policy of allowing at most $50b per month to mature without replacing it at the long end. &amp;lt;/p&amp;gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;
  &lt;sup&gt;&lt;a name=&quot;2&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;QE pushed rates down at the long end. By pegging overnight rates at 0, committing to keeping them at 0 for an extended period, and buying at the long end, the Fed was pushing ZIRP down the yield curve. QT doesn’t directly impact rates at the long end, although maturing bonds maybe be refunded with new issuance. Given that the yield curve is flat, and credit spreads &lt;a href=&quot;https://fred.stlouisfed.org/series/BAA10Y&quot;&gt;pretty normal&lt;/a&gt;, seems like no big deal. &amp;lt;/small&amp;gt;
&lt;/p&gt;</content><author><name>Druce Vertes, CFA</name></author><category term="economics" /><category term="markets" /><category term="economics" /><category term="markets" /><summary type="html">Many people are talking about ‘quantitative tightening’ and ‘balance sheet reduction’, and some people are blaming it for market volatility, discussed here, here, here. IMHO, blaming balance sheet reduction for market volatility is cargo cult mumbo jumbo.</summary></entry><entry><title type="html">The Top 100 People To Follow For Financial News On Twitter, January 2019</title><link href="http://0.0.0.0:4000/2019/01/the-top-100-people-to-follow-for-financial-news-on-twitter-january-2019/" rel="alternate" type="text/html" title="The Top 100 People To Follow For Financial News On Twitter, January 2019" /><published>2019-01-04T19:21:10-05:00</published><updated>2019-01-04T19:21:10-05:00</updated><id>http://0.0.0.0:4000/2019/01/the-top-100-people-to-follow-for-financial-news-on-twitter-january-2019</id><content type="html" xml:base="http://0.0.0.0:4000/2019/01/the-top-100-people-to-follow-for-financial-news-on-twitter-january-2019/">&lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;/assets/js/tablesorter-master/themes/blue/style.css&quot; /&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;em&gt;It’s been more than a year since we posted our &lt;a href=&quot;/2017/05/the-top-100-people-to-follow-to-discover-financial-news-on-twitter-may-2017/&quot;&gt;last list&lt;/a&gt; of people to follow on Twitter for financial news. Time for an update!&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;!--more--&gt;

&lt;p&gt;Without further ado, here is this year’s list (click on headers to re-sort):&lt;/p&gt;

&lt;table id=&quot;myTable&quot; class=&quot;tablesorter&quot;&gt;
  &lt;tr&gt;
    &lt;th&gt;
      Rank
    &lt;/th&gt;
    
    &lt;th&gt;
      Screen Name
    &lt;/th&gt;
    
    &lt;th&gt;
      Name
    &lt;/th&gt;
    
    &lt;th&gt;
      Influence Score
    &lt;/th&gt;
    
    &lt;th&gt;
      Relevance Score
    &lt;/th&gt;
    
    &lt;th&gt;
      Order by Topic
    &lt;/th&gt;
  &lt;/tr&gt;
  
  &lt;tr&gt;
    &lt;td&gt;
      1
    &lt;/td&gt;
    
    &lt;td&gt;
      &lt;a href=&quot;https://twitter.com/pdacosta&quot;&gt;pdacosta&lt;/a&gt;
    &lt;/td&gt;
    
    &lt;td&gt;
      Pedro Nicolaci da Costa
    &lt;/td&gt;
    
    &lt;td&gt;
      6.78
    &lt;/td&gt;
    
    &lt;td&gt;
      7.39
    &lt;/td&gt;
    
    &lt;td&gt;
      149
    &lt;/td&gt;
  &lt;/tr&gt;
  
  &lt;tr&gt;
    &lt;td&gt;
      2
    &lt;/td&gt;
    
    &lt;td&gt;
      &lt;a href=&quot;https://twitter.com/TheStalwart&quot;&gt;TheStalwart&lt;/a&gt;
    &lt;/td&gt;
    
    &lt;td&gt;
      Joe Weisenthal
    &lt;/td&gt;
    
    &lt;td&gt;
      10.0
    &lt;/td&gt;
    
    &lt;td&gt;
      2.48
    &lt;/td&gt;
    
    &lt;td&gt;
      122
    &lt;/td&gt;
  &lt;/tr&gt;
  
  &lt;tr&gt;
    &lt;td&gt;
      3
    &lt;/td&gt;
    
    &lt;td&gt;
      &lt;a href=&quot;https://twitter.com/ReformedBroker&quot;&gt;ReformedBroker&lt;/a&gt;
    &lt;/td&gt;
    
    &lt;td&gt;
      Downtown Josh Brown
    &lt;/td&gt;
    
    &lt;td&gt;
      8.87
    &lt;/td&gt;
    
    &lt;td&gt;
      2.99
    &lt;/td&gt;
    
    &lt;td&gt;
      87
    &lt;/td&gt;
  &lt;/tr&gt;
  
  &lt;tr&gt;
    &lt;td&gt;
      4
    &lt;/td&gt;
    
    &lt;td&gt;
      &lt;a href=&quot;https://twitter.com/ritholtz&quot;&gt;ritholtz&lt;/a&gt;
    &lt;/td&gt;
    
    &lt;td&gt;
      Barry Ritholtz
    &lt;/td&gt;
    
    &lt;td&gt;
      7.68
    &lt;/td&gt;
    
    &lt;td&gt;
      4.1
    &lt;/td&gt;
    
    &lt;td&gt;
      86
    &lt;/td&gt;
  &lt;/tr&gt;
  
  &lt;tr&gt;
    &lt;td&gt;
      5
    &lt;/td&gt;
    
    &lt;td&gt;
      &lt;a href=&quot;https://twitter.com/felixsalmon&quot;&gt;felixsalmon&lt;/a&gt;
    &lt;/td&gt;
    
    &lt;td&gt;
      Felix Salmon
    &lt;/td&gt;
    
    &lt;td&gt;
      8.82
    &lt;/td&gt;
    
    &lt;td&gt;
      2.73
    &lt;/td&gt;
    
    &lt;td&gt;
      470
    &lt;/td&gt;
  &lt;/tr&gt;
  
  &lt;tr&gt;
    &lt;td&gt;
      6
    &lt;/td&gt;
    
    &lt;td&gt;
      &lt;a href=&quot;https://twitter.com/Noahpinion&quot;&gt;Noahpinion&lt;/a&gt;
    &lt;/td&gt;
    
    &lt;td&gt;
      Noah Smith
    &lt;/td&gt;
    
    &lt;td&gt;
      5.12
    &lt;/td&gt;
    
    &lt;td&gt;
      6.07
    &lt;/td&gt;
    
    &lt;td&gt;
      310
    &lt;/td&gt;
  &lt;/tr&gt;
  
  &lt;tr&gt;
    &lt;td&gt;
      7
    &lt;/td&gt;
    
    &lt;td&gt;
      &lt;a href=&quot;https://twitter.com/Frances_Coppola&quot;&gt;Frances_Coppola&lt;/a&gt;
    &lt;/td&gt;
    
    &lt;td&gt;
      (((Frances Coppola)))
    &lt;/td&gt;
    
    &lt;td&gt;
      3.64
    &lt;/td&gt;
    
    &lt;td&gt;
      7.48
    &lt;/td&gt;
    
    &lt;td&gt;
      49
    &lt;/td&gt;
  &lt;/tr&gt;
  
  &lt;tr&gt;
    &lt;td&gt;
      8
    &lt;/td&gt;
    
    &lt;td&gt;
      &lt;a href=&quot;https://twitter.com/hblodget&quot;&gt;hblodget&lt;/a&gt;
    &lt;/td&gt;
    
    &lt;td&gt;
      Henry Blodget
    &lt;/td&gt;
    
    &lt;td&gt;
      6.43
    &lt;/td&gt;
    
    &lt;td&gt;
      4.59
    &lt;/td&gt;
    
    &lt;td&gt;
      252
    &lt;/td&gt;
  &lt;/tr&gt;
  
  &lt;tr&gt;
    &lt;td&gt;
      9
    &lt;/td&gt;
    
    &lt;td&gt;
      &lt;a href=&quot;https://twitter.com/crampell&quot;&gt;crampell&lt;/a&gt;
    &lt;/td&gt;
    
    &lt;td&gt;
      Catherine Rampell
    &lt;/td&gt;
    
    &lt;td&gt;
      5.26
    &lt;/td&gt;
    
    &lt;td&gt;
      5.34
    &lt;/td&gt;
    
    &lt;td&gt;
      338
    &lt;/td&gt;
  &lt;/tr&gt;
  
  &lt;tr&gt;
    &lt;td&gt;
      10
    &lt;/td&gt;
    
    &lt;td&gt;
      &lt;a href=&quot;https://twitter.com/sdonnan&quot;&gt;sdonnan&lt;/a&gt;
    &lt;/td&gt;
    
    &lt;td&gt;
      Shawn Donnan
    &lt;/td&gt;
    
    &lt;td&gt;
      3.23
    &lt;/td&gt;
    
    &lt;td&gt;
      6.94
    &lt;/td&gt;
    
    &lt;td&gt;
      366
    &lt;/td&gt;
  &lt;/tr&gt;
  
  &lt;tr&gt;
    &lt;td&gt;
      11
    &lt;/td&gt;
    
    &lt;td&gt;
      &lt;a href=&quot;https://twitter.com/moorehn&quot;&gt;moorehn&lt;/a&gt;
    &lt;/td&gt;
    
    &lt;td&gt;
      Heidi N. Moore
    &lt;/td&gt;
    
    &lt;td&gt;
      3.96
    &lt;/td&gt;
    
    &lt;td&gt;
      6.06
    &lt;/td&gt;
    
    &lt;td&gt;
      401
    &lt;/td&gt;
  &lt;/tr&gt;
  
  &lt;tr&gt;
    &lt;td&gt;
      12
    &lt;/td&gt;
    
    &lt;td&gt;
      &lt;a href=&quot;https://twitter.com/paulkrugman&quot;&gt;paulkrugman&lt;/a&gt;
    &lt;/td&gt;
    
    &lt;td&gt;
      Paul Krugman
    &lt;/td&gt;
    
    &lt;td&gt;
      6.91
    &lt;/td&gt;
    
    &lt;td&gt;
      2.44
    &lt;/td&gt;
    
    &lt;td&gt;
      358
    &lt;/td&gt;
  &lt;/tr&gt;
  
  &lt;tr&gt;
    &lt;td&gt;
      13
    &lt;/td&gt;
    
    &lt;td&gt;
      &lt;a href=&quot;https://twitter.com/M_C_Klein&quot;&gt;M_C_Klein&lt;/a&gt;
    &lt;/td&gt;
    
    &lt;td&gt;
      Matthew C. Klein
    &lt;/td&gt;
    
    &lt;td&gt;
      7.45
    &lt;/td&gt;
    
    &lt;td&gt;
      1.74
    &lt;/td&gt;
    
    &lt;td&gt;
      333
    &lt;/td&gt;
  &lt;/tr&gt;
  
  &lt;tr&gt;
    &lt;td&gt;
      14
    &lt;/td&gt;
    
    &lt;td&gt;
      &lt;a href=&quot;https://twitter.com/davidmwessel&quot;&gt;davidmwessel&lt;/a&gt;
    &lt;/td&gt;
    
    &lt;td&gt;
      David Wessel
    &lt;/td&gt;
    
    &lt;td&gt;
      5.96
    &lt;/td&gt;
    
    &lt;td&gt;
      3.22
    &lt;/td&gt;
    
    &lt;td&gt;
      364
    &lt;/td&gt;
  &lt;/tr&gt;
  
  &lt;tr&gt;
    &lt;td&gt;
      15
    &lt;/td&gt;
    
    &lt;td&gt;
      &lt;a href=&quot;https://twitter.com/matt_levine&quot;&gt;matt_levine&lt;/a&gt;
    &lt;/td&gt;
    
    &lt;td&gt;
      Matt Levine
    &lt;/td&gt;
    
    &lt;td&gt;
      8.36
    &lt;/td&gt;
    
    &lt;td&gt;
      0.81
    &lt;/td&gt;
    
    &lt;td&gt;
      121
    &lt;/td&gt;
  &lt;/tr&gt;
  
  &lt;tr&gt;
    &lt;td&gt;
      16
    &lt;/td&gt;
    
    &lt;td&gt;
      &lt;a href=&quot;https://twitter.com/tracyalloway&quot;&gt;tracyalloway&lt;/a&gt;
    &lt;/td&gt;
    
    &lt;td&gt;
      Tracy Alloway
    &lt;/td&gt;
    
    &lt;td&gt;
      8.45
    &lt;/td&gt;
    
    &lt;td&gt;
      0.62
    &lt;/td&gt;
    
    &lt;td&gt;
      123
    &lt;/td&gt;
  &lt;/tr&gt;
  
  &lt;tr&gt;
    &lt;td&gt;
      17
    &lt;/td&gt;
    
    &lt;td&gt;
      &lt;a href=&quot;https://twitter.com/IvanTheK&quot;&gt;IvanTheK&lt;/a&gt;
    &lt;/td&gt;
    
    &lt;td&gt;
      Ivan the K™
    &lt;/td&gt;
    
    &lt;td&gt;
      5.78
    &lt;/td&gt;
    
    &lt;td&gt;
      3.29
    &lt;/td&gt;
    
    &lt;td&gt;
      263
    &lt;/td&gt;
  &lt;/tr&gt;
  
  &lt;tr&gt;
    &lt;td&gt;
      18
    &lt;/td&gt;
    
    &lt;td&gt;
      &lt;a href=&quot;https://twitter.com/edwardnh&quot;&gt;edwardnh&lt;/a&gt;
    &lt;/td&gt;
    
    &lt;td&gt;
      Edward Harrison
    &lt;/td&gt;
    
    &lt;td&gt;
      4.01
    &lt;/td&gt;
    
    &lt;td&gt;
      4.75
    &lt;/td&gt;
    
    &lt;td&gt;
      146
    &lt;/td&gt;
  &lt;/tr&gt;
  
  &lt;tr&gt;
    &lt;td&gt;
      19
    &lt;/td&gt;
    
    &lt;td&gt;
      &lt;a href=&quot;https://twitter.com/Nouriel&quot;&gt;Nouriel&lt;/a&gt;
    &lt;/td&gt;
    
    &lt;td&gt;
      Nouriel Roubini
    &lt;/td&gt;
    
    &lt;td&gt;
      6.29
    &lt;/td&gt;
    
    &lt;td&gt;
      2.42
    &lt;/td&gt;
    
    &lt;td&gt;
      183
    &lt;/td&gt;
  &lt;/tr&gt;
  
  &lt;tr&gt;
    &lt;td&gt;
      20
    &lt;/td&gt;
    
    &lt;td&gt;
      &lt;a href=&quot;https://twitter.com/BCAppelbaum&quot;&gt;BCAppelbaum&lt;/a&gt;
    &lt;/td&gt;
    
    &lt;td&gt;
      Binyamin Appelbaum
    &lt;/td&gt;
    
    &lt;td&gt;
      6.98
    &lt;/td&gt;
    
    &lt;td&gt;
      1.7
    &lt;/td&gt;
    
    &lt;td&gt;
      351
    &lt;/td&gt;
  &lt;/tr&gt;
  
  &lt;tr&gt;
    &lt;td&gt;
      21
    &lt;/td&gt;
    
    &lt;td&gt;
      &lt;a href=&quot;https://twitter.com/carlquintanilla&quot;&gt;carlquintanilla&lt;/a&gt;
    &lt;/td&gt;
    
    &lt;td&gt;
      Carl Quintanilla
    &lt;/td&gt;
    
    &lt;td&gt;
      5.19
    &lt;/td&gt;
    
    &lt;td&gt;
      3.33
    &lt;/td&gt;
    
    &lt;td&gt;
      261
    &lt;/td&gt;
  &lt;/tr&gt;
  
  &lt;tr&gt;
    &lt;td&gt;
      22
    &lt;/td&gt;
    
    &lt;td&gt;
      &lt;a href=&quot;https://twitter.com/acrossthecurve&quot;&gt;acrossthecurve&lt;/a&gt;
    &lt;/td&gt;
    
    &lt;td&gt;
      across the curve.com
    &lt;/td&gt;
    
    &lt;td&gt;
      0.92
    &lt;/td&gt;
    
    &lt;td&gt;
      7.53
    &lt;/td&gt;
    
    &lt;td&gt;
      131
    &lt;/td&gt;
  &lt;/tr&gt;
  
  &lt;tr&gt;
    &lt;td&gt;
      23
    &lt;/td&gt;
    
    &lt;td&gt;
      &lt;a href=&quot;https://twitter.com/MarkThoma&quot;&gt;MarkThoma&lt;/a&gt;
    &lt;/td&gt;
    
    &lt;td&gt;
      Mark Thoma
    &lt;/td&gt;
    
    &lt;td&gt;
      5.09
    &lt;/td&gt;
    
    &lt;td&gt;
      3.27
    &lt;/td&gt;
    
    &lt;td&gt;
      317
    &lt;/td&gt;
  &lt;/tr&gt;
  
  &lt;tr&gt;
    &lt;td&gt;
      24
    &lt;/td&gt;
    
    &lt;td&gt;
      &lt;a href=&quot;https://twitter.com/AdamPosen&quot;&gt;AdamPosen&lt;/a&gt;
    &lt;/td&gt;
    
    &lt;td&gt;
      Adam Posen
    &lt;/td&gt;
    
    &lt;td&gt;
      4.18
    &lt;/td&gt;
    
    &lt;td&gt;
      4.07
    &lt;/td&gt;
    
    &lt;td&gt;
      295
    &lt;/td&gt;
  &lt;/tr&gt;
  
  &lt;tr&gt;
    &lt;td&gt;
      25
    &lt;/td&gt;
    
    &lt;td&gt;
      &lt;a href=&quot;https://twitter.com/JustinWolfers&quot;&gt;JustinWolfers&lt;/a&gt;
    &lt;/td&gt;
    
    &lt;td&gt;
      Justin Wolfers
    &lt;/td&gt;
    
    &lt;td&gt;
      6.75
    &lt;/td&gt;
    
    &lt;td&gt;
      1.48
    &lt;/td&gt;
    
    &lt;td&gt;
      356
    &lt;/td&gt;
  &lt;/tr&gt;
  
  &lt;tr&gt;
    &lt;td&gt;
      26
    &lt;/td&gt;
    
    &lt;td&gt;
      &lt;a href=&quot;https://twitter.com/jennablan&quot;&gt;jennablan&lt;/a&gt;
    &lt;/td&gt;
    
    &lt;td&gt;
      Jennifer Ablan
    &lt;/td&gt;
    
    &lt;td&gt;
      5.48
    &lt;/td&gt;
    
    &lt;td&gt;
      2.75
    &lt;/td&gt;
    
    &lt;td&gt;
      162
    &lt;/td&gt;
  &lt;/tr&gt;
  
  &lt;tr&gt;
    &lt;td&gt;
      27
    &lt;/td&gt;
    
    &lt;td&gt;
      &lt;a href=&quot;https://twitter.com/elerianm&quot;&gt;elerianm&lt;/a&gt;
    &lt;/td&gt;
    
    &lt;td&gt;
      Mohamed A. El-Erian
    &lt;/td&gt;
    
    &lt;td&gt;
      5.85
    &lt;/td&gt;
    
    &lt;td&gt;
      2.15
    &lt;/td&gt;
    
    &lt;td&gt;
      151
    &lt;/td&gt;
  &lt;/tr&gt;
  
  &lt;tr&gt;
    &lt;td&gt;
      28
    &lt;/td&gt;
    
    &lt;td&gt;
      &lt;a href=&quot;https://twitter.com/tomkeene&quot;&gt;tomkeene&lt;/a&gt;
    &lt;/td&gt;
    
    &lt;td&gt;
      tom keene
    &lt;/td&gt;
    
    &lt;td&gt;
      7.09
    &lt;/td&gt;
    
    &lt;td&gt;
      0.87
    &lt;/td&gt;
    
    &lt;td&gt;
      148
    &lt;/td&gt;
  &lt;/tr&gt;
  
  &lt;tr&gt;
    &lt;td&gt;
      29
    &lt;/td&gt;
    
    &lt;td&gt;
      &lt;a href=&quot;https://twitter.com/CardiffGarcia&quot;&gt;CardiffGarcia&lt;/a&gt;
    &lt;/td&gt;
    
    &lt;td&gt;
      Cardiff Garcia
    &lt;/td&gt;
    
    &lt;td&gt;
      6.57
    &lt;/td&gt;
    
    &lt;td&gt;
      1.31
    &lt;/td&gt;
    
    &lt;td&gt;
      332
    &lt;/td&gt;
  &lt;/tr&gt;
  
  &lt;tr&gt;
    &lt;td&gt;
      30
    &lt;/td&gt;
    
    &lt;td&gt;
      &lt;a href=&quot;https://twitter.com/jasonzweigwsj&quot;&gt;jasonzweigwsj&lt;/a&gt;
    &lt;/td&gt;
    
    &lt;td&gt;
      Jason Zweig
    &lt;/td&gt;
    
    &lt;td&gt;
      6.28
    &lt;/td&gt;
    
    &lt;td&gt;
      1.56
    &lt;/td&gt;
    
    &lt;td&gt;
      114
    &lt;/td&gt;
  &lt;/tr&gt;
  
  &lt;tr&gt;
    &lt;td&gt;
      31
    &lt;/td&gt;
    
    &lt;td&gt;
      &lt;a href=&quot;https://twitter.com/delong&quot;&gt;delong&lt;/a&gt;
    &lt;/td&gt;
    
    &lt;td&gt;
      Brad DeLong 🖖🏻
    &lt;/td&gt;
    
    &lt;td&gt;
      5.2
    &lt;/td&gt;
    
    &lt;td&gt;
      2.62
    &lt;/td&gt;
    
    &lt;td&gt;
      345
    &lt;/td&gt;
  &lt;/tr&gt;
  
  &lt;tr&gt;
    &lt;td&gt;
      32
    &lt;/td&gt;
    
    &lt;td&gt;
      &lt;a href=&quot;https://twitter.com/ObsoleteDogma&quot;&gt;ObsoleteDogma&lt;/a&gt;
    &lt;/td&gt;
    
    &lt;td&gt;
      Matt O’Brien
    &lt;/td&gt;
    
    &lt;td&gt;
      6.6
    &lt;/td&gt;
    
    &lt;td&gt;
      1.23
    &lt;/td&gt;
    
    &lt;td&gt;
      355
    &lt;/td&gt;
  &lt;/tr&gt;
  
  &lt;tr&gt;
    &lt;td&gt;
      33
    &lt;/td&gt;
    
    &lt;td&gt;
      &lt;a href=&quot;https://twitter.com/greg_ip&quot;&gt;greg_ip&lt;/a&gt;
    &lt;/td&gt;
    
    &lt;td&gt;
      Greg Ip
    &lt;/td&gt;
    
    &lt;td&gt;
      6.67
    &lt;/td&gt;
    
    &lt;td&gt;
      0.94
    &lt;/td&gt;
    
    &lt;td&gt;
      107
    &lt;/td&gt;
  &lt;/tr&gt;
  
  &lt;tr&gt;
    &lt;td&gt;
      34
    &lt;/td&gt;
    
    &lt;td&gt;
      &lt;a href=&quot;https://twitter.com/RobinWigg&quot;&gt;RobinWigg&lt;/a&gt;
    &lt;/td&gt;
    
    &lt;td&gt;
      Robin Wigglesworth
    &lt;/td&gt;
    
    &lt;td&gt;
      5.5
    &lt;/td&gt;
    
    &lt;td&gt;
      2.1
    &lt;/td&gt;
    
    &lt;td&gt;
      5
    &lt;/td&gt;
  &lt;/tr&gt;
  
  &lt;tr&gt;
    &lt;td&gt;
      35
    &lt;/td&gt;
    
    &lt;td&gt;
      &lt;a href=&quot;https://twitter.com/JacobWolinsky&quot;&gt;JacobWolinsky&lt;/a&gt;
    &lt;/td&gt;
    
    &lt;td&gt;
      Jacob Wolinsky
    &lt;/td&gt;
    
    &lt;td&gt;
      2.29
    &lt;/td&gt;
    
    &lt;td&gt;
      5.31
    &lt;/td&gt;
    
    &lt;td&gt;
      238
    &lt;/td&gt;
  &lt;/tr&gt;
  
  &lt;tr&gt;
    &lt;td&gt;
      36
    &lt;/td&gt;
    
    &lt;td&gt;
      &lt;a href=&quot;https://twitter.com/TimOBrien&quot;&gt;TimOBrien&lt;/a&gt;
    &lt;/td&gt;
    
    &lt;td&gt;
      Tim O’Brien
    &lt;/td&gt;
    
    &lt;td&gt;
      2.94
    &lt;/td&gt;
    
    &lt;td&gt;
      4.63
    &lt;/td&gt;
    
    &lt;td&gt;
      442
    &lt;/td&gt;
  &lt;/tr&gt;
  
  &lt;tr&gt;
    &lt;td&gt;
      37
    &lt;/td&gt;
    
    &lt;td&gt;
      &lt;a href=&quot;https://twitter.com/izakaminska&quot;&gt;izakaminska&lt;/a&gt;
    &lt;/td&gt;
    
    &lt;td&gt;
      Izabella Kaminska
    &lt;/td&gt;
    
    &lt;td&gt;
      7.34
    &lt;/td&gt;
    
    &lt;td&gt;
      0.1
    &lt;/td&gt;
    
    &lt;td&gt;
      178
    &lt;/td&gt;
  &lt;/tr&gt;
  
  &lt;tr&gt;
    &lt;td&gt;
      38
    &lt;/td&gt;
    
    &lt;td&gt;
      &lt;a href=&quot;https://twitter.com/tylercowen&quot;&gt;tylercowen&lt;/a&gt;
    &lt;/td&gt;
    
    &lt;td&gt;
      tylercowen
    &lt;/td&gt;
    
    &lt;td&gt;
      5.62
    &lt;/td&gt;
    
    &lt;td&gt;
      1.8
    &lt;/td&gt;
    
    &lt;td&gt;
      307
    &lt;/td&gt;
  &lt;/tr&gt;
  
  &lt;tr&gt;
    &lt;td&gt;
      39
    &lt;/td&gt;
    
    &lt;td&gt;
      &lt;a href=&quot;https://twitter.com/NateSilver538&quot;&gt;NateSilver538&lt;/a&gt;
    &lt;/td&gt;
    
    &lt;td&gt;
      Nate Silver
    &lt;/td&gt;
    
    &lt;td&gt;
      6.52
    &lt;/td&gt;
    
    &lt;td&gt;
      0.87
    &lt;/td&gt;
    
    &lt;td&gt;
      344
    &lt;/td&gt;
  &lt;/tr&gt;
  
  &lt;tr&gt;
    &lt;td&gt;
      40
    &lt;/td&gt;
    
    &lt;td&gt;
      &lt;a href=&quot;https://twitter.com/jbarro&quot;&gt;jbarro&lt;/a&gt;
    &lt;/td&gt;
    
    &lt;td&gt;
      Josh Barro
    &lt;/td&gt;
    
    &lt;td&gt;
      4.7
    &lt;/td&gt;
    
    &lt;td&gt;
      2.65
    &lt;/td&gt;
    
    &lt;td&gt;
      339
    &lt;/td&gt;
  &lt;/tr&gt;
  
  &lt;tr&gt;
    &lt;td&gt;
      41
    &lt;/td&gt;
    
    &lt;td&gt;
      &lt;a href=&quot;https://twitter.com/jessefelder&quot;&gt;jessefelder&lt;/a&gt;
    &lt;/td&gt;
    
    &lt;td&gt;
      Jesse Felder
    &lt;/td&gt;
    
    &lt;td&gt;
      2.04
    &lt;/td&gt;
    
    &lt;td&gt;
      5.23
    &lt;/td&gt;
    
    &lt;td&gt;
      130
    &lt;/td&gt;
  &lt;/tr&gt;
  
  &lt;tr&gt;
    &lt;td&gt;
      42
    &lt;/td&gt;
    
    &lt;td&gt;
      &lt;a href=&quot;https://twitter.com/ezraklein&quot;&gt;ezraklein&lt;/a&gt;
    &lt;/td&gt;
    
    &lt;td&gt;
      Ezra Klein
    &lt;/td&gt;
    
    &lt;td&gt;
      6.0
    &lt;/td&gt;
    
    &lt;td&gt;
      1.26
    &lt;/td&gt;
    
    &lt;td&gt;
      340
    &lt;/td&gt;
  &lt;/tr&gt;
  
  &lt;tr&gt;
    &lt;td&gt;
      43
    &lt;/td&gt;
    
    &lt;td&gt;
      &lt;a href=&quot;https://twitter.com/EddyElfenbein&quot;&gt;EddyElfenbein&lt;/a&gt;
    &lt;/td&gt;
    
    &lt;td&gt;
      Eddy Elfenbein
    &lt;/td&gt;
    
    &lt;td&gt;
      4.16
    &lt;/td&gt;
    
    &lt;td&gt;
      3.05
    &lt;/td&gt;
    
    &lt;td&gt;
      138
    &lt;/td&gt;
  &lt;/tr&gt;
  
  &lt;tr&gt;
    &lt;td&gt;
      44
    &lt;/td&gt;
    
    &lt;td&gt;
      &lt;a href=&quot;https://twitter.com/johnauthers&quot;&gt;johnauthers&lt;/a&gt;
    &lt;/td&gt;
    
    &lt;td&gt;
      John Authers
    &lt;/td&gt;
    
    &lt;td&gt;
      6.08
    &lt;/td&gt;
    
    &lt;td&gt;
      1.13
    &lt;/td&gt;
    
    &lt;td&gt;
      1
    &lt;/td&gt;
  &lt;/tr&gt;
  
  &lt;tr&gt;
    &lt;td&gt;
      45
    &lt;/td&gt;
    
    &lt;td&gt;
      &lt;a href=&quot;https://twitter.com/TabbFORUM&quot;&gt;TabbFORUM&lt;/a&gt;
    &lt;/td&gt;
    
    &lt;td&gt;
      TabbFORUM
    &lt;/td&gt;
    
    &lt;td&gt;
      0.2
    &lt;/td&gt;
    
    &lt;td&gt;
      7.0
    &lt;/td&gt;
    
    &lt;td&gt;
      42
    &lt;/td&gt;
  &lt;/tr&gt;
  
  &lt;tr&gt;
    &lt;td&gt;
      46
    &lt;/td&gt;
    
    &lt;td&gt;
      &lt;a href=&quot;https://twitter.com/AmyResnick&quot;&gt;AmyResnick&lt;/a&gt;
    &lt;/td&gt;
    
    &lt;td&gt;
      Amy Resnick
    &lt;/td&gt;
    
    &lt;td&gt;
      2.32
    &lt;/td&gt;
    
    &lt;td&gt;
      4.75
    &lt;/td&gt;
    
    &lt;td&gt;
      394
    &lt;/td&gt;
  &lt;/tr&gt;
  
  &lt;tr&gt;
    &lt;td&gt;
      47
    &lt;/td&gt;
    
    &lt;td&gt;
      &lt;a href=&quot;https://twitter.com/activiststocks&quot;&gt;activiststocks&lt;/a&gt;
    &lt;/td&gt;
    
    &lt;td&gt;
      Activist Stocks
    &lt;/td&gt;
    
    &lt;td&gt;
      0.81
    &lt;/td&gt;
    
    &lt;td&gt;
      6.13
    &lt;/td&gt;
    
    &lt;td&gt;
      198
    &lt;/td&gt;
  &lt;/tr&gt;
  
  &lt;tr&gt;
    &lt;td&gt;
      48
    &lt;/td&gt;
    
    &lt;td&gt;
      &lt;a href=&quot;https://twitter.com/eisingerj&quot;&gt;eisingerj&lt;/a&gt;
    &lt;/td&gt;
    
    &lt;td&gt;
      Jesse Eisinger
    &lt;/td&gt;
    
    &lt;td&gt;
      4.77
    &lt;/td&gt;
    
    &lt;td&gt;
      2.14
    &lt;/td&gt;
    
    &lt;td&gt;
      435
    &lt;/td&gt;
  &lt;/tr&gt;
  
  &lt;tr&gt;
    &lt;td&gt;
      49
    &lt;/td&gt;
    
    &lt;td&gt;
      &lt;a href=&quot;https://twitter.com/TimHarford&quot;&gt;TimHarford&lt;/a&gt;
    &lt;/td&gt;
    
    &lt;td&gt;
      Tim Harford
    &lt;/td&gt;
    
    &lt;td&gt;
      4.37
    &lt;/td&gt;
    
    &lt;td&gt;
      2.47
    &lt;/td&gt;
    
    &lt;td&gt;
      14
    &lt;/td&gt;
  &lt;/tr&gt;
  
  &lt;tr&gt;
    &lt;td&gt;
      50
    &lt;/td&gt;
    
    &lt;td&gt;
      &lt;a href=&quot;https://twitter.com/Neil_Irwin&quot;&gt;Neil_Irwin&lt;/a&gt;
    &lt;/td&gt;
    
    &lt;td&gt;
      Neil Irwin
    &lt;/td&gt;
    
    &lt;td&gt;
      6.1
    &lt;/td&gt;
    
    &lt;td&gt;
      0.71
    &lt;/td&gt;
    
    &lt;td&gt;
      350
    &lt;/td&gt;
  &lt;/tr&gt;
  
  &lt;tr&gt;
    &lt;td&gt;
      51
    &lt;/td&gt;
    
    &lt;td&gt;
      &lt;a href=&quot;https://twitter.com/Jesse_Livermore&quot;&gt;Jesse_Livermore&lt;/a&gt;
    &lt;/td&gt;
    
    &lt;td&gt;
      Jesse Livermore
    &lt;/td&gt;
    
    &lt;td&gt;
      4.69
    &lt;/td&gt;
    
    &lt;td&gt;
      1.97
    &lt;/td&gt;
    
    &lt;td&gt;
      89
    &lt;/td&gt;
  &lt;/tr&gt;
  
  &lt;tr&gt;
    &lt;td&gt;
      52
    &lt;/td&gt;
    
    &lt;td&gt;
      &lt;a href=&quot;https://twitter.com/carney&quot;&gt;carney&lt;/a&gt;
    &lt;/td&gt;
    
    &lt;td&gt;
      John Carney
    &lt;/td&gt;
    
    &lt;td&gt;
      5.72
    &lt;/td&gt;
    
    &lt;td&gt;
      0.91
    &lt;/td&gt;
    
    &lt;td&gt;
      265
    &lt;/td&gt;
  &lt;/tr&gt;
  
  &lt;tr&gt;
    &lt;td&gt;
      53
    &lt;/td&gt;
    
    &lt;td&gt;
      &lt;a href=&quot;https://twitter.com/mims&quot;&gt;mims&lt;/a&gt;
    &lt;/td&gt;
    
    &lt;td&gt;
      Christopher Mims 🎆
    &lt;/td&gt;
    
    &lt;td&gt;
      3.46
    &lt;/td&gt;
    
    &lt;td&gt;
      3.13
    &lt;/td&gt;
    
    &lt;td&gt;
      448
    &lt;/td&gt;
  &lt;/tr&gt;
  
  &lt;tr&gt;
    &lt;td&gt;
      54
    &lt;/td&gt;
    
    &lt;td&gt;
      &lt;a href=&quot;https://twitter.com/conorsen&quot;&gt;conorsen&lt;/a&gt;
    &lt;/td&gt;
    
    &lt;td&gt;
      Conor Sen
    &lt;/td&gt;
    
    &lt;td&gt;
      3.96
    &lt;/td&gt;
    
    &lt;td&gt;
      2.61
    &lt;/td&gt;
    
    &lt;td&gt;
      142
    &lt;/td&gt;
  &lt;/tr&gt;
  
  &lt;tr&gt;
    &lt;td&gt;
      55
    &lt;/td&gt;
    
    &lt;td&gt;
      &lt;a href=&quot;https://twitter.com/lisaabramowicz1&quot;&gt;lisaabramowicz1&lt;/a&gt;
    &lt;/td&gt;
    
    &lt;td&gt;
      Lisa Abramowicz
    &lt;/td&gt;
    
    &lt;td&gt;
      4.32
    &lt;/td&gt;
    
    &lt;td&gt;
      2.16
    &lt;/td&gt;
    
    &lt;td&gt;
      128
    &lt;/td&gt;
  &lt;/tr&gt;
  
  &lt;tr&gt;
    &lt;td&gt;
      56
    &lt;/td&gt;
    
    &lt;td&gt;
      &lt;a href=&quot;https://twitter.com/JohnCassidy&quot;&gt;JohnCassidy&lt;/a&gt;
    &lt;/td&gt;
    
    &lt;td&gt;
      John Cassidy
    &lt;/td&gt;
    
    &lt;td&gt;
      3.92
    &lt;/td&gt;
    
    &lt;td&gt;
      2.55
    &lt;/td&gt;
    
    &lt;td&gt;
      441
    &lt;/td&gt;
  &lt;/tr&gt;
  
  &lt;tr&gt;
    &lt;td&gt;
      57
    &lt;/td&gt;
    
    &lt;td&gt;
      &lt;a href=&quot;https://twitter.com/economistmeg&quot;&gt;economistmeg&lt;/a&gt;
    &lt;/td&gt;
    
    &lt;td&gt;
      Megan Greene
    &lt;/td&gt;
    
    &lt;td&gt;
      5.12
    &lt;/td&gt;
    
    &lt;td&gt;
      1.34
    &lt;/td&gt;
    
    &lt;td&gt;
      292
    &lt;/td&gt;
  &lt;/tr&gt;
  
  &lt;tr&gt;
    &lt;td&gt;
      58
    &lt;/td&gt;
    
    &lt;td&gt;
      &lt;a href=&quot;https://twitter.com/Austan_Goolsbee&quot;&gt;Austan_Goolsbee&lt;/a&gt;
    &lt;/td&gt;
    
    &lt;td&gt;
      Austan Goolsbee
    &lt;/td&gt;
    
    &lt;td&gt;
      4.67
    &lt;/td&gt;
    
    &lt;td&gt;
      1.78
    &lt;/td&gt;
    
    &lt;td&gt;
      357
    &lt;/td&gt;
  &lt;/tr&gt;
  
  &lt;tr&gt;
    &lt;td&gt;
      59
    &lt;/td&gt;
    
    &lt;td&gt;
      &lt;a href=&quot;https://twitter.com/lopezlinette&quot;&gt;lopezlinette&lt;/a&gt;
    &lt;/td&gt;
    
    &lt;td&gt;
      Linette Lopez
    &lt;/td&gt;
    
    &lt;td&gt;
      2.75
    &lt;/td&gt;
    
    &lt;td&gt;
      3.7
    &lt;/td&gt;
    
    &lt;td&gt;
      264
    &lt;/td&gt;
  &lt;/tr&gt;
  
  &lt;tr&gt;
    &lt;td&gt;
      60
    &lt;/td&gt;
    
    &lt;td&gt;
      &lt;a href=&quot;https://twitter.com/prchovanec&quot;&gt;prchovanec&lt;/a&gt;
    &lt;/td&gt;
    
    &lt;td&gt;
      Patrick Chovanec
    &lt;/td&gt;
    
    &lt;td&gt;
      2.2
    &lt;/td&gt;
    
    &lt;td&gt;
      4.22
    &lt;/td&gt;
    
    &lt;td&gt;
      279
    &lt;/td&gt;
  &lt;/tr&gt;
  
  &lt;tr&gt;
    &lt;td&gt;
      61
    &lt;/td&gt;
    
    &lt;td&gt;
      &lt;a href=&quot;https://twitter.com/mark_dow&quot;&gt;mark_dow&lt;/a&gt;
    &lt;/td&gt;
    
    &lt;td&gt;
      Dow
    &lt;/td&gt;
    
    &lt;td&gt;
      5.23
    &lt;/td&gt;
    
    &lt;td&gt;
      1.18
    &lt;/td&gt;
    
    &lt;td&gt;
      262
    &lt;/td&gt;
  &lt;/tr&gt;
  
  &lt;tr&gt;
    &lt;td&gt;
      62
    &lt;/td&gt;
    
    &lt;td&gt;
      &lt;a href=&quot;https://twitter.com/DLeonhardt&quot;&gt;DLeonhardt&lt;/a&gt;
    &lt;/td&gt;
    
    &lt;td&gt;
      David Leonhardt
    &lt;/td&gt;
    
    &lt;td&gt;
      5.72
    &lt;/td&gt;
    
    &lt;td&gt;
      0.65
    &lt;/td&gt;
    
    &lt;td&gt;
      354
    &lt;/td&gt;
  &lt;/tr&gt;
  
  &lt;tr&gt;
    &lt;td&gt;
      63
    &lt;/td&gt;
    
    &lt;td&gt;
      &lt;a href=&quot;https://twitter.com/niubi&quot;&gt;niubi&lt;/a&gt;
    &lt;/td&gt;
    
    &lt;td&gt;
      Bill Bishop
    &lt;/td&gt;
    
    &lt;td&gt;
      2.35
    &lt;/td&gt;
    
    &lt;td&gt;
      3.89
    &lt;/td&gt;
    
    &lt;td&gt;
      275
    &lt;/td&gt;
  &lt;/tr&gt;
  
  &lt;tr&gt;
    &lt;td&gt;
      64
    &lt;/td&gt;
    
    &lt;td&gt;
      &lt;a href=&quot;https://twitter.com/interfluidity&quot;&gt;interfluidity&lt;/a&gt;
    &lt;/td&gt;
    
    &lt;td&gt;
      Steve Randy Waldman
    &lt;/td&gt;
    
    &lt;td&gt;
      4.4
    &lt;/td&gt;
    
    &lt;td&gt;
      1.84
    &lt;/td&gt;
    
    &lt;td&gt;
      304
    &lt;/td&gt;
  &lt;/tr&gt;
  
  &lt;tr&gt;
    &lt;td&gt;
      65
    &lt;/td&gt;
    
    &lt;td&gt;
      &lt;a href=&quot;https://twitter.com/karaswisher&quot;&gt;karaswisher&lt;/a&gt;
    &lt;/td&gt;
    
    &lt;td&gt;
      Kara Swisher
    &lt;/td&gt;
    
    &lt;td&gt;
      3.55
    &lt;/td&gt;
    
    &lt;td&gt;
      2.68
    &lt;/td&gt;
    
    &lt;td&gt;
      453
    &lt;/td&gt;
  &lt;/tr&gt;
  
  &lt;tr&gt;
    &lt;td&gt;
      66
    &lt;/td&gt;
    
    &lt;td&gt;
      &lt;a href=&quot;https://twitter.com/tomgara&quot;&gt;tomgara&lt;/a&gt;
    &lt;/td&gt;
    
    &lt;td&gt;
      Tom Gara
    &lt;/td&gt;
    
    &lt;td&gt;
      2.45
    &lt;/td&gt;
    
    &lt;td&gt;
      3.74
    &lt;/td&gt;
    
    &lt;td&gt;
      375
    &lt;/td&gt;
  &lt;/tr&gt;
  
  &lt;tr&gt;
    &lt;td&gt;
      67
    &lt;/td&gt;
    
    &lt;td&gt;
      &lt;a href=&quot;https://twitter.com/Convertbond&quot;&gt;Convertbond&lt;/a&gt;
    &lt;/td&gt;
    
    &lt;td&gt;
      Lawrence McDonald
    &lt;/td&gt;
    
    &lt;td&gt;
      5.48
    &lt;/td&gt;
    
    &lt;td&gt;
      0.64
    &lt;/td&gt;
    
    &lt;td&gt;
      135
    &lt;/td&gt;
  &lt;/tr&gt;
  
  &lt;tr&gt;
    &lt;td&gt;
      68
    &lt;/td&gt;
    
    &lt;td&gt;
      &lt;a href=&quot;https://twitter.com/AlephBlog&quot;&gt;AlephBlog&lt;/a&gt;
    &lt;/td&gt;
    
    &lt;td&gt;
      David Merkel
    &lt;/td&gt;
    
    &lt;td&gt;
      2.13
    &lt;/td&gt;
    
    &lt;td&gt;
      3.96
    &lt;/td&gt;
    
    &lt;td&gt;
      129
    &lt;/td&gt;
  &lt;/tr&gt;
  
  &lt;tr&gt;
    &lt;td&gt;
      69
    &lt;/td&gt;
    
    &lt;td&gt;
      &lt;a href=&quot;https://twitter.com/SamRo&quot;&gt;SamRo&lt;/a&gt;
    &lt;/td&gt;
    
    &lt;td&gt;
      Sam Ro
    &lt;/td&gt;
    
    &lt;td&gt;
      3.9
    &lt;/td&gt;
    
    &lt;td&gt;
      2.19
    &lt;/td&gt;
    
    &lt;td&gt;
      137
    &lt;/td&gt;
  &lt;/tr&gt;
  
  &lt;tr&gt;
    &lt;td&gt;
      70
    &lt;/td&gt;
    
    &lt;td&gt;
      &lt;a href=&quot;https://twitter.com/katie_martin_fx&quot;&gt;katie_martin_fx&lt;/a&gt;
    &lt;/td&gt;
    
    &lt;td&gt;
      Katie Martin
    &lt;/td&gt;
    
    &lt;td&gt;
      4.51
    &lt;/td&gt;
    
    &lt;td&gt;
      1.54
    &lt;/td&gt;
    
    &lt;td&gt;
      23
    &lt;/td&gt;
  &lt;/tr&gt;
  
  &lt;tr&gt;
    &lt;td&gt;
      71
    &lt;/td&gt;
    
    &lt;td&gt;
      &lt;a href=&quot;https://twitter.com/JimPethokoukis&quot;&gt;JimPethokoukis&lt;/a&gt;
    &lt;/td&gt;
    
    &lt;td&gt;
      James Pethokoukis
    &lt;/td&gt;
    
    &lt;td&gt;
      3.59
    &lt;/td&gt;
    
    &lt;td&gt;
      2.44
    &lt;/td&gt;
    
    &lt;td&gt;
      368
    &lt;/td&gt;
  &lt;/tr&gt;
  
  &lt;tr&gt;
    &lt;td&gt;
      72
    &lt;/td&gt;
    
    &lt;td&gt;
      &lt;a href=&quot;https://twitter.com/EpicureanDeal&quot;&gt;EpicureanDeal&lt;/a&gt;
    &lt;/td&gt;
    
    &lt;td&gt;
      TED
    &lt;/td&gt;
    
    &lt;td&gt;
      3.05
    &lt;/td&gt;
    
    &lt;td&gt;
      2.97
    &lt;/td&gt;
    
    &lt;td&gt;
      411
    &lt;/td&gt;
  &lt;/tr&gt;
  
  &lt;tr&gt;
    &lt;td&gt;
      73
    &lt;/td&gt;
    
    &lt;td&gt;
      &lt;a href=&quot;https://twitter.com/TimDuy&quot;&gt;TimDuy&lt;/a&gt;
    &lt;/td&gt;
    
    &lt;td&gt;
      Tim Duy
    &lt;/td&gt;
    
    &lt;td&gt;
      3.18
    &lt;/td&gt;
    
    &lt;td&gt;
      2.83
    &lt;/td&gt;
    
    &lt;td&gt;
      144
    &lt;/td&gt;
  &lt;/tr&gt;
  
  &lt;tr&gt;
    &lt;td&gt;
      74
    &lt;/td&gt;
    
    &lt;td&gt;
      &lt;a href=&quot;https://twitter.com/abnormalreturns&quot;&gt;abnormalreturns&lt;/a&gt;
    &lt;/td&gt;
    
    &lt;td&gt;
      Tadas Viskanta
    &lt;/td&gt;
    
    &lt;td&gt;
      4.78
    &lt;/td&gt;
    
    &lt;td&gt;
      1.22
    &lt;/td&gt;
    
    &lt;td&gt;
      94
    &lt;/td&gt;
  &lt;/tr&gt;
  
  &lt;tr&gt;
    &lt;td&gt;
      75
    &lt;/td&gt;
    
    &lt;td&gt;
      &lt;a href=&quot;https://twitter.com/ianbremmer&quot;&gt;ianbremmer&lt;/a&gt;
    &lt;/td&gt;
    
    &lt;td&gt;
      ian bremmer
    &lt;/td&gt;
    
    &lt;td&gt;
      4.95
    &lt;/td&gt;
    
    &lt;td&gt;
      0.92
    &lt;/td&gt;
    
    &lt;td&gt;
      284
    &lt;/td&gt;
  &lt;/tr&gt;
  
  &lt;tr&gt;
    &lt;td&gt;
      76
    &lt;/td&gt;
    
    &lt;td&gt;
      &lt;a href=&quot;https://twitter.com/rortybomb&quot;&gt;rortybomb&lt;/a&gt;
    &lt;/td&gt;
    
    &lt;td&gt;
      Mike Konczal
    &lt;/td&gt;
    
    &lt;td&gt;
      3.92
    &lt;/td&gt;
    
    &lt;td&gt;
      1.94
    &lt;/td&gt;
    
    &lt;td&gt;
      303
    &lt;/td&gt;
  &lt;/tr&gt;
  
  &lt;tr&gt;
    &lt;td&gt;
      77
    &lt;/td&gt;
    
    &lt;td&gt;
      &lt;a href=&quot;https://twitter.com/matthewstoller&quot;&gt;matthewstoller&lt;/a&gt;
    &lt;/td&gt;
    
    &lt;td&gt;
      Matt Stoller
    &lt;/td&gt;
    
    &lt;td&gt;
      1.79
    &lt;/td&gt;
    
    &lt;td&gt;
      4.07
    &lt;/td&gt;
    
    &lt;td&gt;
      378
    &lt;/td&gt;
  &lt;/tr&gt;
  
  &lt;tr&gt;
    &lt;td&gt;
      78
    &lt;/td&gt;
    
    &lt;td&gt;
      &lt;a href=&quot;https://twitter.com/georgemagnus1&quot;&gt;georgemagnus1&lt;/a&gt;
    &lt;/td&gt;
    
    &lt;td&gt;
      George Magnus
    &lt;/td&gt;
    
    &lt;td&gt;
      3.87
    &lt;/td&gt;
    
    &lt;td&gt;
      1.98
    &lt;/td&gt;
    
    &lt;td&gt;
      281
    &lt;/td&gt;
  &lt;/tr&gt;
  
  &lt;tr&gt;
    &lt;td&gt;
      79
    &lt;/td&gt;
    
    &lt;td&gt;
      &lt;a href=&quot;https://twitter.com/Alea_&quot;&gt;Alea_&lt;/a&gt;
    &lt;/td&gt;
    
    &lt;td&gt;
      JC Kommer
    &lt;/td&gt;
    
    &lt;td&gt;
      1.87
    &lt;/td&gt;
    
    &lt;td&gt;
      3.96
    &lt;/td&gt;
    
    &lt;td&gt;
      170
    &lt;/td&gt;
  &lt;/tr&gt;
  
  &lt;tr&gt;
    &lt;td&gt;
      80
    &lt;/td&gt;
    
    &lt;td&gt;
      &lt;a href=&quot;https://twitter.com/ModeledBehavior&quot;&gt;ModeledBehavior&lt;/a&gt;
    &lt;/td&gt;
    
    &lt;td&gt;
      Adam Ozimek
    &lt;/td&gt;
    
    &lt;td&gt;
      3.55
    &lt;/td&gt;
    
    &lt;td&gt;
      2.24
    &lt;/td&gt;
    
    &lt;td&gt;
      325
    &lt;/td&gt;
  &lt;/tr&gt;
  
  &lt;tr&gt;
    &lt;td&gt;
      81
    &lt;/td&gt;
    
    &lt;td&gt;
      &lt;a href=&quot;https://twitter.com/PekingMike&quot;&gt;PekingMike&lt;/a&gt;
    &lt;/td&gt;
    
    &lt;td&gt;
      Mike Forsythe 傅才德
    &lt;/td&gt;
    
    &lt;td&gt;
      1.21
    &lt;/td&gt;
    
    &lt;td&gt;
      4.57
    &lt;/td&gt;
    
    &lt;td&gt;
      287
    &lt;/td&gt;
  &lt;/tr&gt;
  
  &lt;tr&gt;
    &lt;td&gt;
      82
    &lt;/td&gt;
    
    &lt;td&gt;
      &lt;a href=&quot;https://twitter.com/kadhimshubber&quot;&gt;kadhimshubber&lt;/a&gt;
    &lt;/td&gt;
    
    &lt;td&gt;
      kadhim (＾ｰ^)ノ
    &lt;/td&gt;
    
    &lt;td&gt;
      2.16
    &lt;/td&gt;
    
    &lt;td&gt;
      3.59
    &lt;/td&gt;
    
    &lt;td&gt;
      27
    &lt;/td&gt;
  &lt;/tr&gt;
  
  &lt;tr&gt;
    &lt;td&gt;
      83
    &lt;/td&gt;
    
    &lt;td&gt;
      &lt;a href=&quot;https://twitter.com/morningmoneyben&quot;&gt;morningmoneyben&lt;/a&gt;
    &lt;/td&gt;
    
    &lt;td&gt;
      Ben White
    &lt;/td&gt;
    
    &lt;td&gt;
      3.89
    &lt;/td&gt;
    
    &lt;td&gt;
      1.83
    &lt;/td&gt;
    
    &lt;td&gt;
      428
    &lt;/td&gt;
  &lt;/tr&gt;
  
  &lt;tr&gt;
    &lt;td&gt;
      84
    &lt;/td&gt;
    
    &lt;td&gt;
      &lt;a href=&quot;https://twitter.com/NinjaEconomics&quot;&gt;NinjaEconomics&lt;/a&gt;
    &lt;/td&gt;
    
    &lt;td&gt;
      Ninja Economics
    &lt;/td&gt;
    
    &lt;td&gt;
      2.17
    &lt;/td&gt;
    
    &lt;td&gt;
      3.54
    &lt;/td&gt;
    
    &lt;td&gt;
      312
    &lt;/td&gt;
  &lt;/tr&gt;
  
  &lt;tr&gt;
    &lt;td&gt;
      85
    &lt;/td&gt;
    
    &lt;td&gt;
      &lt;a href=&quot;https://twitter.com/BaldwinRE&quot;&gt;BaldwinRE&lt;/a&gt;
    &lt;/td&gt;
    
    &lt;td&gt;
      Richard Baldwin
    &lt;/td&gt;
    
    &lt;td&gt;
      2.45
    &lt;/td&gt;
    
    &lt;td&gt;
      3.25
    &lt;/td&gt;
    
    &lt;td&gt;
      297
    &lt;/td&gt;
  &lt;/tr&gt;
  
  &lt;tr&gt;
    &lt;td&gt;
      86
    &lt;/td&gt;
    
    &lt;td&gt;
      &lt;a href=&quot;https://twitter.com/valuewalk&quot;&gt;valuewalk&lt;/a&gt;
    &lt;/td&gt;
    
    &lt;td&gt;
      ValueWalk
    &lt;/td&gt;
    
    &lt;td&gt;
      3.49
    &lt;/td&gt;
    
    &lt;td&gt;
      2.2
    &lt;/td&gt;
    
    &lt;td&gt;
      197
    &lt;/td&gt;
  &lt;/tr&gt;
  
  &lt;tr&gt;
    &lt;td&gt;
      87
    &lt;/td&gt;
    
    &lt;td&gt;
      &lt;a href=&quot;https://twitter.com/LaurenLaCapra&quot;&gt;LaurenLaCapra&lt;/a&gt;
    &lt;/td&gt;
    
    &lt;td&gt;
      Lauren Tara LaCapra
    &lt;/td&gt;
    
    &lt;td&gt;
      2.17
    &lt;/td&gt;
    
    &lt;td&gt;
      3.52
    &lt;/td&gt;
    
    &lt;td&gt;
      404
    &lt;/td&gt;
  &lt;/tr&gt;
  
  &lt;tr&gt;
    &lt;td&gt;
      88
    &lt;/td&gt;
    
    &lt;td&gt;
      &lt;a href=&quot;https://twitter.com/ryanavent&quot;&gt;ryanavent&lt;/a&gt;
    &lt;/td&gt;
    
    &lt;td&gt;
      Ryan Avent
    &lt;/td&gt;
    
    &lt;td&gt;
      5.04
    &lt;/td&gt;
    
    &lt;td&gt;
      0.63
    &lt;/td&gt;
    
    &lt;td&gt;
      331
    &lt;/td&gt;
  &lt;/tr&gt;
  
  &lt;tr&gt;
    &lt;td&gt;
      89
    &lt;/td&gt;
    
    &lt;td&gt;
      &lt;a href=&quot;https://twitter.com/D_Blanchflower&quot;&gt;D_Blanchflower&lt;/a&gt;
    &lt;/td&gt;
    
    &lt;td&gt;
      Danny Blanchflower
    &lt;/td&gt;
    
    &lt;td&gt;
      3.28
    &lt;/td&gt;
    
    &lt;td&gt;
      2.36
    &lt;/td&gt;
    
    &lt;td&gt;
      150
    &lt;/td&gt;
  &lt;/tr&gt;
  
  &lt;tr&gt;
    &lt;td&gt;
      90
    &lt;/td&gt;
    
    &lt;td&gt;
      &lt;a href=&quot;https://twitter.com/danprimack&quot;&gt;danprimack&lt;/a&gt;
    &lt;/td&gt;
    
    &lt;td&gt;
      Dan Primack
    &lt;/td&gt;
    
    &lt;td&gt;
      3.97
    &lt;/td&gt;
    
    &lt;td&gt;
      1.67
    &lt;/td&gt;
    
    &lt;td&gt;
      475
    &lt;/td&gt;
  &lt;/tr&gt;
  
  &lt;tr&gt;
    &lt;td&gt;
      91
    &lt;/td&gt;
    
    &lt;td&gt;
      &lt;a href=&quot;https://twitter.com/cullenroche&quot;&gt;cullenroche&lt;/a&gt;
    &lt;/td&gt;
    
    &lt;td&gt;
      Cullen Roche
    &lt;/td&gt;
    
    &lt;td&gt;
      4.58
    &lt;/td&gt;
    
    &lt;td&gt;
      1.03
    &lt;/td&gt;
    
    &lt;td&gt;
      88
    &lt;/td&gt;
  &lt;/tr&gt;
  
  &lt;tr&gt;
    &lt;td&gt;
      92
    &lt;/td&gt;
    
    &lt;td&gt;
      &lt;a href=&quot;https://twitter.com/scottlincicome&quot;&gt;scottlincicome&lt;/a&gt;
    &lt;/td&gt;
    
    &lt;td&gt;
      Scott Lincicome
    &lt;/td&gt;
    
    &lt;td&gt;
      1.11
    &lt;/td&gt;
    
    &lt;td&gt;
      4.46
    &lt;/td&gt;
    
    &lt;td&gt;
      367
    &lt;/td&gt;
  &lt;/tr&gt;
  
  &lt;tr&gt;
    &lt;td&gt;
      93
    &lt;/td&gt;
    
    &lt;td&gt;
      &lt;a href=&quot;https://twitter.com/MikeIsaac&quot;&gt;MikeIsaac&lt;/a&gt;
    &lt;/td&gt;
    
    &lt;td&gt;
      rat king
    &lt;/td&gt;
    
    &lt;td&gt;
      2.58
    &lt;/td&gt;
    
    &lt;td&gt;
      2.87
    &lt;/td&gt;
    
    &lt;td&gt;
      454
    &lt;/td&gt;
  &lt;/tr&gt;
  
  &lt;tr&gt;
    &lt;td&gt;
      94
    &lt;/td&gt;
    
    &lt;td&gt;
      &lt;a href=&quot;https://twitter.com/jmackin2&quot;&gt;jmackin2&lt;/a&gt;
    &lt;/td&gt;
    
    &lt;td&gt;
      James Mackintosh
    &lt;/td&gt;
    
    &lt;td&gt;
      4.81
    &lt;/td&gt;
    
    &lt;td&gt;
      0.64
    &lt;/td&gt;
    
    &lt;td&gt;
      17
    &lt;/td&gt;
  &lt;/tr&gt;
  
  &lt;tr&gt;
    &lt;td&gt;
      95
    &lt;/td&gt;
    
    &lt;td&gt;
      &lt;a href=&quot;https://twitter.com/LorcanRK&quot;&gt;LorcanRK&lt;/a&gt;
    &lt;/td&gt;
    
    &lt;td&gt;
      Lorcan Roche Kelly
    &lt;/td&gt;
    
    &lt;td&gt;
      4.93
    &lt;/td&gt;
    
    &lt;td&gt;
      0.51
    &lt;/td&gt;
    
    &lt;td&gt;
      67
    &lt;/td&gt;
  &lt;/tr&gt;
  
  &lt;tr&gt;
    &lt;td&gt;
      96
    &lt;/td&gt;
    
    &lt;td&gt;
      &lt;a href=&quot;https://twitter.com/alex&quot;&gt;alex&lt;/a&gt;
    &lt;/td&gt;
    
    &lt;td&gt;
      in Providence alex
    &lt;/td&gt;
    
    &lt;td&gt;
      0.72
    &lt;/td&gt;
    
    &lt;td&gt;
      4.72
    &lt;/td&gt;
    
    &lt;td&gt;
      477
    &lt;/td&gt;
  &lt;/tr&gt;
  
  &lt;tr&gt;
    &lt;td&gt;
      97
    &lt;/td&gt;
    
    &lt;td&gt;
      &lt;a href=&quot;https://twitter.com/MattGoldstein26&quot;&gt;MattGoldstein26&lt;/a&gt;
    &lt;/td&gt;
    
    &lt;td&gt;
      Matthew Goldstein
    &lt;/td&gt;
    
    &lt;td&gt;
      2.65
    &lt;/td&gt;
    
    &lt;td&gt;
      2.79
    &lt;/td&gt;
    
    &lt;td&gt;
      431
    &lt;/td&gt;
  &lt;/tr&gt;
  
  &lt;tr&gt;
    &lt;td&gt;
      98
    &lt;/td&gt;
    
    &lt;td&gt;
      &lt;a href=&quot;https://twitter.com/AnnPettifor&quot;&gt;AnnPettifor&lt;/a&gt;
    &lt;/td&gt;
    
    &lt;td&gt;
      Ann Pettifor
    &lt;/td&gt;
    
    &lt;td&gt;
      2.52
    &lt;/td&gt;
    
    &lt;td&gt;
      2.88
    &lt;/td&gt;
    
    &lt;td&gt;
      50
    &lt;/td&gt;
  &lt;/tr&gt;
  
  &lt;tr&gt;
    &lt;td&gt;
      99
    &lt;/td&gt;
    
    &lt;td&gt;
      &lt;a href=&quot;https://twitter.com/modestproposal1&quot;&gt;modestproposal1&lt;/a&gt;
    &lt;/td&gt;
    
    &lt;td&gt;
      modest proposal
    &lt;/td&gt;
    
    &lt;td&gt;
      3.91
    &lt;/td&gt;
    
    &lt;td&gt;
      1.48
    &lt;/td&gt;
    
    &lt;td&gt;
      243
    &lt;/td&gt;
  &lt;/tr&gt;
  
  &lt;tr&gt;
    &lt;td&gt;
      100
    &lt;/td&gt;
    
    &lt;td&gt;
      &lt;a href=&quot;https://twitter.com/rodrikdani&quot;&gt;rodrikdani&lt;/a&gt;
    &lt;/td&gt;
    
    &lt;td&gt;
      Dani Rodrik
    &lt;/td&gt;
    
    &lt;td&gt;
      4.26
    &lt;/td&gt;
    
    &lt;td&gt;
      1.12
    &lt;/td&gt;
    
    &lt;td&gt;
      318
    &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;A word about methodology:&lt;/p&gt;

&lt;p&gt;1) Start with a few highly-followed accounts, e.g.: &lt;a href=&quot;https://twitter.com/pdacosta&quot;&gt;pdacosta&lt;/a&gt;, &lt;a href=&quot;https://twitter.com/TheStalwart&quot;&gt;TheStalwart&lt;/a&gt;, &lt;a href=&quot;https://twitter.com/ReformedBroker&quot;&gt;ReformedBroker&lt;/a&gt;, &lt;a href=&quot;https://twitter.com/ritholtz&quot;&gt;ritholtz&lt;/a&gt;, &lt;a href=&quot;https://twitter.com/felixsalmon&quot;&gt;felixsalmon&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;2) Determine who they follow! Traverse the Twitter graph using the API.&lt;/p&gt;

&lt;p&gt;3) From these 5 people you can get a pretty great starting list:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;About 14,700 users&lt;/li&gt;
  &lt;li&gt;But only 112 users who are followed by all 5, or 4 out of the 5&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;4) That would be a great place to start, but we can do a little better:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Cull the non-financial users, like &lt;a href=&quot;https://twitter.com/darth&quot;&gt;darth&lt;/a&gt; or &lt;insert political=&quot;&quot; figure=&quot;&quot;&gt;&lt;/insert&gt;&lt;/li&gt;
  &lt;li&gt;Iterate and create a new Twitter graph starting from the users remaining&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In general, from a given list of users, get a better list by&lt;/p&gt;

&lt;p&gt;1) First expanding it, by finding who these users follow&lt;/p&gt;

&lt;p&gt;2) Then ranking and filtering the new list by&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Influence: how many people in the list follow them, and recursively how influential they are (PageRank)&lt;/li&gt;
  &lt;li&gt;Relevance: how frequently they post financial content (financial sites, tickers, topics, and recursively, items that hit the StreetEYE frontpage)&lt;/li&gt;
  &lt;li&gt;Timeliness: how often they are first to post something that later gets popular&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Iterate a few times, and you get a pretty good list of people to follow.&lt;/p&gt;

&lt;p&gt;In the past I’ve generated a graph of the users, and this year I really went ham on it and created &lt;a href=&quot;http://media.streeteye.com/static/fintwit201901/&quot;&gt;this magnificent beast&lt;/a&gt;:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://media.streeteye.com/static/fintwit201901/&quot;&gt;&lt;img src=&quot;/assets/wp-content/uploads/2019/01/graph2019.png&quot; alt=&quot;&quot; width=&quot;476&quot; height=&quot;242&quot; class=&quot;aligncenter size-full wp-image-6758&quot; srcset=&quot;/assets/wp-content/uploads/2019/01/graph2019.png 476w, /assets/wp-content/uploads/2019/01/graph2019-300x153.png 300w&quot; sizes=&quot;(max-width: 476px) 100vw, 476px&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;If you &lt;a href=&quot;http://media.streeteye.com/static/fintwit201901/&quot;&gt;click here&lt;/a&gt; you can explore the graph interactively:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Roll over and get detailed info on each FinTwit personality&lt;/li&gt;
  &lt;li&gt;Word cloud (roll over)&lt;/li&gt;
  &lt;li&gt;3 most similar accounts, using topic analysis of what they post about, who they share same URLs as, who they follow and are followed by. &lt;em&gt;P.S. I LOVE THIS FEATURE!&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;Each user’s most frequently shared domains, hashtag, tickers, other FinTwitterers they mention&lt;/li&gt;
  &lt;li&gt;Who they follow/are followed by (roll over ‘followed by’/’followed’)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I hope this helps everyone find great new FinTwit BFFs to follow.&lt;/p&gt;

&lt;p&gt;It’s always a bit arbitrary, where to cut off people who aren’t relevant. Some people may find the influential tech or political accounts a bore, but I try to find a balance.&lt;/p&gt;

&lt;p&gt;The biggest problem is churn. There are some people who are highly followed who don’t really post relevant stuff any more. There are people who are pretty relevant but it takes a very long time to break through and get influential. I could expand the panel, but the more you expand it the more the common denominator is… all Trump all the time.&lt;/p&gt;

&lt;p&gt;Then, I guess I could use topic analysis to try to downvote Trump and politics… use noise cancellation to determine what is popular out in the broad population and penalize it … but it’s turtles all the way down the rabbit hole.&lt;/p&gt;

&lt;p&gt;That’s it! If you’re looking for top blogs for your daily list or Feedly reader check out the July listicle of &lt;a href=&quot;/2018/07/the-most-shared-financial-blogs-2018/&quot;&gt;the most shared financial blogs&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Not to be too thirsty but if you like it don’t hesitate to share!&lt;/p&gt;

&lt;p&gt;&lt;small&gt;(Never say never again, but very probably never doing this graph again. Fun mad science, but disproportionately time-consuming. Twitter makes the API more restrictive every year, knucklehead sites block me, the graveyard is full of algorithmic news apps and news bots. C’est la vie!)&lt;/small&gt;&lt;/p&gt;</content><author><name>Druce Vertes, CFA</name></author><category term="tech" /><category term="fintwit" /><summary type="html">It’s been more than a year since we posted our last list of people to follow on Twitter for financial news. Time for an update!</summary></entry></feed>